{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############# BASIC PACKAGES TO IMPORT ############\n",
    "import os\n",
    "import pandas as pd #To allow us to work with dataframes\n",
    "import numpy as np #To allow us to make mathematical transformations\n",
    "import matplotlib.mlab as mlab #To create plots\n",
    "import matplotlib.pylab as plt #To create plots\n",
    "%matplotlib inline \n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "from mpl_toolkits import mplot3d\n",
    "import category_encoders as ce #To encode our nominal and categorical variables\n",
    "from sklearn import preprocessing, metrics #This module can be helpful when processing data\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import uniform, chi2_contingency, chisquare\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import pylab as py\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "#import scipy as sp #To play with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /anaconda3/lib/python3.6/site-packages (0.90)\r\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.3.1)\r\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.15.2)\r\n"
     ]
    }
   ],
   "source": [
    "############# Models to import #############\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "#from xgboost import XGBClassifier\n",
    "import sys \n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "#updating xgboost and scipy to get rid of an error (9/18/19)\n",
    "#RUN IN TERMINAL\n",
    "# pip install --upgrade pip\n",
    "# pip install --upgrade xgboost \n",
    "# pip install --upgrade scipy\n",
    "# pip install --upgrade sklearn\n",
    "# pip install --upgrade plotly\n",
    "# pip install --upgrade pydotplus \n",
    "# pip install --upgrade graphviz\n",
    "# Use 'brew' instead of pip for updates to get the right packages on your computer.\n",
    "\n",
    "\n",
    "#I don't believe that we need this if we export the entire sklearn library. I will comment these out \n",
    "#until it's time to start training models.\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package to visualize decision trees\n",
    "#from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########### IMPORT SAVED VARIABLES FROM PREVIOUS SCRIPTS (Data Pipeline 2 Pt.2 Feature Engineering) ###########\n",
    "#Open up the pickle file\n",
    "f = open('DataPipeline2_pt2.pckl','rb')\n",
    "pickle_list = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#[0] = train_X2\n",
    "#[1] = train_y2\n",
    "#[2] = test_X2\n",
    "#[3] = test_IDs\n",
    "\n",
    "#Pull out the important objects in the pickle file\n",
    "train_X2 = pickle_list[0]\n",
    "train_y2 = pickle_list[1]\n",
    "test_X2 = pickle_list[2]\n",
    "test_IDs = pickle_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########### IMPORTANT FUNCTIONS INVOLVING TRAIN/DEV SETS ###########\n",
    "\n",
    "######### SPLITSTANDARD ##########\n",
    "def SplitStandard(train_X, train_y, test_X, train_split = 0.70, random_seed = 7, \n",
    "                  Standardizer = True, scaler = preprocessing.StandardScaler(), Normalizer = False, SandN = False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that will split the train_X and train_y into a train_X, dev_X, train_y, and dev_y sets so that\n",
    "    #we can evaluate our models to see how well different combos of parameters are doing. This will also do a Standard\n",
    "    #transform_fit on train_X, and do a transform on dev_X, and test_X. This is done so we do not get test data leak \n",
    "    #when we do the transform_fit on train_X and can get a purer examination of our models and compare them.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only). (I may include a predict option later, I am not sure)\n",
    "    #***Standardizer = boolean that determines if we scale the features or not. Cannot be used \n",
    "    # if Normalizer is True.\n",
    "    #***scaler = the sklearn scaler that we will use to scale the data along the columns. \n",
    "    # The 3 options are MinMaxScaler, RobustScaler, and StandardScaler. \n",
    "    #***train_split = The percentage of training data that will go into the new train set. \n",
    "    # 1 - train_split will go into the new dev set.\n",
    "    #***random_seed = the value we insert into np.random.seed(). This is done so that \n",
    "    # we get the same train/dev split everytime we insert the same value into np.random.seed()\n",
    "    #***Normalizer = boolean value that determines if we normalize the values along the rows. \n",
    "    # scalers will scale across the features so that the distribution of values along the features \n",
    "    # changes, but this variable affects the actual rows (or vectors if you will) instead. Not \n",
    "    # recommended unless you understand the changes that will occur after normalization.\n",
    "    # Cannot be used with Standardizer = True\n",
    "    #***SandN = Boolean that determines if we Normalize (first) and Standardize (second) the data. \n",
    "    \n",
    "    \n",
    "    #Combine the train_X and train_y dataframes. This is done so that when we split \n",
    "    #them into their new train / dev sets, the target variables stay with the appropriate \n",
    "    #feature vectors.\n",
    "    combo_df = pd.concat([train_X,train_y],axis = 1)\n",
    "    \n",
    "    train, dev = train_test_split(combo_df,train_size=train_split,random_state=random_seed)\n",
    "    \n",
    "    \n",
    "    #Split our data back into feature dataframes (new_train_X, dev_X) and the target Series (new_train_y, dev_y)\n",
    "    new_train_X = train.loc[:,train.columns != 'SalePrice']\n",
    "    new_train_y = train.loc[:,'SalePrice']\n",
    "    dev_X = dev.loc[:,dev.columns != 'SalePrice']\n",
    "    dev_y = dev.loc[:,'SalePrice']\n",
    "    columns = new_train_X.columns\n",
    "    \n",
    "    #Now we standardize our data.\n",
    "    #We initially fit the scaler to the train data (find the mean and std to be used on the other sets)\n",
    "    #then we take the fit scaler and transform the dev and test set.\n",
    "    if Standardizer:\n",
    "        standardized_train_X = scaler.fit_transform(new_train_X)\n",
    "        standardized_train_X = pd.DataFrame(standardized_train_X, columns=columns)\n",
    "        standardized_dev_X = scaler.transform(dev_X)\n",
    "        standardized_dev_X = pd.DataFrame(standardized_dev_X, columns=columns)\n",
    "        standardized_test_X = scaler.transform(test_X)\n",
    "        standardized_test_X = pd.DataFrame(standardized_test_X, columns=columns)\n",
    "        #Save our standardized dataframes.\n",
    "        #Standard_Data = [standardized_train_X, new_train_y, standardized_dev_X, dev_y, standardized_test_X]\n",
    "        return standardized_train_X, new_train_y, standardized_dev_X, dev_y, standardized_test_X\n",
    "    \n",
    "    elif Normalizer:\n",
    "        normalizer = preprocessing.Normalizer()\n",
    "        normalized_train_X = normalizer.fit_transform(new_train_X)\n",
    "        normalized_train_X = pd.DataFrame(normalized_train_X,columns=columns)\n",
    "        normalized_dev_X = normalizer.transform(dev_X)\n",
    "        normalized_dev_X = pd.DataFrame(normalized_dev_X,columns=columns)\n",
    "        normalized_test_X = normalizer.transform(test_X)\n",
    "        normalized_test_X = pd.DataFrame(normalized_test_X,columns=columns)\n",
    "        #Save our normalized dataframes.\n",
    "        #Normalized_Data = [normalized_train_X, new_train_y, normalized_dev_X, dev_y, normalized_test_X]\n",
    "        return normalized_train_X, new_train_y, normalized_dev_X, dev_y, normalized_test_X\n",
    "    \n",
    "    elif SandN:\n",
    "        normalizer = preprocessing.Normalizer()\n",
    "        s_train_X = scaler.fit_transform(new_train_X)\n",
    "        s_dev_X = scaler.transform(dev_X)\n",
    "        s_test_X = scaler.transform(test_X)\n",
    "        sn_train_X = normalizer.fit_transform(s_train_X)\n",
    "        sn_train_X = pd.DataFrame(sn_train_X,columns=columns)\n",
    "        sn_dev_X = normalizer.transform(s_dev_X)\n",
    "        sn_dev_X = pd.DataFrame(sn_dev_X,columns=columns)\n",
    "        sn_test_X = normalizer.transform(s_test_X)\n",
    "        sn_test_X = pd.DataFrame(sn_test_X,columns=columns)\n",
    "        return sn_train_X, new_train_y, sn_dev_X, dev_y, sn_test_X\n",
    "    \n",
    "    else:\n",
    "        #If we don't standardize or normalize, then we will just return the regular split dataframes\n",
    "        #Non_Standard_Data = [new_train_X, new_train_y, dev_X, dev_y, test_X]\n",
    "        return new_train_X, new_train_y, dev_X, dev_y, test_X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############# STANDARDIZE AND SPLIT THE DATA #############\n",
    "#Standardize our data, and pull out the new dataframes\n",
    "\n",
    "#Standardized Split Data\n",
    "strain_X, strain_y, sdev_X, sdev_y, stest_X = SplitStandard(train_X=train_X2,train_y=train_y2,test_X=test_X2)\n",
    "\n",
    "#Regular Split Data \n",
    "#I created these variables incase we want to see how the same model runs on non-standardized data.\n",
    "train_X3, train_y3, dev_X, dev_y, test_X3 = SplitStandard(train_X=train_X2,train_y=train_y2,test_X=test_X2,Standardizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########### IMPORTANT FUNCTIONS INVOLVING TRAIN/DEV SETS PT.2 ###########\n",
    "def TrainDevTestErrors(model,savefileName=None,save=True,train_X=strain_X,train_y=strain_y,dev_X=sdev_X,dev_y=sdev_y,\n",
    "                       test_X=stest_X,t_IDs=test_IDs,metric=metrics.mean_squared_log_error):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that will compute the train and dev set errors and explained variances of a specific model.\n",
    "    #This will also compute the test predictions, and save them if save=True.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***model = The Machine Learning model.\n",
    "    #***savefileName = The string of the filename.\n",
    "    #***save = Boolean that determines whether we save the test predictions.\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***dev_X = the dev data (features only).\n",
    "    #***dev_y = the dev data (target only).\n",
    "    #***test_X = the testing data (features only).\n",
    "    #***t_IDs = IDs for the testing data.\n",
    "    #***metric = the metric for which we are examining the error.\n",
    "    \n",
    "    #Fit the model to the training data \n",
    "    model_fit = model.fit(train_X,train_y)\n",
    "    \n",
    "    #Create predictions on the training set. Compute the error and explained variance.\n",
    "    train_pred = model_fit.predict(train_X)\n",
    "    train_error = metric(train_y,train_pred)\n",
    "    train_explained_var = metrics.explained_variance_score(train_y,train_pred)\n",
    "    \n",
    "    #Create predictions on the dev set. Compute the error and explained variance.\n",
    "    dev_pred = model_fit.predict(dev_X)\n",
    "    dev_error = metric(dev_y,dev_pred)\n",
    "    dev_explained_var = metrics.explained_variance_score(dev_y,dev_pred)\n",
    "    \n",
    "    #If save = True, create predictions on the test set, and save the predictions using SaveFitModels() \n",
    "    if save:\n",
    "        test_pred = model_fit.predict(test_X)\n",
    "        SaveFitModels(test_pred,t_IDs,savefileName)\n",
    "    \n",
    "    return train_error, train_explained_var, dev_error, dev_explained_var\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def XGBTrainDevComparisons(xgb_model, xgb_param, xgb_parameter_values, metric=metrics.mean_squared_log_error, \n",
    "                           train_X=strain_X, train_y=strain_y, dev_X=sdev_X, dev_y=sdev_y,exp_var=False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that will take a parameter and set of parameter values that will be replaced within an \n",
    "    #xgb model. For each value of the parameter, a fit is done on the traiing set. Then, we will predict on \n",
    "    #the train and dev sets and compare their errors on a graph to see how well each parameter does on each set.\n",
    "    #We also collect the explained variances on each instance of the training and dev set, and will plot that \n",
    "    #if exp_var = True.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***xgb_model= the XGBRegressor algorithm with defined parameters (can decide to leave some parameters blank)\n",
    "    #***xgb_param = This is the parameter name. This will be a string of the parameter we are changing.\n",
    "    #***xgb_param_vals = This is the series of values that we will be changing in our model, and will tell us\n",
    "    # how well a given parameter is doing on both train and dev sets.\n",
    "    #***metric = The metric that we are using to evaluate the error.\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***dev_X = the dev set data (features only). \n",
    "    #***dev_y = the dev set data (target only).\n",
    "    #***exp_var = boolean that determines if we graph the expected variance values from the dev and train set.\n",
    "    \n",
    "    #The number of iterations \n",
    "    rounds = len(xgb_parameter_values)\n",
    "    \n",
    "    train_error_arr = []\n",
    "    dev_error_arr = []\n",
    "   \n",
    "    #Create the arrays to be filled with the explained variances.\n",
    "    exp_var_arr = pd.DataFrame(columns=[xgb_param,'Explained Variance','Type'])\n",
    "    #error_arr = pd.DataFrame(columns=[xgb_param,'Error','Type'])\n",
    "    counter = 0 #counter for the exp_var_arr\n",
    "    \n",
    "    #Loop through the different parameters to try.\n",
    "    for i in range(rounds):\n",
    "        #Create a parameter dictionary so that we can change the parameter in the model\n",
    "        param_dict = {xgb_param:xgb_parameter_values[i]}\n",
    "        #Update the model to have the parameter that we need.\n",
    "        new_model = xgb_model.set_params(**param_dict)\n",
    "        #Fit to the new model.\n",
    "        new_model_fit = new_model.fit(train_X,train_y)\n",
    "        \n",
    "        #Dealing with the training set.\n",
    "        train_pred = new_model.predict(train_X) #Create predictions\n",
    "        train_error = metric(train_y,train_pred) #Find the error\n",
    "        train_error_arr.append(train_error)\n",
    "        #error_arr.loc[counter] = [xgb_parameter_values[i]] + [train_error,'training'] #Append to error array\n",
    "        train_exp_var = metrics.explained_variance_score(train_y,train_pred) #Find the explained variance\n",
    "        exp_var_arr.loc[counter] = [xgb_parameter_values[i]] + [train_exp_var,'training'] #Append to explained variance array\n",
    "        counter = counter + 1 #increment counter\n",
    "        \n",
    "        #Dealing with the dev set.\n",
    "        dev_pred = new_model.predict(dev_X) #Create predictions\n",
    "        dev_error = metric(dev_y,dev_pred) #Find the error\n",
    "        dev_error_arr.append(dev_error)\n",
    "        #error_arr.loc[counter] = [xgb_parameter_values[i]] + [dev_error,'dev'] #Append to error array\n",
    "        dev_exp_var = metrics.explained_variance_score(dev_y,dev_pred) #Find the explained variance\n",
    "        exp_var_arr.loc[counter] = [xgb_parameter_values[i]] + [dev_exp_var,'dev'] #Append to explained variance array\n",
    "        counter = counter + 1 #increment counter\n",
    "   \n",
    "    #Create the figures.\n",
    "    rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "    \n",
    "#     #Plot the error_arr with px.scatter\n",
    "#     fig1 = px.scatter(error_arr, x=error_arr.columns[0], y='Error', color='Type')\n",
    "#     fig1.show() #Show the plot\n",
    "    \n",
    "    #Create the first plot - training errors              \n",
    "    df1 = pd.DataFrame(data=list(zip(xgb_parameter_values,train_error_arr)),columns=[xgb_param,'Training Error'])\n",
    "    fig1 = px.scatter(df1, x=df1.columns[0], y='Training Error', color='Training Error')\n",
    "\n",
    "    #Create the second plot - dev errors\n",
    "    df2 = pd.DataFrame(data=list(zip(xgb_parameter_values,dev_error_arr)),columns=[xgb_param,'Dev Error'])\n",
    "    fig2 = px.scatter(df2, x=df2.columns[0], y='Dev Error', color='Dev Error')\n",
    "    \n",
    "    fig1.show()\n",
    "    fig2.show()\n",
    "    \n",
    "    #If we want to show explained variance, we will plot this.\n",
    "    if exp_var:\n",
    "        #Create the third plot - explained variance score\n",
    "        fig3 = px.scatter(exp_var_arr, x=exp_var_arr.columns[0], y='Explained Variance', color='Type')\n",
    "        fig3.show()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########## OTHER IMPORTANT FUNCTIONS ###########\n",
    "#Random Forest Regressor model fit function\n",
    "def RFRmodelfitCV(alg, train_X, train_y, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_X,train_y)\n",
    "    \n",
    "    #Predict on the training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, train_X, train_y, cv = cv_folds, scoring='neg_mean_squared_log_error')\n",
    "        \n",
    "    #Print the model report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    if performCV:\n",
    "        #print('CV Score: %s'% cv_score)\n",
    "        print(\"CV Scores \\nMean : %.7g | Std : %.7g | Min : %.7g | Max : %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #Print Feature Importance\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_,train_X.columns).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar', title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        #print(feat_imp) #I may add this\n",
    "\n",
    "\n",
    "\n",
    "########### MODELFITXGB ###########\n",
    "#Same as above function, however it works for XGBoost and can be used to \n",
    "#find the best n_estimators value at the start of the program      \n",
    "def modelfitXGB(alg, train_X, train_y, useTrainCV=True, printFeatureImportance=True, cv_folds=3, early_stopping_rounds=50):\n",
    "    \n",
    "    #This will fit the train data to the xgboost model and cross-validate \n",
    "    #on the data until the error rate stops improving. This will find an \n",
    "    #appropriate value of n_estimators.\n",
    "    if useTrainCV:\n",
    "        xgb_params = alg.get_xgb_params()\n",
    "        #print(alg.get_params()['n_estimators'])\n",
    "        xgtrain = xgb.DMatrix(train_X.values,label=train_y.values)\n",
    "        cvresult = xgb.cv(xgb_params,xgtrain,num_boost_round=alg.get_params()['n_estimators'],nfold=cv_folds,metrics='rmse',\n",
    "                          early_stopping_rounds=early_stopping_rounds,verbose_eval=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(\"n_estimators: %.4g\" % alg.get_params()['n_estimators'])\n",
    "    \n",
    "    #Fit Algorithm on the data\n",
    "    alg.fit(train_X,train_y,eval_metric='rmse')\n",
    "    \n",
    "    #Predict training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Print Model Report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    #Print feature importances\n",
    "    if printFeatureImportance:\n",
    "        rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "        feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar',title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Important Score')\n",
    "\n",
    "        \n",
    "\n",
    "######### SAVEFITMODELS #########\n",
    "#Save our predictions to the proper directory.\n",
    "def SaveFitModels(pred, IDs, fileName, saveDirectory1 = '/Users/armenta/Kaggle/Housing Prices/Predictions 2/', \n",
    "                  saveDirectory2 ='/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS 2/'):\n",
    "    \n",
    "    #Converting the predictions into a form that can be combined with their ID's\n",
    "    pred = pd.Series(pred)\n",
    "    pred = pd.concat([pred,IDs.rename('Id')],axis=1)\n",
    "    pred = pred.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "    pred = pred[['Id','SalePrice']]\n",
    "    #Create the path to save the outputs\n",
    "    path1 = saveDirectory1 + fileName\n",
    "    path2 = saveDirectory2 + fileName\n",
    "    #Save the outputs\n",
    "    pred.to_csv(path_or_buf = path1)\n",
    "    pred.to_csv(path_or_buf = path2)\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "def TDComp(model_results,column,number=10):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that takes the model_results from a parameter gridsearch or randomizedsearch, and\n",
    "    #grabs the top # of column values (specified by column and number) and transforms it into a viable \n",
    "    #format that can be used for the XGBTrainDevComparisons function. The output is supposed to be used for \n",
    "    #the xgb_parameter_values variable.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***model_results = the model results from running a parameter grid or randomized search on a ML model.\n",
    "    #***column = The column that you want to look at, or the parameter that was tuned. Usually, the string is\n",
    "    # 'param_' + paramter name \n",
    "    #***number = The number of values that you want to collect from the model_results. Cannot be bigger than \n",
    "    # model_results.shape[0], or else you would be wanting to grab more values than there are in the dataframe.\n",
    "\n",
    "    #Example code:\n",
    "    #top_n_est = pd.Series(model_results_dart1_2.loc[model_results_dart1_2.rank_test_score<=10,'param_n_estimators']).reset_index(drop=True)\n",
    "    \n",
    "    #Grab the specific values that you want. This series is designed to be used as xgb_parameter_values for the \n",
    "    #XGBTrainDevComparisons function below.\n",
    "    top_values = pd.Series(model_results.loc[model_results.rank_test_score<=number,column]).reset_index(drop=True)\n",
    "    return top_values\n",
    "\n",
    "\n",
    "\n",
    "def Standardizer(train_X = train_X2, test_X = test_X2, StandardScaler = preprocessing.StandardScaler(), Standardizer = True, Normalizer = False, SandN = False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This function will do a Standard transform_fit on train_X and test_X. This is done so we do not get test data leak \n",
    "    #when we do the transform_fit on train_X and can get a purer examination of our models and compare them.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only). (I may include a predict option later, I am not sure)\n",
    "    #***Standardizer = boolean that determines if we scale the features or not. Cannot be used \n",
    "    # if Normalizer is True.\n",
    "    #***scaler = the sklearn scaler that we will use to scale the data along the columns. \n",
    "    # The 3 options are MinMaxScaler, RobustScaler, and StandardScaler. \n",
    "    #***Normalizer = boolean value that determines if we normalize the values along the rows. \n",
    "    # scalers will scale across the features so that the distribution of values along the features \n",
    "    # changes, but this variable affects the actual rows (or vectors if you will) instead. Not \n",
    "    # recommended unless you understand the changes that will occur after normalization.\n",
    "    # Cannot be used with Standardizer = True\n",
    "    #***SandN = Boolean that determines if we Normalize (first) and Standardize (second) the data. \n",
    "    \n",
    "    #Save the column names so that we can convert the arrays to dataframes\n",
    "    columns = train_X.columns\n",
    "    \n",
    "    if Standardizer:\n",
    "        #Now we standardize our data.\n",
    "        #We initially fit the scaler to the train data (find the mean and std to be used on the other sets)\n",
    "        #then we take the fit scaler and transform the dev and test set.\n",
    "        standardized_train_X = StandardScaler.fit_transform(train_X) #Transform the train data\n",
    "        standardized_train_X = pd.DataFrame(standardized_train_X, columns=columns) #Convert to a dataframe\n",
    "        standardized_test_X = StandardScaler.transform(test_X) #Transform the test data\n",
    "        standardized_test_X = pd.DataFrame(standardized_test_X, columns=columns) #Convert to a dataframe\n",
    "        #Return the standardized datasets\n",
    "        return standardized_train_X, standardized_test_X\n",
    "    \n",
    "    elif Normalizer:\n",
    "        #We can normalize the data\n",
    "        normalizer = preprocessing.Normalizer() #Instantiate the normalizer\n",
    "        normalized_train_X = normalizer.fit_transform(train_X) #Transform the train data\n",
    "        normalized_train_X = pd.DataFrame(normalized_train_X,columns=columns) #Convert to a dataframe\n",
    "        normalized_test_X = normalizer.transform(test_X) #Transform the test data\n",
    "        normalized_test_X = pd.DataFrame(normalized_test_X,columns=columns) #Convert to a dataframe\n",
    "        #Return the normalized datasets\n",
    "        return normalized_train_X, normalized_test_X\n",
    "    \n",
    "    elif SandN:\n",
    "        normalizer = preprocessing.Normalizer() #Instantiate the normalizer\n",
    "        s_train_X = StandardScaler.fit_transform(train_X) #Standardize the train data\n",
    "        s_test_X = StandardScaler.transform(test_X) #Standardize the test data \n",
    "        sn_train_X = normalizer.fit_transform(s_train_X) #Normalize the train data \n",
    "        sn_train_X = pd.DataFrame(sn_train_X,columns=columns) #Convert to a dataframe\n",
    "        sn_test_X = normalizer.transform(s_test_X) #Normalize the test data\n",
    "        sn_test_X = pd.DataFrame(sn_test_X,columns=columns) #Convert to a dataframe\n",
    "        #Return the standardized / normalized datasets\n",
    "        return sn_train_X, sn_test_X\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# STANDARDIZE THE DATA (NO SPLITS) #############\n",
    "#Standardized Data\n",
    "strain_X2, stest_X2 = Standardizer()\n",
    "\n",
    "#Normalized Data\n",
    "ntrain_X2, ntest_X2 = Standardizer(Standardizer=False, Normalizer=True)\n",
    "\n",
    "#Standardized and Normalized Data\n",
    "sntrain_X2, sntest_X2 = Standardizer(Standardizer=False, SandN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########## OTHER IMPORTANT FUNCTIONS pt.2 ###########\n",
    "################### XGBRModelTune Function ###################\n",
    "def XGBRModelTune(xgb_alg, xgb_param, xgb_param_vals, train_X=strain_X2, train_y=train_y2, test_X=stest_X2, \n",
    "                  cv_num=3, scoring='neg_mean_squared_log_error',Randomized = False, n_iter = 10, \n",
    "                  plot2d = True, modelfit = False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that is used to tune parameters for the XGBoost parameters. There are a total of approximately\n",
    "    #11 parameters to change in XGBoost, but there will only be 9 that can be tuned in this function. \n",
    "    #The only 2 that are not being tuned: objective and booster. You can change these in the definition of the function,\n",
    "    #but they will not be tuned in this function because the number of values are so low, that I think its best \n",
    "    #to manually test it.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only). (I may include a predict option later, I am not sure)\n",
    "    #***xgb_alg = the XGBRegressor algorithm with starting parameters (can decide to leave some parameters blank)\n",
    "    #***xgb_param = This is the parameter name. This will be a string of the parameter we are tuning.\n",
    "    #***xgb_param_vals = This will be the range that we will search for when we grid search for the best variable values.\n",
    "    # The range should be as long as you can possibly make it so we can test a plethora of values.\n",
    "    # If Randomized = True, make sure that the array is larger than the value given for n_jobs, \n",
    "    # as this will return an error for RandomizedSearchCV. If you are unsure, then just leave Randomized = False. \n",
    "    # The different variables are as follows:\n",
    "          #***learning_rate = the learning rate of the XGBRegressor algorithm.\n",
    "          #***n_estimators = the number of trees to use in this ensemble model. \n",
    "          #***max_depth = maximum depth allowed for an individual tree.\n",
    "          #***min_child_weight = minimum number of weights allowed for a child node; basically a variable that describes the amount of \n",
    "          # observations that are allowed in each child node. The higher the value, the more values that are required in each node.\n",
    "          #***gamma = A value that defines the minimum positive reduction in the loss function that must occur for a node to split.\n",
    "          #***subsample = A value that denotes the % of samples to be used in each node of the tree.\n",
    "          #***colsample_bytree = A value that determines the % of columns to be used for each tree.\n",
    "          #***objective = The loss function to be minimized.\n",
    "          #***booster = The type of model that we run at each iteration. Can choose gbtree (tree-based models), gblinear (linear models),\n",
    "          # or dart which is similar to gbtree but it implements deep neural networks drop-out technique.\n",
    "          #***reg_lambda = L2 regularization term on weights. Used to handle the main regularization part of XGBoost.\n",
    "          #***reg_alpha = L1 regularization term on weights.  \n",
    "    #***cv_num = The number of cross-validation folds that will be used in the parameter search process.\n",
    "    #***Randomized = A boolean value that decides if the first search you do for parameter searches is randomized or not.\n",
    "    #***n_iter = A number that is only used if Randomized is true. It essentially determines the number of minimum iterations \n",
    "    # RandomizedSearchCV will do before it stops testing random values of the variable in the distribution.\n",
    "    # I recommend len(xgb_param_vals) - 10.\n",
    "    #***plot2d = A boolean that will decide whether we show a 2d plot of error vs variable values. This will essentially help\n",
    "    # us determine a more effective and smaller range to look at after we do the search.\n",
    "    #***modelfit = A boolean that will determine if we run the modelfitXGB function to observe important features \n",
    "    # in the XGBR model\n",
    "    \n",
    "    #This prevents us from getting warnings that are unnecessary and don't add to anything.\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    #For RandomizedCVSearch\n",
    "    if Randomized:\n",
    "        #Create the dictionary object that is used in RandomizedSearchCV\n",
    "        param_distributions = {xgb_param:xgb_param_vals}\n",
    "        #Create the RandomizedSearchCV object\n",
    "        random_search_model = RandomizedSearchCV(estimator = xgb_alg,param_distributions = param_distributions,\n",
    "                                           n_iter = n_iter,scoring = scoring,n_jobs=-1,iid=False,cv=cv_num)\n",
    "        #Fit the data to our random search object\n",
    "        random_search_model.fit(train_X,train_y)\n",
    "        #These variables will be returned along with the model.\n",
    "        rs_results = pd.DataFrame(random_search_model.cv_results_) #The results of the random search\n",
    "        best_param_val = random_search_model.best_params_ #The best parameter\n",
    "        best_score_val = random_search_model.best_score_ #The best score associated with the best parameter\n",
    "        \n",
    "        #Store the returned values in a single list \n",
    "        return_values = [random_search_model,rs_results,best_param_val,best_score_val]\n",
    "        print(best_param_val, best_score_val)\n",
    "        #Create a 2d plot of mean_test_score (y) vs parameter values (x)\n",
    "        if plot2d:\n",
    "            rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "            param_name = 'param_'+ xgb_param\n",
    "            fig = px.scatter(rs_results,x=param_name,y='mean_test_score',color='mean_test_score')\n",
    "            fig.show()\n",
    "        #Create a bar plot showing the weights of the most important features so far. \n",
    "        if modelfit:\n",
    "            p_dict = {xgb_param:best_param_val[xgb_param]}\n",
    "            xgb_alg.set_params(**p_dict)\n",
    "            modelfitXGB(xgb_alg,train_X,train_y,cv_folds=cv_num)  \n",
    "        \n",
    "        return return_values \n",
    "    \n",
    "    \n",
    "    \n",
    "    #For a GridSearchCV\n",
    "    else:\n",
    "        #Create the dictionary object that is used in GridSearchCV\n",
    "        param_grid = {xgb_param:xgb_param_vals}\n",
    "        #Create the GridSearch object that will be fitted on the training_data.\n",
    "        grid_search_model = GridSearchCV(estimator = xgb_alg,param_grid = param_grid,scoring = scoring,\n",
    "                                        n_jobs = -1,iid = False, cv = cv_num)\n",
    "        #Fit the training data to the grid search object\n",
    "        grid_search_model.fit(train_X,train_y)\n",
    "        \n",
    "        #Save these following three variables to be returned later \n",
    "        gs_results = pd.DataFrame(grid_search_model.cv_results_) #The results of the grid search\n",
    "        best_param_val = grid_search_model.best_params_ #The best parameter value\n",
    "        best_score_val = grid_search_model.best_score_ #The best score associated with the best parameter value\n",
    "        \n",
    "        #Save the return values in a single list\n",
    "        return_values = [grid_search_model,gs_results,best_param_val,best_score_val]\n",
    "        print(best_param_val, best_score_val)\n",
    "        \n",
    "        #Create a 2d plot of mean_test_score (y) vs parameter values (x)\n",
    "        if plot2d:\n",
    "            rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "            param_name = 'param_'+ xgb_param\n",
    "            fig = px.scatter(gs_results,x=param_name,y='mean_test_score',color='mean_test_score')\n",
    "            fig.show()\n",
    "        #Create a bar plot showing the weights of the most important features so far. \n",
    "        if modelfit:\n",
    "            p_dict = {xgb_param:best_param_val[xgb_param]}\n",
    "            xgb_alg.set_params(**p_dict)\n",
    "            modelfitXGB(xgb_alg,train_X,train_y,cv_folds=cv_num)\n",
    "        return return_values \n",
    "    \n",
    "    \n",
    "    \n",
    "def TrainTestErrors(model,savefileName=None,save=True,train_X=strain_X2,train_y=train_y2,test_X=stest_X2,\n",
    "                    t_IDs=test_IDs,metric=metrics.mean_squared_log_error):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that will compute the train set errors and explained variances of a specific model.\n",
    "    #This will also compute the test predictions, and save them if save=True.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***model = The Machine Learning model.\n",
    "    #***savefileName = The string of the filename.\n",
    "    #***save = Boolean that determines whether we save the test predictions.\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only).\n",
    "    #***t_IDs = IDs for the testing data.\n",
    "    #***metric = the metric for which we are examining the error.\n",
    "    \n",
    "    #Fit the model to the training data \n",
    "    model_fit = model.fit(train_X,train_y)\n",
    "    \n",
    "    #Create predictions on the training set. Compute the error and explained variance.\n",
    "    train_pred = model_fit.predict(train_X)\n",
    "    train_error = metric(train_y,train_pred)\n",
    "    train_explained_var = metrics.explained_variance_score(train_y,train_pred)\n",
    "    \n",
    "    #If save = True, create predictions on the test set, and save the predictions using SaveFitModels() \n",
    "    if save:\n",
    "        test_pred = model_fit.predict(test_X)\n",
    "        SaveFitModels(test_pred,t_IDs,savefileName)\n",
    "    \n",
    "    return train_error, train_explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############# RIDGE REGRESSION ############\n",
    "###### NOTE: THIS MODEL WAS USED BERFORE I MADE STANDARDSCALER ########\n",
    "######## Ridge Regression with RandomizedCV ########\n",
    "# alpha_val = [0.001,0.005,0.01,0.025,0.05,0.075,0.1,0.25,0.5,0.75,1,2]\n",
    "# param_test1 = {'alpha':alpha_val}\n",
    "# RidgeGridSearch1 = GridSearchCV(estimator = Ridge(normalize=True),param_grid=param_test1,\n",
    "#                                         scoring='neg_mean_squared_error',n_jobs=-1,iid=False,cv=5)\n",
    "# RidgeGridSearch1.fit(train_X2,train_y2)\n",
    "# Ridge1_Results = pd.DataFrame(RidgeGridSearch1.cv_results_)\n",
    "# Ridge1_Results.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# RidgeGridSearch1.best_params_, RidgeGridSearch1.best_score_\n",
    "\n",
    "########Ridge Regression with different alpha values, doing by hand without the gridsearch\n",
    "######## alpha = 0.001 #########\n",
    "# ridge_reg1 = Ridge(alpha=0.001)\n",
    "# ridge_model1 = ridge_reg1.fit(train_X2,train_y2)\n",
    "# ridge_predictions1 = ridge_model1.predict(test_X2)\n",
    "# SaveFitModels(pred=ridge_predictions1, IDs=test_IDs, fileName = 'Ridge1_10222019.csv')\n",
    "\n",
    "\n",
    "######## alpha = 0.001, normalize = True #########\n",
    "# ridge_reg2 = Ridge(alpha=0.001,normalize=True)\n",
    "# ridge_model2 = ridge_reg2.fit(train_X2,train_y2)\n",
    "# ridge_predictions2 = ridge_model2.predict(test_X2)\n",
    "# SaveFitModels(pred=ridge_predictions2, IDs=test_IDs, fileName = 'Ridge2_10222019.csv')\n",
    "\n",
    "######## alpha = 0.025, normalize = True ########\n",
    "# ridge_reg3 = Ridge(alpha=0.025,normalize=True)\n",
    "# ridge_model3 = ridge_reg3.fit(train_X2,train_y2)\n",
    "# ridge_predictions3 = ridge_model3.predict(test_X2)\n",
    "# SaveFitModels(pred=ridge_predictions3, IDs=test_IDs, fileName = 'Ridge3_10222019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0,
     4,
     12,
     35,
     42,
     51,
     63,
     70,
     92,
     111,
     129,
     138
    ]
   },
   "outputs": [],
   "source": [
    "############ RANDOM FOREST REGRESSOR ############\n",
    "###### NOTE: THIS MODEL WAS USED BERFORE I MADE STANDARDSCALER ########\n",
    "#rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "\n",
    "######## The Beginner Model (NO TUNING)#########\n",
    "# rf_0 = RandomForestRegressor(random_state=5)\n",
    "# # RFRmodelfitCV(rf_0,train_X2,train_y2)\n",
    "# rf_0_model = rf_0.fit(train_X2,train_y2)\n",
    "# rf_0_pred = rf_0_model.predict(test_X2)\n",
    "# SaveFitModels(pred=rf_0_pred,IDs=test_IDs,fileName='RandomForestRegressor0_10222019.csv')\n",
    "\n",
    "\n",
    "############## TUNE N_ESTIMATORS, MAX_DEPTH, MIN_SAMPLES_LEAF #############\n",
    "###########################################################################\n",
    "####### ALREADY RAN ONCE, NOW LETS DO GRIDSEARCHCV WITH THESE VALUES ######\n",
    "###########################################################################\n",
    "# # r_exp1 = -4*np.random.rand(100)\n",
    "# # learn_rate1 = 10**r_exp1\n",
    "# n_est1 = range(20,151,10)\n",
    "# max_depth1 = range(3,11,1)\n",
    "# msl1 = range(10,101,10)\n",
    "# param_test1 = {'n_estimators':n_est1,'max_depth':max_depth1,'min_samples_leaf':msl1}\n",
    "# rfr_search1 = RandomizedSearchCV(estimator = RandomForestRegressor(random_state=5,min_samples_split=14,max_features='sqrt'),\n",
    "#                               param_distributions = param_test1,n_iter=60,scoring='neg_mean_squared_log_error',\n",
    "#                                  n_jobs=-1,iid=False,cv=5)\n",
    "# rfr_search1.fit(train_X2,train_y2)\n",
    "# rfr_results1 = pd.DataFrame(rfr_search1.cv_results_)\n",
    "# #rfr_results1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rfr1_plot3D = rfr_results1.loc[:,('param_n_estimators','param_min_samples_leaf','param_max_depth','mean_test_score')]\n",
    "# rfr_search1.best_params_,rfr_search1.best_score_\n",
    "###############################################################################\n",
    "####### ALREADY RAN ONCE, NOW LETS DO GRIDSEARCHCV WITH THESE VALUES ##########\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "########### Save these variables so that we do not need to run the randomized search\n",
    "# #pickle_list2 = [rfr1_plot3D,rfr_search1.best_params_,rfr_search1.best_score_]\n",
    "# #f = open('/Users/armenta/Kaggle/Housing Prices/Data Pipeline 2 Saved Variables/DP2_rfr1.pckl','wb')\n",
    "# #pickle.dump(pickle_list2,f)\n",
    "# #f.close()\n",
    "\n",
    "\n",
    "########### Open up these variables to look at them at a scatter plot\n",
    "# f = open('/Users/armenta/Kaggle/Housing Prices/Data Pipeline 2 Saved Variables/DP2_rfr1.pckl','rb')\n",
    "# pickle_list2 = pickle.load(f)\n",
    "# f.close()\n",
    "# rfr1_plot3D = pickle_list2[0]\n",
    "# rfr1_best_params_ = pickle_list2[1]\n",
    "# rfr1_best_score_ = pickle_list2[2]\n",
    "\n",
    "\n",
    "########## MATPLOTLIB 3D SCATTER ########\n",
    "#########################################\n",
    "# fig1 = plt.figure()\n",
    "# ax1 = fig1.add_subplot(111,projection='3d')\n",
    "# zdata1 = rfr1_plot3D.param_n_estimators\n",
    "# ydata1 = rfr1_plot3D.param_min_samples_leaf\n",
    "# xdata1 = rfr1_plot3D.param_max_depth\n",
    "# cdata1 = rfr1_plot3D.mean_test_score\n",
    "# plt1 = ax1.scatter(xdata1,ydata1,zdata1,c=cdata1,s=50,depthshade=True)\n",
    "# fig1.colorbar(plt1)\n",
    "\n",
    "\n",
    "########## PLOTLY.EXPRESS 3D SCATTER #########\n",
    "##############################################\n",
    "# fig = px.scatter_3d(rfr1_plot3D,x='param_n_estimators',y='param_min_samples_leaf',\n",
    "#                     z='param_max_depth',color='mean_test_score')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "########### GridSearchCV w/ n_estimators, max_depth, min_samples_leaf ###########\n",
    "##################################################################################\n",
    "# n_est2 = range(80,201,5)\n",
    "# max_depth2 = range(8,15,1)\n",
    "# msl_2 = range(5,16,1)\n",
    "# param_test2 = {'n_estimators':n_est2,'max_depth':max_depth2,'min_samples_leaf':msl_2}\n",
    "# rfr_gsearch1 = GridSearchCV(estimator = RandomForestRegressor(random_state=5,min_samples_split=14,max_features='sqrt'),\n",
    "#                            param_grid = param_test2, scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# rfr_gsearch1.fit(train_X2,train_y2)\n",
    "#\n",
    "###################Pull out the relevant variables that will be graphed.\n",
    "# gs_results1 = pd.DataFrame(rfr_gsearch1.cv_results_)\n",
    "# rfr_results2 = gs_results1.loc[:,('param_n_estimators','param_min_samples_leaf','param_max_depth','mean_test_score')]\n",
    "# #gs_results1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rfr_gsearch1.best_params_, rfr_gsearch1.best_score_\n",
    "#\n",
    "################## Plot the variables in a 3d scatter plot \n",
    "# fig = px.scatter_3d(rfr_results2,x='param_n_estimators',y='param_min_samples_leaf',\n",
    "#                    z='param_max_depth',color='mean_test_score')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "########### GridSearchCV w/ n_estimators, min_samples_leaf, min_samples_split ####\n",
    "##################################################################################\n",
    "# n_est3 = range(175,251,5)\n",
    "# msl_3 = range(2,8,1)\n",
    "# mss_3 = range(5,21,1)\n",
    "# param_test3 = {'n_estimators':n_est3,'min_samples_leaf':msl_3,'min_samples_split':mss_3}\n",
    "# rfr_gsearch2 = GridSearchCV(estimator = RandomForestRegressor(random_state=5,max_depth=13,max_features='sqrt'),\n",
    "#                            param_grid = param_test3, scoring = 'neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# rfr_gsearch2.fit(train_X2,train_y2)\n",
    "#\n",
    "#gs_results2 = pd.DataFrame(rfr_gsearch2.cv_results_)\n",
    "#rfr_results3 = gs_results2.loc[:,('param_n_estimators','param_min_samples_leaf','param_min_samples_split','mean_test_score')]\n",
    "#rfr_gsearch2.best_params_, rfr_gsearch2.best_score_\n",
    "#\n",
    "# fig = px.scatter_3d(rfr_results3,x='param_n_estimators',y='param_min_samples_leaf',\n",
    "#                     z='param_min_samples_split',color='mean_test_score')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "################ GridSearchCV w/ min_samples_split, max_features #################\n",
    "##################################################################################\n",
    "# mss_4 = range(3,11,1)\n",
    "# mf_4 = range(6,31,2)\n",
    "# param_test4 = {'min_samples_split':mss_4,'max_features':mf_4}\n",
    "# rfr_gsearch3 = GridSearchCV(estimator = RandomForestRegressor(random_state=5,max_depth=13,n_estimators=225,min_samples_split=2),\n",
    "#                            param_grid = param_test4, scoring = 'neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# rfr_gsearch3.fit(train_X2,train_y2)\n",
    "#\n",
    "# gs_results3 = pd.DataFrame(rfr_gsearch3.cv_results_)\n",
    "# rfr_results4 = gs_results3.loc[:,('param_min_samples_split','param_max_features','mean_test_score')]\n",
    "# rfr_gsearch3.best_params_, rfr_gsearch3.best_score_\n",
    "#\n",
    "# fig = px.scatter(rfr_results4,x='param_min_samples_split',y='param_max_features',\n",
    "#                  color='mean_test_score')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "#################### TUNED RANDOMFORESTREGRESSOR MODEL #######################\n",
    "#rfr_tuned = RandomForestRegressor(random_state=5,max_depth=13,n_estimators=225,min_samples_leaf=2,\n",
    "#                                 min_samples_split=4,max_features=26)\n",
    "# #modelfitCV(rfr_tuned,train_X2,train_y2)\n",
    "# rfr_model4 = rfr_tuned.fit(train_X2,train_y2)\n",
    "# rfr_pred4 = rfr_model4.predict(test_X2)\n",
    "# SaveFitModels(pred=rfr_pred4,IDs=test_IDs,fileName='RandomForestRegressor4_10232019.csv')\n",
    "\n",
    "\n",
    "############### VISUALIZING ONE OF THE TREES IN A RANDOMFOREST ##############\n",
    "#NOTE: RANDOM FORESTS IS AN ENSEMBLE METHOD WHERE NOT A SINGLE ESTIMATOR IS USED,\n",
    "#BUT ALL OF THEM ARE COMBINED TOGETHER TO CREATE A BETTER MODEL THAN ANY SINGLE TREE.\n",
    "# rfr_tuned = RandomForestRegressor(random_state=5,max_depth=13,n_estimators=225,min_samples_leaf=2,\n",
    "#                                  min_samples_split=4,max_features=26)\n",
    "# rfr_tuned.fit(train_X2,train_y2)\n",
    "# estimator = rfr_tuned.estimators_[1] #There are 225 trees in the random forest.\n",
    "#\n",
    "# export_graphviz(estimator,out_file='rfr_tree1.dot',feature_names=train_X2.columns,\n",
    "#                 class_names='SalePrice',rounded=True,proportion=False,precision=2,filled=True)\n",
    "#\n",
    "# call(['dot','-Tpng', 'rfr_tree1.dot', '-o', 'rfr_tree1.png'])\n",
    "#\n",
    "# Image(filename = 'rfr_tree1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [
     0,
     10,
     15,
     25,
     34,
     43,
     51,
     62,
     68,
     74,
     87,
     95,
     102,
     109,
     120,
     139,
     155,
     169,
     183,
     203,
     212,
     221,
     231,
     240,
     246,
     254,
     262,
     270,
     280,
     290,
     298
    ]
   },
   "outputs": [],
   "source": [
    "############ XGBRegressor (Standardized Train/Dev Sets Split) #############\n",
    "#****NOTE: I changed the inputs of the XGBRModelTune to be different datasets, \n",
    "# so change the inputs of the functions if you want to rerun this code.\n",
    "\n",
    "\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "\n",
    "########################### PART I ############################\n",
    "\n",
    "########## Round I - n_estimators #########\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1, n_estimators=1000, max_depth=6, min_child_weight=4, subsample=0.6, \n",
    "#                    colsample_bytree=0.109, objective='reg:squarederror',seed=20)\n",
    "#modelfitXGB(xgb1,strain_X,strain_y)\n",
    "\n",
    "########## Example of comparing train and dev sets #########\n",
    "# xgb1_model = xgb1.fit(strain_X,strain_y)\n",
    "# train_p1 = xgb1_model.predict(strain_X)\n",
    "# train_error = metrics.mean_squared_log_error(strain_y,train_p1)\n",
    "# dev_p1 = xgb1_model.predict(sdev_X)\n",
    "# dev_error = metrics.mean_squared_log_error(sdev_y,dev_p1)\n",
    "# print(train_error, dev_error)\n",
    "# xgb1_predict = xgb1_model.predict(stest_X)\n",
    "# SaveFitModels(pred=xgb1_predict,IDs=test_IDs,fileName='xgb1_11062019.csv')\n",
    "\n",
    "########## Round II - max_depth ##########\n",
    "# #Instantiate the new model with updated n_estimators value\n",
    "# xgb2 = XGBRegressor(learning_rate=0.1, n_estimators=156, min_child_weight=4, subsample=0.6, colsample_bytree=0.109,\n",
    "#                    objective='reg:squarederror',seed=20)\n",
    "# #Create the range that the grid search will be performed over \n",
    "# max_depth_range = range(2,16,1)\n",
    "# model2, model_results2, best_max_depth, best_score2 = XGBRModelTune(xgb_alg=xgb2, xgb_param = 'max_depth',\n",
    "#                                                                     xgb_param_vals = max_depth_range, modelfit = False)\n",
    "\n",
    "######### Round III - min_child_weight ##########\n",
    "# #Instantiate the new model with updated max_depth\n",
    "# xgb3 = XGBRegressor(learning_rate=0.1, n_estimators=156, max_depth=4, subsample=0.6, colsample_bytree=0.109, \n",
    "#                    objective ='reg:squarederror',seed=20)\n",
    "# #Create the range that the grid search will be performed over\n",
    "# min_child_weight_range = range(1,11,1)\n",
    "# model3, model_results3, best_min_child_weight, best_score3 = XGBRModelTune(xgb3,'min_child_weight',min_child_weight_range,\n",
    "#                                                                           modelfit = False)\n",
    "\n",
    "######### Round IV - gamma #########\n",
    "# #Instantiate the new model with updated gamma\n",
    "# xgb4 = XGBRegressor(learning_rate=0.1,n_estimators=156,max_depth=4,min_child_weight=4,subsample=0.6,\n",
    "#                     colsample_bytree=0.109,objective='reg:squarederror',seed=20)\n",
    "# #Create the range that the grid search will be performed over \n",
    "# gamma_range = range(0,21,1)\n",
    "# model4, model_results4, best_gamma, best_score4 = XGBRModelTune(xgb4,'gamma',gamma_range, modelfit=False)\n",
    "\n",
    "######### Round IV - gamma pt.2 #########\n",
    "# #Copy and pasted the one above because its the exact same parameters.\n",
    "# xgb4 = XGBRegressor(learning_rate=0.1,n_estimators=156,max_depth=4,min_child_weight=4,subsample=0.6,\n",
    "#                     colsample_bytree=0.109,objective='reg:squarederror',seed=20)\n",
    "# exp_gamma = -2*np.random.rand(60) #Randomly sample the exponent for gammas range\n",
    "# gamma_range = 10**exp_gamma #Randomly will pick values between 0.01 -> 1\n",
    "# n_iterations = len(gamma_range2) - 10\n",
    "#\n",
    "# model4, model_results4, best_gamma, best_score = XGBRModelTune(xgb4,'gamma',gamma_range,Randomized=True,\n",
    "#                                                                  n_iter = n_iterations,modelfit=False)\n",
    "\n",
    "######### Round V - subsample #########\n",
    "# xgb5 = XGBRegressor(learning_rate=0.1,n_estimators=156,max_depth=4,min_child_weight=4,colsample_bytree=0.109,\n",
    "#                     objective='reg:squarederror',seed=20)\n",
    "# subsample_range = np.arange(0.05,1.05,0.05)\n",
    "# model5, model_results5, best_subsample, best_score5 = XGBRModelTune(xgb5,'subsample',subsample_range,modelfit=False)\n",
    "\n",
    "######## Round VI - colsample_bytree ########\n",
    "# xgb6 = XGBRegressor(learning_rate=0.1,n_estimators=156,max_depth=4,min_child_weight=4,objective='reg:squarederror',seed=20)\n",
    "# colsample_bytree_range = np.arange(0.10,1.05,0.05)\n",
    "# model6, model_results6, best_colsample_bytree, best_score6 = XGBRModelTune(xgb6,'colsample_bytree',colsample_bytree_range,\n",
    "#                                                                           modelfit=False)\n",
    "\n",
    "######## Round VII - n_estimators *BUST* *NOT USABLE DATA* ########\n",
    "# xgb7 = XGBRegressor(learning_rate=0.1,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=0.25,\n",
    "#                     objective='reg:squarederror',seed=20)\n",
    "# n_estimators_range = range(100,305,5)\n",
    "# model7, model_results7, best_n_estimators, best_score7 = XGBRModelTune(xgb7,'n_estimators',n_estimators_range,modelfit=False)\n",
    "#\n",
    "######## Round VII - n_estimators pt.2 #######\n",
    "# xgb7 = XGBRegressor(learning_rate=0.1,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=0.25,\n",
    "#                     objective='reg:squarederror',seed=20)\n",
    "# n_estimators_range = range(285,301,1)\n",
    "# model7, model_results7, best_n_estimators, best_score7 = XGBRModelTune(xgb7,'n_estimators',n_estimators_range,\n",
    "#                                                                        modelfit=False)\n",
    "\n",
    "######## Round VIII - learning_rate #########\n",
    "# xgb8 = XGBRegressor(n_estimators=500,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=0.25,\n",
    "#                    objective='reg:squarederror',seed=20)\n",
    "# exp_LR = -3*np.random.rand(100)\n",
    "# learning_rate_range = 10**exp_LR\n",
    "# model8, model_results8, best_learning_rate, best_score8 = XGBRModelTune(xgb8,'learning_rate',learning_rate_range,\n",
    "#                                                                        Randomized=True, n_iter=50, modelfit=False)\n",
    "\n",
    "######## Round VII REDUX - n_estimators ############\n",
    "# xgb7_redux = XGBRegressor(learning_rate = best_learning_rate['learning_rate'],max_depth=4,min_child_weight=4,\n",
    "#                           subsample=1,colsample_bytree=0.25,objective='reg:squarederror',seed=20)\n",
    "# n_estimators_range = range(400,605,5)\n",
    "# model7, model_results7, best_n_estimators, best_score7 = XGBRModelTune(xgb7_redux, 'n_estimators',n_estimators_range,\n",
    "#                                                                       modelfit=False)\n",
    "\n",
    "######## SAVE THE TUNED VARIABLES FROM PART I ########\n",
    "# pickle_dict1 = {'learning_rate':best_learning_rate['learning_rate'],'n_estimators':595,'max_depth':4,'min_child_weight':4,\n",
    "#                'subsample':1,'colsample_bytree':0.25}\n",
    "# f = open('/Users/armenta/Kaggle/Housing Prices/Data Pipeline 2 Saved Variables/DP2_xgb_tuned1_vars.pckl','wb')\n",
    "# pickle.dump(pickle_dict1,f)\n",
    "# f.close()\n",
    "\n",
    "######## OPEN THE TUNED VARIABLES FROM PART I ########\n",
    "# f = open('/Users/armenta/Kaggle/Housing Prices/Data Pipeline 2 Saved Variables/DP2_xgb_tuned1_vars.pckl','rb')\n",
    "# pickle_dict1 = pickle.load(f)\n",
    "# f.close()\n",
    "# best_lr = pickle_dict1['learning_rate']\n",
    "# n_est = pickle_dict1['n_estimators']\n",
    "# max_d = pickle_dict1['max_depth']\n",
    "# min_cw = pickle_dict1['min_child_weight']\n",
    "# subsamp = pickle_dict1['subsample']\n",
    "# colsamp_bytree = pickle_dict1['colsample_bytree']\n",
    "\n",
    "######## LOOKING AT TRAIN, DEV, AND TEST ERROR #########\n",
    "# xgb_tuned1 = XGBRegressor(learning_rate = best_lr,n_estimators=n_est,max_depth=max_d,min_child_weight=min_cw,\n",
    "#                           subsample=subsamp,colsample_bytree=colsamp_bytree,objective='reg:squarederror',seed=20)\n",
    "#modelfitXGB(xgb_tuned1,strain_X,strain_y) # I ran this to look at the feature importances of this first tuned model.\n",
    "# xgb_tunedmodel1 = xgb_tuned1.fit(strain_X,strain_y)\n",
    "# train_pred1 = xgb_tunedmodel1.predict(strain_X)\n",
    "# train_error = metrics.mean_squared_log_error(strain_y,train_pred1)\n",
    "# dev_pred1 = xgb_tunedmodel1.predict(sdev_X)\n",
    "# dev_error = metrics.mean_squared_log_error(sdev_y,dev_pred1)\n",
    "# print(train_error, dev_error)\n",
    "# test_pred1 = xgb_tunedmodel1.predict(stest_X)\n",
    "# SaveFitModels(test_pred1,test_IDs,'xgb_tuned1_11072019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### PART II ############################\n",
    "\n",
    "########## Round I - reg_lambda #########\n",
    "# xgb1 = XGBRegressor(learning_rate=best_lr, n_estimators=n_est, max_depth=max_d, min_child_weight=min_cw, \n",
    "#                     subsample=subsamp, colsample_bytree=colsamp_bytree, objective='reg:squarederror',seed=20)\n",
    "# lambda_range = range(1,21,1)\n",
    "# model1, model_results1, best_lambda, best_score1 = XGBRModelTune(xgb1, 'reg_lambda',lambda_range)\n",
    "#\n",
    "# # Testing values between 0.001 -> 1\n",
    "########## Round I - reg_lambda pt.2 #########\n",
    "# xgb1 = XGBRegressor(learning_rate=best_lr, n_estimators=n_est, max_depth=max_d, min_child_weight=min_cw, \n",
    "#                      subsample=subsamp, colsample_bytree=colsamp_bytree, objective='reg:squarederror',seed=20)\n",
    "# lambda_exp = -4*np.random.rand(100) #Going to try out much smaller values.\n",
    "# lambda_range = 10**lambda_exp\n",
    "# model1_2, model_results1_2, best_lambda, best_score1_2 = XGBRModelTune(xgb1, 'reg_lambda', lambda_range, \n",
    "#                                                                        Randomized = True,n_iter = 70, \n",
    "#                                                                        modelfit = False)\n",
    "\n",
    "########## Round II - reg_alpha ##########\n",
    "# xgb2 = XGBRegressor(learning_rate=best_lr, n_estimators=n_est, max_depth=max_d, min_child_weight=min_cw, \n",
    "#                      subsample=subsamp, colsample_bytree=colsamp_bytree, reg_lambda = 6, objective='reg:squarederror',\n",
    "#                      seed=20)\n",
    "# alpha_range = [0.01,0.05,0.1,0.25,0.5,0.75,1,5,10,15,20]\n",
    "# model2, model_results2, best_alpha, best_score2 = XGBRModelTune(xgb2, 'reg_alpha',alpha_range,modelfit=False)\n",
    "#\n",
    "# # Play around with alpha a little more.\n",
    "# xgb2 = XGBRegressor(learning_rate=best_lr, n_estimators=n_est, max_depth=max_d, min_child_weight=min_cw, \n",
    "#                       subsample=subsamp, colsample_bytree=colsamp_bytree, reg_lambda = 6, objective='reg:squarederror',\n",
    "#                       seed=20)\n",
    "# alpha_range = range(15,41,1)\n",
    "# model2, model_results2, best_alpha, best_score2 = XGBRModelTune(xgb2,'reg_alpha',alpha_range)\n",
    "\n",
    "########## Check Train, Dev, and Test Errors ##########\n",
    "# xgb_tuned2 = XGBRegressor(learning_rate=best_lr, n_estimators=n_est, max_depth=max_d, min_child_weight=min_cw, \n",
    "#                       subsample=subsamp, colsample_bytree=colsamp_bytree, reg_lambda = 6, reg_alpha=17,\n",
    "#                       objective='reg:squarederror',seed=20)\n",
    "# train_tuned_error2, train_exp_var2, dev_tuned_error2, dev_exp_var2 = TrainDevTestErrors(xgb_tuned2,'xgb_tuned2_11092019.csv')\n",
    "# train_tuned_error2, dev_tuned_error2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################## PART III ############################\n",
    "# I will use booster = 'dart', and re-tune the parameters to see if it works better.\n",
    "\n",
    "############ Round I - n_estimators ###########\n",
    "# xgb_dart1 = XGBRegressor(learning_rate=best_lr, max_depth=max_d, min_child_weight=min_cw, subsample=subsamp, \n",
    "#                          colsample_bytree=colsamp_bytree, reg_lambda = 6, reg_alpha=17,objective='reg:squarederror',\n",
    "#                          booster='dart',seed=20)\n",
    "# n_est_range = range(250,605,5)\n",
    "# model_dart1, model_results_dart1, best_n_est, best_score_dart1 = XGBRModelTune(xgb_dart1,'n_estimators',n_est_range,modelfit=False)\n",
    "#\n",
    "############ Round I - n_estimators pt.2 ###########\n",
    "# xgb_dart1 = XGBRegressor(learning_rate=best_lr, max_depth=max_d, min_child_weight=min_cw, subsample=subsamp, \n",
    "#                          colsample_bytree=colsamp_bytree, reg_lambda = 6, reg_alpha=17,objective='reg:squarederror',\n",
    "#                          booster='dart',seed=20)\n",
    "# n_est_range2 = range(585,651,1)\n",
    "# model_dart1_2, model_results_dart1_2, best_n_est2, best_score_dart1_2 = XGBRModelTune(xgb_dart1,'n_estimators',n_est_range2,modelfit=False)\n",
    "#\n",
    "############ Check the top 10 n_est values on train and dev sets ############\n",
    "#These are the top 10 n_estimators values.\n",
    "#Turn this into a small function.\n",
    "# top_n_est = pd.Series(model_results_dart1_2.loc[model_results_dart1_2.rank_test_score<=10,'param_n_estimators']).reset_index(drop=True)\n",
    "# XGBTrainDevComparisons(xgb_dart1,'n_estimators',top_n_est)\n",
    "\n",
    "############ Round II - max_depth ###########\n",
    "# xgb_dart2 = XGBRegressor(learning_rate=best_lr, n_estimators=650, min_child_weight=4,subsample=1,colsample_bytree=0.25,\n",
    "#                         reg_lambda=6, reg_alpha=17,objective='reg:squarederror',booster='dart',seed=20)\n",
    "# max_depth_range = range(3,11,1)\n",
    "# model_dart2, model_results_dart2, best_max_d2, best_score_dart2 = XGBRModelTune(xgb_dart2,'max_depth',max_depth_range,modelfit=False)\n",
    "#\n",
    "# top_max_d = TDComp(model_results_dart2,'param_max_depth',8)\n",
    "# XGBTrainDevComparisons(xgb_dart2,'max_depth',top_max_d)\n",
    "\n",
    "############ Round III - min_child_weight ##########\n",
    "# xgb_dart3 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, subsample=1, colsample_bytree=0.25,\n",
    "#                         reg_lambda=6, reg_alpha=17, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# min_cw_range = range(1,16,1)\n",
    "# model_dart3, model_results_dart3, best_mincw, best_score_dart3 = XGBRModelTune(xgb_dart3, 'min_child_weight',min_cw_range,modelfit=False)\n",
    "#\n",
    "# top_mcw = TDComp(model_results_dart3,'param_min_child_weight',7)\n",
    "# XGBTrainDevComparisons(xgb_dart3,'min_child_weight',top_mcw)\n",
    "\n",
    "############ Round IV - gamma ##########\n",
    "# xgb_dart4 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, subsample=1, \n",
    "#                          colsample_bytree=0.25,reg_lambda=6, reg_alpha=17, objective='reg:squarederror',\n",
    "#                          booster='dart',seed=20)\n",
    "# gamma_range = np.arange(0.0,1.05,0.05)\n",
    "# model_dart4, model_results_dart4, best_gamma, best_score_dart4 = XGBRModelTune(xgb_dart4, 'gamma', gamma_range, modelfit=False)\n",
    "#\n",
    "# top_gamma = TDComp(model_results_dart4,'param_gamma',5)\n",
    "# XGBTrainDevComparisons(xgb_dart4,'gamma',top_gamma)\n",
    "\n",
    "############ Round V - subsample ##########\n",
    "# xgb_dart5 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, colsample_bytree=0.25,\n",
    "#                          reg_lambda=6, reg_alpha=17, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# subsample_range = np.arange(0.30,1.05,0.05)\n",
    "# model_dart5, model_results_dart5, best_subsample, best_score_dart5 = XGBRModelTune(xgb_dart5, 'subsample', subsample_range, modelfit=False)\n",
    "#\n",
    "# top_subsample = TDComp(model_results_dart5,'param_subsample',10)\n",
    "# XGBTrainDevComparisons(xgb_dart5,'subsample',top_subsample)\n",
    "\n",
    "############ CHECK HOW WE DO ON THE TEST SET NOW ############\n",
    "# xgb_dart_tuned1 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, subsample=0.85,\n",
    "#                                colsample_bytree=0.25,reg_lambda=6, reg_alpha=17, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# train_dart_error1, train_dart_exp_var1, dev_dart_error1, dev_dart_exp_var1 = TrainDevTestErrors(xgb_dart_tuned1,'xgb_dart_tuned1_11132019.csv')\n",
    "#train_dart_error1, dev_dart_error1\n",
    "\n",
    "############ Round VI - colsample_bytree ###########\n",
    "# xgb_dart6 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, subsample=0.85,\n",
    "#                          reg_lambda=6, reg_alpha=17, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# colsamp_range = np.arange(0.10,1.05,0.05)\n",
    "# model_dart6, model_results_dart6, best_colsamp, best_score_dart6 = XGBRModelTune(xgb_dart6,'colsample_bytree',colsamp_range,modelfit=False)\n",
    "#top_colsamp = TDComp(model_results_dart6,'param_colsample_bytree',10)\n",
    "#XGBTrainDevComparisons(xgb_dart6,'colsample_bytree',top_colsamp)\n",
    "\n",
    "############ Round VII - lambda ###########\n",
    "# xgb_dart7 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, subsample=0.85,\n",
    "#                          colsample_bytree=0.2,reg_alpha=17, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# lambda_range = range(1,21,1)\n",
    "# model_dart7, model_results_dart7, best_lambda, best_score_dart7 = XGBRModelTune(xgb_dart7,'reg_lambda',lambda_range,modelfit=False)\n",
    "# top_lambda = TDComp(model_results_dart7,'param_reg_lambda',10)\n",
    "# XGBTrainDevComparisons(xgb_dart7,'reg_lambda',top_lambda)\n",
    "\n",
    "############ Round VIII - alpha ###########\n",
    "# xgb_dart8 = XGBRegressor(learning_rate=best_lr, n_estimators=650, max_depth=7, min_child_weight=14, subsample=0.85,\n",
    "#                          colsample_bytree=0.2, reg_lambda=11, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# alpha_range = range(1,11,1)\n",
    "# model_dart8, model_results_dart8, best_alpha, best_score_dart8 = XGBRModelTune(xgb_dart8,'reg_alpha',alpha_range,modelfit=False)\n",
    "# top_alpha = TDComp(model_results_dart8,'param_reg_alpha',5)\n",
    "# XGBTrainDevComparisons(xgb_dart8,'reg_alpha',top_alpha)\n",
    "\n",
    "############ Round IX - learning_rate ##########\n",
    "# xgb_dart9 = XGBRegressor(n_estimators=650, max_depth=7, min_child_weight=14, subsample=0.85, colsample_bytree=0.2, \n",
    "#                          reg_lambda=11, reg_alpha=4, objective='reg:squarederror',booster='dart',seed=20)\n",
    "# lr_exp = -3*np.random.rand(100)\n",
    "# learning_rate_range = 10**lr_exp\n",
    "# model_dart9, model_results_dart9, best_learnrate, best_score_dart9 = XGBRModelTune(xgb_dart9,'learning_rate',learning_rate_range,\n",
    "#                                                                                    Randomized=True,n_iter=60,modelfit=False)\n",
    "# top_learning_rate = TDComp(model_results_dart9,'param_learning_rate',20)\n",
    "# XGBTrainDevComparisons(xgb_dart9,'learning_rate',top_learning_rate)\n",
    "\n",
    "############ BEST PARAMETERS ############\n",
    "# lr_dart = 0.02467365\n",
    "# n_est_dart = 650\n",
    "# max_d_dart = 7\n",
    "# min_cw_dart = 14\n",
    "# subsample_dart = 0.85\n",
    "# colsamp_dart = 0.2\n",
    "# lambda_dart = 11\n",
    "# alpha_dart = 4\n",
    "\n",
    "############ FINAL TUNED DART MODEL ############\n",
    "# xgb_dart_tuned2 = XGBRegressor(learning_rate=lr_dart, n_estimators=n_est_dart, max_depth=max_d_dart, min_child_weight=min_cw_dart,\n",
    "#                               subsample=subsample_dart, colsample_bytree=colsamp_dart, reg_lambda=lambda_dart, reg_alpha=alpha_dart,\n",
    "#                               objective='reg:squarederror', booster='dart',seed=20)\n",
    "# modelfitXGB(xgb_dart_tuned1, strain_X, strain_y)\n",
    "#train_err_dart2, train_exp_var_dart2, dev_err_dart2, dev_exp_var_dart2 = TrainDevTestErrors(xgb_dart_tuned2,'xgb_dart_tuned2_11132019.csv')\n",
    "#train_err_dart2, dev_err_dart2\n",
    "\n",
    "############## PLAYING AROUND ###############\n",
    "#Check the same model on unstandardized data\n",
    "#This performed better than the regular standardized data set. I wonder how it will performed on the unsplit data?\n",
    "# xgb_dart_tuned2 = XGBRegressor(learning_rate=lr_dart, n_estimators=n_est_dart, max_depth=max_d_dart, min_child_weight=min_cw_dart,\n",
    "#                               subsample=subsample_dart, colsample_bytree=colsamp_dart, reg_lambda=lambda_dart, reg_alpha=alpha_dart,\n",
    "#                               objective='reg:squarederror', booster='dart',seed=20)\n",
    "# t_err_NS, t_ev_NS, d_err_NS, d_ev_NS = TrainDevTestErrors(xgb_dart_tuned2,'xgb_dart_tuned2_NS_11132019.csv',train_X=train_X3,\n",
    "#                                                           train_y=train_y3,dev_X=dev_X,dev_y=dev_y,test_X=test_X3)\n",
    "# t_err_NS, d_err_NS\n",
    "#\n",
    "# t_err_NS2, t_ev_NS2, d_err_NS2, d_ev_NS2 = TrainDevTestErrors(xgb_dart_tuned2,'xgb_dart_tuned2_NS2_11132019.csv',train_X=train_X2,\n",
    "#                                                                train_y=train_y2,test_X=test_X2)\n",
    "# t_err_NS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [
     6,
     12,
     18,
     24,
     32,
     38,
     44,
     54,
     61,
     70,
     86,
     94,
     101,
     108,
     122,
     128,
     138,
     144,
     151,
     165,
     172,
     186,
     193,
     221,
     228,
     235,
     249,
     256,
     270
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.934918</td>\n",
       "      <td>0.126686</td>\n",
       "      <td>0.021812</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>15</td>\n",
       "      <td>{'reg_lambda': 15}</td>\n",
       "      <td>-0.014262</td>\n",
       "      <td>-0.020268</td>\n",
       "      <td>-0.014986</td>\n",
       "      <td>-0.016506</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.526330</td>\n",
       "      <td>1.763654</td>\n",
       "      <td>0.028285</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>16</td>\n",
       "      <td>{'reg_lambda': 16}</td>\n",
       "      <td>-0.014388</td>\n",
       "      <td>-0.020473</td>\n",
       "      <td>-0.015018</td>\n",
       "      <td>-0.016626</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.456749</td>\n",
       "      <td>0.720290</td>\n",
       "      <td>0.028786</td>\n",
       "      <td>0.004045</td>\n",
       "      <td>17</td>\n",
       "      <td>{'reg_lambda': 17}</td>\n",
       "      <td>-0.014265</td>\n",
       "      <td>-0.020254</td>\n",
       "      <td>-0.015479</td>\n",
       "      <td>-0.016666</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.306814</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.024281</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>18</td>\n",
       "      <td>{'reg_lambda': 18}</td>\n",
       "      <td>-0.014181</td>\n",
       "      <td>-0.020237</td>\n",
       "      <td>-0.015551</td>\n",
       "      <td>-0.016656</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26.357809</td>\n",
       "      <td>0.092393</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>19</td>\n",
       "      <td>{'reg_lambda': 19}</td>\n",
       "      <td>-0.014608</td>\n",
       "      <td>-0.019852</td>\n",
       "      <td>-0.015502</td>\n",
       "      <td>-0.016654</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.227064</td>\n",
       "      <td>0.340867</td>\n",
       "      <td>0.021986</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>20</td>\n",
       "      <td>{'reg_lambda': 20}</td>\n",
       "      <td>-0.014452</td>\n",
       "      <td>-0.019817</td>\n",
       "      <td>-0.015463</td>\n",
       "      <td>-0.016577</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26.458188</td>\n",
       "      <td>0.205551</td>\n",
       "      <td>0.025080</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>21</td>\n",
       "      <td>{'reg_lambda': 21}</td>\n",
       "      <td>-0.014290</td>\n",
       "      <td>-0.020034</td>\n",
       "      <td>-0.015459</td>\n",
       "      <td>-0.016595</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>27.964081</td>\n",
       "      <td>0.621432</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>22</td>\n",
       "      <td>{'reg_lambda': 22}</td>\n",
       "      <td>-0.014091</td>\n",
       "      <td>-0.020045</td>\n",
       "      <td>-0.015832</td>\n",
       "      <td>-0.016656</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33.301161</td>\n",
       "      <td>0.192749</td>\n",
       "      <td>0.032615</td>\n",
       "      <td>0.006115</td>\n",
       "      <td>23</td>\n",
       "      <td>{'reg_lambda': 23}</td>\n",
       "      <td>-0.014242</td>\n",
       "      <td>-0.020479</td>\n",
       "      <td>-0.015687</td>\n",
       "      <td>-0.016803</td>\n",
       "      <td>0.002665</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35.236221</td>\n",
       "      <td>0.973561</td>\n",
       "      <td>0.049463</td>\n",
       "      <td>0.016523</td>\n",
       "      <td>24</td>\n",
       "      <td>{'reg_lambda': 24}</td>\n",
       "      <td>-0.014153</td>\n",
       "      <td>-0.020225</td>\n",
       "      <td>-0.015887</td>\n",
       "      <td>-0.016755</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29.362740</td>\n",
       "      <td>8.296346</td>\n",
       "      <td>0.017894</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>25</td>\n",
       "      <td>{'reg_lambda': 25}</td>\n",
       "      <td>-0.014166</td>\n",
       "      <td>-0.020022</td>\n",
       "      <td>-0.015702</td>\n",
       "      <td>-0.016630</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       21.934918      0.126686         0.021812        0.000652   \n",
       "1       24.526330      1.763654         0.028285        0.005643   \n",
       "2       26.456749      0.720290         0.028786        0.004045   \n",
       "3       27.306814      0.270677         0.024281        0.001487   \n",
       "4       26.357809      0.092393         0.023848        0.000196   \n",
       "5       26.227064      0.340867         0.021986        0.000652   \n",
       "6       26.458188      0.205551         0.025080        0.004626   \n",
       "7       27.964081      0.621432         0.032292        0.004307   \n",
       "8       33.301161      0.192749         0.032615        0.006115   \n",
       "9       35.236221      0.973561         0.049463        0.016523   \n",
       "10      29.362740      8.296346         0.017894        0.003590   \n",
       "\n",
       "   param_reg_lambda              params  split0_test_score  split1_test_score  \\\n",
       "0                15  {'reg_lambda': 15}          -0.014262          -0.020268   \n",
       "1                16  {'reg_lambda': 16}          -0.014388          -0.020473   \n",
       "2                17  {'reg_lambda': 17}          -0.014265          -0.020254   \n",
       "3                18  {'reg_lambda': 18}          -0.014181          -0.020237   \n",
       "4                19  {'reg_lambda': 19}          -0.014608          -0.019852   \n",
       "5                20  {'reg_lambda': 20}          -0.014452          -0.019817   \n",
       "6                21  {'reg_lambda': 21}          -0.014290          -0.020034   \n",
       "7                22  {'reg_lambda': 22}          -0.014091          -0.020045   \n",
       "8                23  {'reg_lambda': 23}          -0.014242          -0.020479   \n",
       "9                24  {'reg_lambda': 24}          -0.014153          -0.020225   \n",
       "10               25  {'reg_lambda': 25}          -0.014166          -0.020022   \n",
       "\n",
       "    split2_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           -0.014986        -0.016506        0.002677                1  \n",
       "1           -0.015018        -0.016626        0.002732                4  \n",
       "2           -0.015479        -0.016666        0.002585                9  \n",
       "3           -0.015551        -0.016656        0.002593                8  \n",
       "4           -0.015502        -0.016654        0.002291                6  \n",
       "5           -0.015463        -0.016577        0.002328                2  \n",
       "6           -0.015459        -0.016595        0.002479                3  \n",
       "7           -0.015832        -0.016656        0.002499                7  \n",
       "8           -0.015687        -0.016803        0.002665               11  \n",
       "9           -0.015887        -0.016755        0.002554               10  \n",
       "10          -0.015702        -0.016630        0.002479                5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ XGBRegressor Pt.2 (strain_X2,train_y2,stest_X2) ############\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "\n",
    "####################### PART I ######################\n",
    "\n",
    "############ Round I - n_est ###########\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1, max_depth=5, min_child_weight=3, subsample=0.6, colsample_bytree=0.109, \n",
    "#                          objective='reg:squarederror',seed=7)\n",
    "# n_est_range = range(100,305,5)\n",
    "# model1, model_results1, best_n_est, best_score1 = XGBRModelTune(xgb1,'n_estimators',n_est_range)\n",
    "\n",
    "############ Round II - max_depth ##########\n",
    "# xgb2 = XGBRegressor(learning_rate=0.1,n_estimators=240,min_child_weight=3,subsample=0.6,colsample_bytree=0.109,\n",
    "#                    objective='reg:squarederror',seed=7)\n",
    "# max_d_range = range(3,13,1)\n",
    "#model2, model_results2, best_max_d, best_score2 = XGBRModelTune(xgb2,'max_depth',max_d_range)\n",
    "\n",
    "############ Round III - min_child_weight #########\n",
    "# xgb3 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,subsample=0.6,colsample_bytree=0.109,\n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# mcw_range = range(1,11,1)\n",
    "# model3, model_results3, best_mcw, best_score3 = XGBRModelTune(xgb3,'min_child_weight',mcw_range)\n",
    "\n",
    "############ Round IV - gamma ###########\n",
    "# xgb4 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,min_child_weight=4,subsample=0.6,\n",
    "#                     colsample_bytree=0.109,objective='reg:squarederror',seed=7)\n",
    "# #gamma_range1 = [0,1,5,10,20,50,100]\n",
    "# gamma_exp = -5**np.random.rand(100)\n",
    "# gamma_range2 = 10**gamma_exp\n",
    "# model4, model_results4, best_gamma, best_score4 = XGBRModelTune(xgb4,'gamma',gamma_range2,Randomized=True,n_iter=50)\n",
    "\n",
    "############ Round V - subsample ##########\n",
    "# xgb5 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,min_child_weight=4,colsample_bytree=0.109,\n",
    "#                    objective='reg:squarederror',seed=7)\n",
    "# subsample_range = np.arange(0.1,1.05,0.05)\n",
    "# model5, model_results5, best_subsample, best_score5 = XGBRModelTune(xgb5,'subsample',subsample_range)\n",
    "\n",
    "############ Round VI - colsample_bytree ###########\n",
    "# xgb6 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,min_child_weight=4,subsample=1,\n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# colsamp_range = np.arange(0.05,1.05,0.05)\n",
    "# model6, model_results6, best_colsamp, best_score6 = XGBRModelTune(xgb6,'colsample_bytree',colsamp_range)\n",
    "\n",
    "############ Round VII - lambda ##########\n",
    "# xgb7 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# lambda_range = range(1,11,1)\n",
    "# # lamb_exp = -3*np.random.rand(100)\n",
    "# # lambda_range2 = 10**lamb_exp\n",
    "# # model7, model_results7, best_lambda, best_score7 = XGBRModelTune(xgb7,'reg_lambda',lambda_range2,Randomized=True,n_iter=60)\n",
    "# model7_2, model_results7_2, best_lambda2, best_score7_2 = XGBRModelTune(xgb7,'reg_lambda',lambda_range)\n",
    "# # lamb = 0.2897271836971993\n",
    "\n",
    "############ Round VIII - alpha ##########\n",
    "# xgb8 = XGBRegressor(learning_rate=0.1,n_estimators=240,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                      objective='reg:squarederror',seed=7)\n",
    "# alpha_range = [0.01,0.05,0.1,0.25,0.5,0.75,1,5,10,15,20]\n",
    "# alpha_range2 = range(5,16,1)\n",
    "# model8, model_results8, best_alpha, best_score8 = XGBRModelTune(xgb8,'reg_alpha',alpha_range2)\n",
    "\n",
    "############ Round IX - learning_rate ###########\n",
    "# xgb9 = XGBRegressor(n_estimators=400,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,reg_alpha=11,\n",
    "#                      objective='reg:squarederror',seed=7)\n",
    "# lr_exp = -3*np.random.rand(100)\n",
    "# lr_range = 10**lr_exp\n",
    "# model9, model_results9, best_lr, best_score9 = XGBRModelTune(xgb9,'learning_rate',lr_range,Randomized=True,n_iter=70)\n",
    "# #lr_val = 0.057559203317644844 #for n_estimators = 240\n",
    "# lr_val = 0.04097649273411524 #for n_estimators = 400\n",
    "\n",
    "############ SAVE TUNED MODEL ############\n",
    "# lr_val = 0.04097649273411524 #for n_estimators = 400\n",
    "# xgb_tuned = XGBRegressor(learning_rate=lr_val,n_estimators=400,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                          reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# modelfitXGB(xgb_tuned,strain_X2,train_y2)\n",
    "# train_err1, train_exp_var1 = TrainTestErrors(xgb_tuned, 'xgb_tuned_strain_X2_11142019.csv')\n",
    "# train_err1\n",
    "# train_err2, train_exp_var2 = TrainTestErrors(xgb_tuned,'xgb_tuned_train_X2_11142019.csv',\n",
    "#                                              train_X=train_X2,train_y=train_y2,test_X=test_X2)\n",
    "# train_err2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################## PART II #####################\n",
    "\n",
    "########## Round I - learning_rate ##############\n",
    "# xgbII_1 = XGBRegressor(n_estimators=1000,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                           reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# lr_exp = -3*np.random.rand(100)\n",
    "# lr_range = 10**lr_exp\n",
    "# modelII_1, model_resultsII_1, best_lrII, best_scoreII_1 = XGBRModelTune(xgbII_1,'learning_rate',lr_range,Randomized=True,n_iter=70)\n",
    "\n",
    "best_lr = 0.02156731032878086\n",
    "########## Round II - n_estimators ##############\n",
    "# xgbII_2 = XGBRegressor(learning_rate=best_lr,max_depth=4,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                             reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# n_est_range = range(300,1010,10)\n",
    "# n_est_range2 = range(1000,1510,10)\n",
    "# modelII_2, model_resultsII_2, best_n_est, best_scoreII_2 = XGBRModelTune(xgbII_2,'n_estimators',n_est_range2)\n",
    "\n",
    "########## Round III - max_depth ##############\n",
    "# xgbII_3 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                        reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# max_d_range = range(3,10,1)\n",
    "# modelII_3, model_resultsII_3, best_max_d, best_scoreII_3 = XGBRModelTune(xgbII_3,'max_depth',max_d_range)\n",
    "# model_resultsII_3\n",
    "\n",
    "########## TEST OUT MULTIPLE MAX_DEPTH VALUES ON THE TEST SET #########\n",
    "# xgbII_MD4 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,max_depth=4,subsample=1,\n",
    "#                          colsample_bytree=1,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err3, train_exp_var3 = TrainTestErrors(xgbII_MD4,'xgbII_MD4_11142019.csv')\n",
    "# train_err3\n",
    "# xgbII_MD6 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,max_depth=6,subsample=1,\n",
    "#                          colsample_bytree=1,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err4, train_exp_var4 = TrainTestErrors(xgbII_MD6,'xgbII_MD6_11142019.csv')\n",
    "# train_err4\n",
    "# xgbII_MD8 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,max_depth=8,subsample=1,\n",
    "#                           colsample_bytree=1,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err5, train_exp_var5 = TrainTestErrors(xgbII_MD8, 'xgbII_MD8_11142019.csv')\n",
    "# train_err5\n",
    "\n",
    "########## Round IV - reg_lambda ##########\n",
    "# xgbII_4 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,subsample=1,colsample_bytree=1,\n",
    "#                         reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# lambda_range = range(1,22,1)\n",
    "# modelII_4, model_resultsII_4, best_lambda, best_scoreII_4 = XGBRModelTune(xgbII_4,'reg_lambda',lambda_range)\n",
    "\n",
    "########## CHECK OUT THE NEW MODEL ON TEST SET ###########\n",
    "# xgbII_L6 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,max_depth=4,subsample=1,\n",
    "#                           colsample_bytree=1,reg_alpha=11,reg_lambda=6,objective='reg:squarederror',seed=7)\n",
    "# train_err6, train_exp_var6 = TrainTestErrors(xgbII_L6,'xgbII_L6_11142019.csv')\n",
    "# train_err6\n",
    "# xgbII_L12 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,min_child_weight=4,max_depth=4,subsample=1,\n",
    "#                            colsample_bytree=1,reg_alpha=11,reg_lambda=12,objective='reg:squarederror',seed=7)\n",
    "# train_err7, train_exp_var7 = TrainTestErrors(xgbII_L12,'xgbII_L12_11142019.csv')\n",
    "# train_err7\n",
    "\n",
    "########## Round V - colsample_bytree #########\n",
    "# xgbII_5 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,subsample=1,\n",
    "#                          reg_lambda=12,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# colsamp_range = np.arange(0.01,1.01,0.01)\n",
    "# modelII_5, model_resultsII_5, best_colsamp, best_scoreII_5 = XGBRModelTune(xgbII_5,'colsample_bytree',colsamp_range)\n",
    "\n",
    "########## Round VI - subsample ##########\n",
    "# xgbII_6 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                        reg_lambda=12,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# subsample_range = np.arange(0.1,1.01,0.01)\n",
    "# modelII_6, model_resultsII_6, best_subsample, best_scoreII_6 = XGBRModelTune(xgbII_6,'subsample',subsample_range)\n",
    "# model_resultsII_6\n",
    "\n",
    "########### COMPARE SUBSAMPLE VALUES ON TEST SET ##########\n",
    "# xgbII_SS1 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                          subsample=1,reg_lambda=12,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err8, train_exp_var8 = TrainTestErrors(xgbII_SS1,'xgbII_SS1_11142019.csv')\n",
    "# train_err8\n",
    "# xgbII_SS66 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                           subsample=0.66,reg_lambda=12,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err9, train_exp_var9 = TrainTestErrors(xgbII_SS66,'xgbII_SS66_11142019.csv')\n",
    "# train_err9\n",
    "# xgbII_SS50 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                            subsample=0.5,reg_lambda=12,reg_alpha=11,objective='reg:squarederror',seed=7)\n",
    "# train_err10, train_exp_var10 = TrainTestErrors(xgbII_SS50,'xgbII_SS50_11142019.csv')\n",
    "# train_err10\n",
    "\n",
    "########## Round VII - reg_alpha ###########\n",
    "# xgbII_7 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                         subsample=0.66, reg_lambda=12,objective='reg:squarederror',seed=7)\n",
    "# alpha_range = range(0,26,1)\n",
    "# modelII_7, model_resultsII_7, best_alpha, best_scoreII_7 = XGBRModelTune(xgbII_7,'reg_alpha',alpha_range)\n",
    "# model_resultsII_7\n",
    "\n",
    "########## COMPARE ALPHA VALUES ON TEST SET ###########\n",
    "# xgbII_A8 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                         subsample=0.66, reg_alpha=8, reg_lambda=12,objective='reg:squarederror',seed=7)\n",
    "# train_err11, train_exp_var11 = TrainTestErrors(xgbII_A8,'xgbII_A8_11162019.csv')\n",
    "# train_err11\n",
    "# xgbII_A5 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                          subsample=0.66, reg_alpha=5, reg_lambda=12,objective='reg:squarederror',seed=7)\n",
    "# train_err12, train_exp_var12 = TrainTestErrors(xgbII_A5, 'xgbII_A5_11162019.csv')\n",
    "# train_err12\n",
    "# xgbII_A13 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,min_child_weight=4,colsample_bytree=0.29,\n",
    "#                          subsample=0.66, reg_alpha=13, reg_lambda=12,objective='reg:squarederror',seed=7)\n",
    "# train_err13, train_exp_var13 = TrainTestErrors(xgbII_A13, 'xgbII_A13_11162019.csv')\n",
    "# train_err13\n",
    "\n",
    "########## Round VIII - min_child_weights ##########\n",
    "# xgbII_8 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,\n",
    "#                          subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# mcw_range = range(1,16,1)\n",
    "# modelII_8, model_resultsII_8, best_mcw, best_scoreII_8 = XGBRModelTune(xgbII_8,'min_child_weight',mcw_range)\n",
    "# model_resultsII_8\n",
    "\n",
    "########## COMPARE MCW VALUES ON TEST SET ##########\n",
    "# xgbII_MCW1 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                           subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# train_err14, train_exp_var14 = TrainTestErrors(xgbII_MCW1, 'xgbII_MCW1_11162019.csv')\n",
    "# train_err14\n",
    "# xgbII_MCW5 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                           subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# train_err15, train_exp_var15 = TrainTestErrors(xgbII_MCW5, 'xgbII_MCW5_11162019.csv')\n",
    "# train_err15\n",
    "# xgbII_MCW8 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=8,\n",
    "#                            subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# train_err16, train_exp_var16 = TrainTestErrors(xgbII_MCW8, 'xgbII_MCW8_11162019.csv')\n",
    "# train_err16\n",
    "# xgbII_MCW3 =  XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=3,\n",
    "#                             subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# train_err17, train_exp_var17 = TrainTestErrors(xgbII_MCW3, 'xgbII_MCW3_11162019.csv')\n",
    "# train_err17\n",
    "# xgbII_MCW6 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=6,\n",
    "#                              subsample=0.66, reg_lambda=12,reg_alpha=11, objective='reg:squarederror',seed=7)\n",
    "# train_err18, train_exp_var18 = TrainTestErrors(xgbII_MCW6, 'xgbII_MCW6_11162019.csv')\n",
    "# train_err18\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################### PART III - dart ######################\n",
    "\n",
    "########### Round I - n_estimators #############\n",
    "# xgbII_dart1 = XGBRegressor(learning_rate=best_lr,n_estimators=1280,max_depth=6,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                           subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# n_est_range = range(400,1005,5)\n",
    "# n_est_range2 = range(850,1305,5)\n",
    "# modelII_dart1, model_resultsII_dart1, best_n_est, best_scoreII_dart1 = XGBRModelTune(xgbII_dart1,'n_estimators',n_est_range2)\n",
    "\n",
    "########### Round II - max_depth ################\n",
    "# xgbII_dart2 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                            subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# max_d_range = range(3,11,1)\n",
    "# modelII_dart2, model_resultsII_dart2, best_max_d, best_scoreII_dart2 = XGBRModelTune(xgbII_dart2,'max_depth',max_d_range)\n",
    "# model_resultsII_dart2\n",
    "\n",
    "########### CHECK MODEL ON TEST SET ###########\n",
    "# xgbII_dartMD5 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=5,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                              subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err19, train_exp_var19 = TrainTestErrors(xgbII_dartMD5, 'xgbII_dartMD5_11172019.csv')\n",
    "# train_err19\n",
    "# xgbII_dartMD6 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=6,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                               subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err20, train_exp_var20 = TrainTestErrors(xgbII_dartMD6, 'xgbII_dartMD6_11172019.csv')\n",
    "# train_err20\n",
    "# xgbII_dartMD4 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=5,\n",
    "#                                subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err21, train_exp_var21 = TrainTestErrors(xgbII_dartMD4, 'xgbII_dartMD4_11172019.csv')\n",
    "# train_err21\n",
    "\n",
    "########### Round III - min_child_weight ##############\n",
    "# xgbII_dart3 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,colsample_bytree=0.29,subsample=0.66, \n",
    "#                            reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# mcw_range = range(1,11,1)\n",
    "# modelII_dart3, model_resultsII_dart3, best_mcw, best_scoreII_dart3 = XGBRModelTune(xgbII_dart3, 'min_child_weight', mcw_range)\n",
    "# model_resultsII_dart3\n",
    "\n",
    "########### CHECK MODEL ON TEST SET ###########\n",
    "# xgbII_dartMCW1 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                               subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err22, train_exp_var22 = TrainTestErrors(xgbII_dartMCW1, 'xgbII_dartMCW1_11182019.csv')\n",
    "# train_err22\n",
    "# xgbII_dartMCW3 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=3,\n",
    "#                               subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err23, train_exp_var23 = TrainTestErrors(xgbII_dartMCW3, 'xgbII_dartMCW3_11182019.csv')\n",
    "# train_err23\n",
    "# xgbII_dartMCW2 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=2,\n",
    "#                               subsample=0.66, reg_lambda=12,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err24, train_exp_var24 = TrainTestErrors(xgbII_dartMCW2, 'xgbII_dartMCW2_11182019.csv')\n",
    "# train_err24\n",
    "\n",
    "########### Round IV - lambda ###########\n",
    "# xgbII_dart4 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,colsample_bytree=0.29,subsample=0.66,min_child_weight=1, \n",
    "#                            reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# lambda_range = range(0,16,1)\n",
    "# lambda_range2 = range(15,26,1)\n",
    "# modelII_dart4, model_resultsII_dart4, best_lambda, best_scoreII_dart4 = XGBRModelTune(xgbII_dart4, 'reg_lambda', lambda_range2)\n",
    "# model_resultsII_dart4\n",
    "\n",
    "########### CHECK MODEL ON TEST SET ###########\n",
    "# xgbII_dartL2 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                                subsample=0.66, reg_lambda=2,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err25, train_exp_var25 = TrainTestErrors(xgbII_dartL2, 'xgbII_dartL2_11182019.csv')\n",
    "# train_err25\n",
    "# xgbII_dartL8 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                                subsample=0.66, reg_lambda=8,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err26, train_exp_var26 = TrainTestErrors(xgbII_dartL8, 'xgbII_dartL8_11182019.csv')\n",
    "# train_err26\n",
    "# xgbII_dartL14 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                                subsample=0.66, reg_lambda=14,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err27, train_exp_var27 = TrainTestErrors(xgbII_dartL14, 'xgbII_dartL14_11182019.csv')\n",
    "# train_err27\n",
    "# xgbII_dartL17 = XGBRegressor(learning_rate=best_lr,n_estimators=1030,max_depth=4,colsample_bytree=0.29,min_child_weight=1,\n",
    "#                                 subsample=0.66, reg_lambda=17,reg_alpha=11, booster='dart',objective='reg:squarederror',seed=7)\n",
    "# train_err28, train_exp_var28 = TrainTestErrors(xgbII_dartL17, 'xgbII_dartL17_11182019.csv')\n",
    "# train_err28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ XGBRegressor Pt.3 (train_X2,train_y2,test_X2) ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ K-NEAREST NEIGHBOR REGRESSION ###############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########## CODE THAT IS NOT BEING USED ############\n",
    "\n",
    "\n",
    "########### TRYING TO FIGURE OUT WHY RIDGE REGRESSION DOESNT ACCEPT THE DATA & SCORE WITH NEG_MEAN_LOG_ERROR ###########\n",
    "# list_valcounts = []\n",
    "# for cols in train_X2.columns:\n",
    "#     list_valcounts.append(train_X2[cols].value_counts())\n",
    "\n",
    "#list_valcounts[0]\n",
    "#train_X2.lt(0).sum()\n",
    "#train_X2.columns\n",
    "#train_y2.lt(0).sum()\n",
    "# np.sum(train_y2 > 0)\n",
    "# len(train_y2)\n",
    "\n",
    "\n",
    "\n",
    "####### THIS PIECE OF CODE ONLY WORKS FOR A SINGLE DECISION TREE\n",
    "# dotData = StringIO()\n",
    "# rfr_tuned.fit(train_X2,train_y2)\n",
    "# export_graphviz(rfr_tuned,out_file=dotData,filled=True,\n",
    "#                 rounded=True,special_characters=True)\n",
    "\n",
    "# graph_rfr4 = pydotplus.graph_from_dot_data(dotData.getvalue())\n",
    "# Image(graph.create_png())\n",
    "\n",
    "########### I am going to play around a little with the XGB package\n",
    "\n",
    "# #Parameter dictionary for XGB\n",
    "# param_dict = {'learning_rate':0.1,'n_estimators':500,'max_depth':5,'min_child_weight':1,\n",
    "#               'gamma':0.1,'subsample':0.6,'colsample_bytree':0.6,'reg_lambda':0.1,'reg_alpha':0.1}\n",
    "# #The model \n",
    "# basic_xgb = XGBRegressor(param_dict)\n",
    "# #Use a DMatrix so we can do the Cross-Validation\n",
    "# Dtrain = xgb.DMatrix(train_X2.values,label=train_y2.values)\n",
    "\n",
    "# #Don't bother with this because this only conducts a single cross validation test \n",
    "# #on a single set of variables. This may be optimal when you are looking for the value\n",
    "# #of n_estimators but is not that valuable in general.\n",
    "# #Cross validation test.\n",
    "# cv_test = xgb.cv(param_dict,Dtrain,num_boost_round=basic_xgb.get_params()['n_estimators'],nfold=10,\n",
    "#                  metrics='rmse',early_stopping_rounds=50)\n",
    "\n",
    "\n",
    "# A function to help tune XGBoost models.\n",
    "# def XGBRModelTune(train_X,train_y,test_X,learning_rate=0.1,n_estimators=500,max_depth=5,min_child_weight=1,gamma=0.1,\n",
    "#                  subsample=0.6,colsample_bytree=0.6,objective='reg:squarederror',booster='gbtree',reg_lambda=0.1,reg_alpha=0,\n",
    "#                  cv_num=5,Randomized=False,n_jobs=0,order=range(0,9,1),num_rounds=3):\n",
    "#     #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "#     #################################################### BASIC INTRODUCTION ################################################\n",
    "#     #This is a function that is used to tune parameters for the XGBoost parameters. There are a total of approximately\n",
    "#     #11 parameters to change in XGBoost, but there will only be 9 that are being tuned in this function. \n",
    "#     #The only 2 that are not being tuned: objective and booster. You can change these in the definition of the function,\n",
    "#     #but they will not be tuned in the function itself.\n",
    "#     #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "#     #***train_X = the training data (features only).  \n",
    "#     #***train_y = the training data (target only).\n",
    "#     #***test_X = the testing data (features only).\n",
    "    \n",
    "#     #***learning_rate = the learning rate of the XGBRegressor algorithm.\n",
    "#     #***n_estimators = the number of trees to use in this ensemble model. \n",
    "#     #***max_depth = maximum depth allowed for an individual tree.\n",
    "#     #***min_child_weight = minimum number of weights allowed for a child node; basically a variable that describes the amount of \n",
    "#     # observations that are allowed in each child node. The higher the value, the more values that are required in each node.\n",
    "#     #***gamma = A value that defines the minimum positive reduction in the loss function that must occur for a node to split.\n",
    "#     #***subsample = A value that denotes the % of samples to be used in each node of the tree.\n",
    "#     #***colsample_bytree = A value that determines the % of columns to be used for each tree.\n",
    "#     #***objective = The loss function to be minimized.\n",
    "#     #***booster = The type of model that we run at each iteration. Can choose gbtree (tree-based models), gblinear (linear models),\n",
    "#     # or dart which is similar to gbtree but it implements deep neural networks drop-out technique.\n",
    "#     #***reg_lambda = L2 regularization term on weights. Used to handle the main regularization part of XGBoost.\n",
    "#     #***reg_alpha = L1 regularization term on weights.\n",
    "#     #***cv_num = The number of cross-validation folds that will be used in the parameter search process.\n",
    "#     #***Randomized = A boolean value that decides if the first search you do for parameter searches is randomized or not.\n",
    "#     #***n_iter = A number that is only used if Randomized is true. It essentially determines the number of minimum jobs \n",
    "#     # RandomizedSearchCV will do before it stops testing random values of a variable\n",
    "\n",
    "#Practice with dictionary objects \n",
    "# dict_test = {'key1':0,'key2':1}\n",
    "# keys = list(dict_test.keys())\n",
    "# keys\n",
    "\n",
    "#Parameter dictionary for XGB\n",
    "#This did not work at all.\n",
    "# param_dict = {'learning_rate':0.1,'n_estimators':500,'max_depth':5,'min_child_weight':1,\n",
    "#               'gamma':0.1,'subsample':0.6,'colsample_bytree':0.6,'reg_lambda':0.1,'reg_alpha':0.1}\n",
    "\n",
    "#The model \n",
    "# basic_xgb = XGBRegressor(learning_rate=0.1,n_estimators=500,max_depth=5,min_child_weight=1,gamma=0.1,subsample=0.6,\n",
    "#                         colsample_bytree=0.6,reg_lambda=0.1,reg_alpha=0.1,random_state=5)\n",
    "# #Use a DMatrix so we can do the Cross-Validation\n",
    "# Dtrain = xgb.DMatrix(train_X2.values,label=train_y2.values)\n",
    "\n",
    "# #get_params() gets us all the parameters defined in the XGBRegressor\n",
    "# p1 = basic_xgb.get_params() #This shows us 3 extra variables: n_jobs, random_state, silent\n",
    "# kp1 = list(p1.keys())\n",
    "\n",
    "# #These are xgb parameters. This is the better function to use.\n",
    "# p2 = basic_xgb.get_xgb_params()\n",
    "# kp2 = list(p2.keys())\n",
    "\n",
    "# for key in kp1:\n",
    "#     if key not in p2: print(key)\n",
    "\n",
    "\n",
    "\n",
    "#############Untuned model\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=6,min_child_weight=1,gamma=0,subsample=0.6,\n",
    "#                    colsample_bytree=0.1,scale_pos_weight=1,seed=13,objective='reg:squarederror')\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb1,train_X2,train_y2)\n",
    "\n",
    "\n",
    "###### THIS RAN FINE!!!!\n",
    "# XGB_param_test1 = {'max_depth':range(2,13,2),'min_child_weight':range(1,6,1)}\n",
    "# XGBgsearch1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),param_grid=XGB_param_test1,\n",
    "#                           scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch1.fit(train_X2,train_y2)\n",
    "\n",
    "# THIS RAN FINE AS WELL!!\n",
    "# XGB_param_test1 = {'max_depth':range(2,13,2),'min_child_weight':range(1,6,1)}\n",
    "# XGBgsearch1 = RandomizedSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),param_distributions=XGB_param_test1,\n",
    "#                           scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch1.fit(train_X2,train_y2)\n",
    "# It turns out the reason was I did not define train_X, train_y, or test_X in the function.\n",
    "\n",
    "\n",
    "# # We must first split the training set into the training and dev set before we use .fit_transform on our training data.\n",
    "# train_X2_Standard = standardize_scaler.fit_transform(train_X2)\n",
    "# test_X2_Standard = standardize_scaler.transform(test_X2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Random Forest Regressor model fit function\n",
    "# def RFRmodelfitCV(alg, train_X, train_y, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "#     #Fit the algorithm on the data\n",
    "#     alg.fit(train_X,train_y)\n",
    "    \n",
    "#     #Predict on the training set\n",
    "#     train_predictions = alg.predict(train_X)\n",
    "    \n",
    "#     #Perform cross-validation\n",
    "#     if performCV:\n",
    "#         cv_score = cross_val_score(alg, train_X, train_y, cv = cv_folds, scoring='neg_mean_squared_log_error')\n",
    "        \n",
    "#     #Print the model report\n",
    "#     print(\"\\nModel Report\")\n",
    "#     print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "#     print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "#     if performCV:\n",
    "#         #print('CV Score: %s'% cv_score)\n",
    "#         print(\"CV Scores \\nMean : %.7g | Std : %.7g | Min : %.7g | Max : %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "#     #Print Feature Importance\n",
    "#     if printFeatureImportance:\n",
    "#         feat_imp = pd.Series(alg.feature_importances_,train_X.columns).sort_values(ascending=False)[0:30]\n",
    "#         feat_imp.plot(kind='bar', title = 'Feature Importances')\n",
    "#         plt.ylabel('Feature Importance Score')\n",
    "#         #print(feat_imp) #I may add this\n",
    "\n",
    "\n",
    "#I will come back to 11/8/19\n",
    "#Trying to trouble shoot how to fix the modelfit portion of XGBRModelTune\n",
    "#This works here but its not working in my function!?\n",
    "# param = 'n_estimators'\n",
    "# p_val = 294\n",
    "# p_dict = {param:p_val}\n",
    "# #THIS WORKS!!!!!!!\n",
    "# xgb7.set_params(**p_dict)\n",
    "# modelfitXGB(xgb7,strain_X,strain_y,cv_folds=3)\n",
    "\n",
    "\n",
    "#FROM MY FUNCTION\n",
    "# p_dict = {xgb_param:best_param_val} #this didn't work because best_param_val is a dict not just a value!!!!!!\n",
    "# p_dict = {xgb_param:best_param_val[xgb_param]} #this should work!!!!!\n",
    "# xgb_alg.set_params(**p_dict)\n",
    "# modelfitXGB(xgb_alg,train_X,train_y,cv_folds=cv_num)\n",
    "\n",
    "\n",
    "\n",
    "# #Doesn't work\n",
    "# string_style = param + '=' + str(p_val)\n",
    "# e_ss = exec(string_style)\n",
    "\n",
    "#This doesn't change the value\n",
    "# xgb7.get_params()[param] = p_val\n",
    "# xgb7.get_params()\n",
    "\n",
    "\n",
    "# xgb7.set_params(n_estimators=p_val)\n",
    "# xgb7.get_params()\n",
    "\n",
    "#This segment works to change the dictionary values.\n",
    "# xgb_params = xgb7.get_params()\n",
    "# xgb_params[param] = p_val\n",
    "# xgb_params\n",
    "# #However, this segment returns an error\n",
    "# xgb7.set_params(xgb_params)\n",
    "\n",
    "\n",
    "\n",
    "#     title1 = str(metric) + 'Vs' + xgb_param + \"Training Errors\"\n",
    "#     xaxis1 = xgb_param\n",
    "#     yaxis1 = str(metric)\n",
    "    \n",
    "#     fig1.update_layout(\n",
    "#     title=title1,\n",
    "#     xaxis_title=xaxis1,\n",
    "#     yaxis_title=yaxis1,\n",
    "#     font=dict(\n",
    "#         family=\"Courier New, monospace\",\n",
    "#         size=12,\n",
    "#         color=\"#7f7f7f\"\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "\n",
    "#This is a good way to combine lists together into a dataframe.\n",
    "# list1 = [1,1,2,3,5,8,13]\n",
    "# list2 = [0,1,2,3,4,5,6]\n",
    "# col1 = '1'\n",
    "\n",
    "\n",
    "# df0 = pd.DataFrame(data=list(zip(list1,list2)),columns=[col1,'2'])\n",
    "# df0.columns[0]\n",
    "\n",
    "# #Great strategy to combine lists so that we can plot them with px.scatter\n",
    "# #Create the first plot - training errors              \n",
    "#     df1 = pd.DataFrame(data=list(zip(xgb_parameter_values,train_error_arr)),columns=[xgb_param,'Training Error'])\n",
    "#     fig1 = px.scatter(df1, x=df1.columns[0], y='Training Error', color='Training Error')\n",
    "\n",
    "#     #Create the second plot - dev errors\n",
    "#     df2 = pd.DataFrame(data=list(zip(xgb_parameter_values,dev_error_arr)),columns=[xgb_param,'Dev Error'])\n",
    "#     fig2 = px.scatter(df2, x=df2.columns[0], y='Dev Error', color='Dev Error')\n",
    "\n",
    "\n",
    "### TESTING THE FUNCTION I JUST MADE ###\n",
    "\n",
    "# #Create a basic XGBRegressor model\n",
    "# basic_xgb = XGBRegressor(objective='reg:squarederror',seed=7,n_estimators=500) #Make sure objective = 'reg:squarederror'\n",
    "# #The parameter we will tune\n",
    "# parameter_tune1 = 'learning_rate'\n",
    "# #We create an exponent like this to get an even distribution between -4 -> 0\n",
    "# exp_LR = -4*np.random.rand(100)\n",
    "# #We use this exponent array to act like an exponent, so that our distribution is just as likely to pick values\n",
    "# #between 0.001 - 0.01 as it is to pick values between 0.1 - 1.\n",
    "# parameter_tune_vals1 = 10**exp_LR\n",
    "\n",
    "# #Now we run the program to make sure that the program runs well.\n",
    "# model1, model_results, best_lr, best_score = XGBRModelTune(xgb_alg = basic_xgb, xgb_param = parameter_tune1, xgb_param_vals = parameter_tune_vals1,\n",
    "#                              Randomized=True, n_iter = len(parameter_tune_vals1)-50)\n",
    "\n",
    "# # #EVERYTHING IS GOOD!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
