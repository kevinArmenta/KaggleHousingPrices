{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /anaconda3/lib/python3.6/site-packages (0.90)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.15.2)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.6/site-packages (from xgboost) (1.3.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #To allow us to work with dataframes\n",
    "import numpy as np #To allow us to make mathematical transformations\n",
    "import matplotlib.mlab as mlab #To create plots\n",
    "import matplotlib.pylab as plt #To create plots\n",
    "%matplotlib inline \n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "import category_encoders as ce #To encode our nominal and categorical variables\n",
    "from sklearn import preprocessing, metrics #This module can be helpful when processing data\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import pickle\n",
    "#import scipy as sp #To play with scikit-learn.\n",
    "import sys \n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Models to import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "#updating xgboost and scipy to get rid of an error (9/18/19)\n",
    "#RUN IN TERMINAL\n",
    "# pip install --upgrade xgboost \n",
    "# pip install --upgrade scipy\n",
    "# pip install --upgrade sklearn\n",
    "\n",
    "#I don't believe that we need this if we export the entire sklearn library. I will comment these out \n",
    "#until it's time to start training models.\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Load in the test and train data\n",
    "train_house = pd.read_csv('train.csv')\n",
    "test_house = pd.read_csv('test.csv')\n",
    "\n",
    "#Take a look at a summary of the training data.\n",
    "#train_house.describe()\n",
    "\n",
    "#We will seperate the training set into features (train_X) and the predictor variable (train_y)\n",
    "train_X = train_house.loc[:,train_house.columns != 'SalePrice']\n",
    "train_y = train_house.SalePrice\n",
    "\n",
    "#There are no predictions to compare with, you submit them on Kaggle.\n",
    "test_X = test_house\n",
    "\n",
    "#This is used so we can see the full output display of the iPython Notebook.\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Look at the column names\n",
    "#train_X.columns\n",
    "#Verified that all the column names are the names on the sheet of paper \n",
    "#test_X.shape  (1459,80)\n",
    "#train_X.shape (1460, 80)\n",
    "\n",
    "#Look at what the values look like for each column\n",
    "#train_X.MiscVal\n",
    "#train_X.OverallCond.dtypes\n",
    "\n",
    "#LETS LOOK AT HISTS OF VARS IF WE NEED TO.\n",
    "#NUMERICAL VARIABLES \n",
    "#curr_col = train_house.LotFrontage\n",
    "#curr_col.hist(bins=25)\n",
    "#curr_col.value_counts()\n",
    "\n",
    "#CAT VARIABLES\n",
    "# curr_col = train_X.BsmtExposure\n",
    "# curr_col.value_counts().plot(kind='bar')\n",
    "# curr_col.value_counts()\n",
    "\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "\n",
    "# curr_var = train_house.loc[train_house.Neighborhood == neighborhood[24],'SalePrice']\n",
    "# curr_var.hist(bins=25)\n",
    "# curr_var.describe()\n",
    "\n",
    "#Checking out the null values for each column where there are null values.\n",
    "#inds = train_house.BsmtQual.loc[pd.isnull(train_house.BsmtQual)]\n",
    "#inds = inds.index\n",
    "#train_house.loc[inds,]\n",
    "\n",
    "\n",
    "######## HANDLING MISSING VALUES <ONLY RUN ONCE>\n",
    "#We need to handle all the missing values in the following columns:\n",
    "\n",
    "\n",
    "#THESE WERE HANDLED FROM THE TRAIN_X DATASET.\n",
    "#LotFrontage = 484\n",
    "#Alley = 2709\n",
    "#MasVnrType = 24 \n",
    "#MasVnrArea = 23\n",
    "#BsmtQual = 76\n",
    "#BsmtCond = 77\n",
    "#BsmtExposure = 76\n",
    "#BsmtFinType1 = 74\n",
    "#BsmtFinType2 = 74\n",
    "#FireplaceQu = 1412\n",
    "#GarageType = 156\n",
    "#GarageYrBlt = 157\n",
    "#GarageFinish = 157\n",
    "#GarageQual = 157\n",
    "#GarageCond = 157\n",
    "#PoolQC = 2896\n",
    "#Fence = 2337\n",
    "#MiscFeature = 2802\n",
    "\n",
    "#THESE ARE NEW FEATURES WITH MISSING VALUES ADDED FROM TEST_X DATASET (EXCEPT ELECTRICAL).\n",
    "#THESE WILL ALL BE REMOVED (12 ROWS IN TOTAL)\n",
    "#MSZoning = 4, Utilities = 2, Exterior1st = 1, Exterior2nd = 1, BsmtFinSF1 = 1, BsmtFinSF2 = 1, BsmtUnfSF = 1, TotalBsmtSF = 1, \n",
    "#BsmtFullBath = 2, BsmtHalfBath = 2, KitchenQual = 1, Functional = 2, GarageCars = 1, GarageArea = 1, SaleType = 1, Electrical = 1\n",
    "\n",
    "#Removing some indices that I discovered from preliminary research along with some bad data from the columns listed above\n",
    "#332 -> BsmtFinType2 is NaN while the other basement variables are okay, so I didn't know what to replace this with.\n",
    "#948 -> BsmtExposure was NaN while other basement variables are okay.\n",
    "#1379 -> Removed the Electrical NaN in the dataset, it is stupid to keep this.\n",
    "#We can still clean up the training data and throw some rows out but we cannot do this for the\n",
    "#testing data because Kaggle requires all 1459 rows to be intact. Therefore, we need to fix the \n",
    "#the NaN's in the testing set by setting them as NA instead.\n",
    "train_X = train_X.drop(train_X.index[[332,948,1379]])\n",
    "train_X = train_X.reset_index(drop = True)\n",
    "train_y = train_y.drop(train_y.index[[332,948,1379]])\n",
    "train_y = train_y.reset_index(drop = True)\n",
    "\n",
    "train_IDs = train_X.Id #Save these so we can extract the training data from the combined_df later\n",
    "test_IDs = test_X.Id #Save these so we can extract the testing datra from the combined_df later\n",
    "\n",
    "\n",
    "#COMBINING THE TRAINING AND TESTING DATASETS TO CREATE A SUPER DATA SET.\n",
    "combined_df = pd.concat([train_X,test_X])\n",
    "combined_df = combined_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "#Uncomment this once we've handled the missing values that need to be removed. Do this for the\n",
    "#combined dataset, not just the training dataset.\n",
    "#First we will do the transformations on practice_train_X to make sure it does what we really want to do.\n",
    "# train_X = train_X.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "#                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "#                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None',\n",
    "#                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "#                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "# test_X = test_X.fillna({'MSZoning':'NA','LotFrontage' : 0,'Utilities':'NA','Alley' : 'No Alley','MasVnrType': 'NA',\n",
    "#                         'MasVnrArea':0,'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', 'Exterior1st':'NA',\n",
    "#                                             'Exterior2nd':'NA','BsmtFinType1' : 'None','BsmtFinSF1':0,'BsmtFinType2':'None',\n",
    "#                                             'BsmtFinSF2':0,'BsmtUnfSF':0,'TotalBsmtSF':0,'BsmtFullBath':0,'BsmtHalfBath':0,\n",
    "#                         'KitchenQual':'NA','Functional':'NA','FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "#                                             'GarageFinish' : 'None','GarageCars':0,'GarageArea':0,'GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None','SaleType':'NA'})\n",
    "\n",
    "\n",
    "\n",
    "combined_df = combined_df.fillna({'MSZoning':'NA','LotFrontage' : 0,'Utilities':'NA','Alley' : 'No Alley','MasVnrType': 'NA',\n",
    "                                  'MasVnrArea':0,'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', 'Exterior1st':'NA',\n",
    "                                  'Exterior2nd':'NA','BsmtFinType1' : 'None','BsmtFinSF1':0,'BsmtFinType2':'None',\n",
    "                                  'BsmtFinSF2':0,'BsmtUnfSF':0,'TotalBsmtSF':0,'BsmtFullBath':0,'BsmtHalfBath':0,\n",
    "                                  'KitchenQual':'NA','Functional':'NA','FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "                                  'GarageFinish' : 'None','GarageCars':0,'GarageArea':0,'GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "                                  'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None','SaleType':'NA'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Defining important functions for evaluating Boosting models.\n",
    "\n",
    "def modelfitCV(alg, train_X, train_y, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_X,train_y)\n",
    "    \n",
    "    #Predict on the training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, train_X, train_y, cv = cv_folds, scoring='neg_mean_squared_log_error')\n",
    "        \n",
    "    #Print the model report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    if performCV:\n",
    "        #print('CV Score: %s'% cv_score)\n",
    "        print(\"CV Scores \\nMean : %.7g | Std : %.7g | Min : %.7g | Max : %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #Print Feature Importance\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_,train_X.columns).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar', title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        #print(feat_imp) #I may add this\n",
    "\n",
    "\n",
    "        \n",
    "def modelfitXGB(alg, train_X, train_y, useTrainCV=True, printFeatureImportance=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_params = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_X.values,label=train_y.values)\n",
    "        cvresult = xgb.cv(xgb_params,xgtrain,num_boost_round=alg.get_params()['n_estimators'],nfold=cv_folds,metrics='rmse',\n",
    "                          early_stopping_rounds=early_stopping_rounds,verbose_eval=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(\"n_estimators: %.4g\" % alg.get_params()['n_estimators'])\n",
    "        \n",
    "    #Fit Algorithm on the data\n",
    "    alg.fit(train_X,train_y,eval_metric='rmse')\n",
    "    \n",
    "    #Predict training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Print Model Report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar',title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Important Score')\n",
    "\n",
    "##################################\n",
    "#LAST CELL BEFORE PIPELINES\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################### DATA PIPELINE #1 ###########################\n",
    "################################################################\n",
    "################### MOST BASIC PIPELINE ########################\n",
    "# 1. NO COLS REMOVED (EXCEPT ID) -> 2. Convert ORD VARS TO # (intuitive) -> 3. Convert CAT Vars to # (One-hot) -> \n",
    "# 4. Train Models (Many models to try)\n",
    "\n",
    "\n",
    "##########Step 1: Remove the ID column because it essentially just numbers the rows. \n",
    "#Also, lets instantiate new variables so that anything that we do to them won't be reflected in the original \n",
    "#dataset, and won't affect the other pipelines I plan to create in the future. \n",
    "#train_X1 = train_X.loc[:,train_X.columns != 'Id']\n",
    "#train_y1 = train_y\n",
    "#test_X1 = test_X .loc[:,test_X.columns != 'Id']\n",
    "combined_df1 = combined_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Step 2 Convert all ORD variables into numbers, that way we can use them in regression.\n",
    "#I will try to use category_encoder for basic ordinal encoding.\n",
    "\n",
    "#8/21/19\n",
    "#THIS IS THE CORRECT FORMATING FOR THE CATEGORICAL_ENCODER MODULE!!! \n",
    "#THE DOCUMENTATION THAT I WAS USING WAS OUTDATED!!!\n",
    "\n",
    "#We create a STANDARD mapping for all ORDINAL VARIABLES. \n",
    "ordinal_cols_mapping1 = [{\"col\":\"Utilities\",\n",
    "                            \"mapping\": {'AllPub':4,\n",
    "                                       'NoSewr':3,\n",
    "                                       'NoSeWa':2,\n",
    "                                       'ELO':1}},\n",
    "                         {\"col\":\"LandSlope\",\n",
    "                            \"mapping\":{'Sev':3,\n",
    "                                       'Mod':2,\n",
    "                                       'Gtl':1}},\n",
    "                         {\"col\":\"ExterQual\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"ExterCond\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"BsmtQual\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtCond\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtExposure\",\n",
    "                             \"mapping\":{'Gd':4,\n",
    "                                        'Av':3,\n",
    "                                        'Mn':2,\n",
    "                                        'No':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtFinType1\",\n",
    "                             \"mapping\":{'GLQ':6,\n",
    "                                        'ALQ':5,\n",
    "                                        'BLQ':4,\n",
    "                                        'Rec':3,\n",
    "                                        'LwQ':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtFinType2\",\n",
    "                             \"mapping\":{'GLQ':6,\n",
    "                                        'ALQ':5,\n",
    "                                        'BLQ':4,\n",
    "                                        'Rec':3,\n",
    "                                        'LwQ':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"HeatingQC\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"KitchenQual\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"FireplaceQu\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageFinish\",\n",
    "                             \"mapping\":{'Fin':3,\n",
    "                                        'RFn':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageQual\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageCond\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"PoolQC\",\n",
    "                             \"mapping\":{'Ex':4,\n",
    "                                        'Gd':3,\n",
    "                                        'TA':2,\n",
    "                                        'Fa':1,\n",
    "                                        'None':0}}\n",
    "                       ]\n",
    "\n",
    "#We input these columns and there corresponding dictionaries into ce.OrdinalEncoder in order to swap these values out\n",
    "ce_ord = ce.OrdinalEncoder(mapping = ordinal_cols_mapping1,return_df = True)\n",
    "#Now we have to fit the encoder to our training data.\n",
    "#train_X1 = ce_ord.fit_transform(train_X1,train_y1) #This doesn't create a combined dataframe like I originally thought, this is just the X dataframe.\n",
    "#test_X1 = ce_ord.fit_transform(test_X1)\n",
    "combined_df1 = ce_ord.fit_transform(combined_df1)\n",
    "\n",
    "\n",
    "##########Step 3: Convert all CAT variables into numerical values.\n",
    "#We need to examine each CAT column and determine which would be the best way to convert that column.\n",
    "#For this pipeline, all categorical variables are going to be converted into one hot vectors.\n",
    "\n",
    "ce_one_hot = ce.OneHotEncoder(cols = ['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','LotConfig',\n",
    "                                     'Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle',\n",
    "                                     'RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating',\n",
    "                                     'CentralAir','Electrical','Functional','GarageType','PavedDrive','Fence',\n",
    "                                     'MiscFeature','SaleType','SaleCondition'])\n",
    "\n",
    "# train_X1 = ce_one_hot.fit_transform(train_X1,train_y1)\n",
    "# test_X1 = ce_one_hot.fit_transform(test_X1)\n",
    "combined_df1 = ce_one_hot.fit_transform(combined_df1)\n",
    "\n",
    "\n",
    "#We need to make sure that they have the exact number of columns and that they are lined up the same.\n",
    "#Shows the columns that are in train_X1 but are not in test_X1\n",
    "#missing_col_from_test = train_X1.columns.difference(test_X1.columns)\n",
    "#missing_col_from_train = test_X1.columns.difference(train_X1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step 4: Train the Models!\n",
    "#First, we break up combined_df1 into train_X1 and test_X1\n",
    "train_X1 = combined_df1.loc[combined_df1.Id <= train_IDs[len(train_IDs)-1],:]\n",
    "train_X1 = train_X1.loc[:,train_X1.columns != 'Id']\n",
    "train_X1 = train_X1.reset_index(drop = True)\n",
    "\n",
    "test_X1 = combined_df1.loc[combined_df1.Id >= test_IDs[0],:]\n",
    "test_X1 = test_X1.loc[:,test_X1.columns != 'Id']\n",
    "test_X1 = test_X1.reset_index(drop = True)\n",
    "\n",
    "train_y1 = train_y\n",
    "\n",
    "\n",
    "#For this first pipeline, we will test these different algorithms to see which perform the best, and which ones\n",
    "#we will continue to try in other pipelines.\n",
    "# - Linear Regression\n",
    "# - Regression Trees\n",
    "# - Regression Forests\n",
    "# - Gradient Boost Models\n",
    "#   - AdaBoost\n",
    "#   - GBM\n",
    "#   - XGBoost\n",
    "# - Support Vector Regression*\n",
    "# - K-NN Regression*\n",
    "# * - I will do these models just to become more familiar with them, but I can already tell that they \n",
    "#     will not outperform XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step4a1: Linear Regression\n",
    "# regressor = LinearRegression(normalize = True)\n",
    "# linmodel = regressor.fit(train_X1,train_y1)\n",
    "# linmodel_p = linmodel.predict(test_X1) #These are the predictions for simple linear regression'\n",
    "# linmodel_p = pd.Series(linmodel_p) #Need to convert it to a series before we can concatenate it.\n",
    "# linmodel_p = pd.concat([linmodel_p,test_IDs.rename('Id')],axis=1) #Add the IDs\n",
    "# linmodel_p = linmodel_p.rename(columns = {0:'SalePrice','Id':'Id'}) #Rename the Columns\n",
    "# linmodel_p = linmodel_p[['Id','SalePrice']] #Switch the order of the columns \n",
    "#Comment out after you run once.\n",
    "#linmodel_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/linmodel_p_09042019.csv')\n",
    "#linmodel_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/linmodel_p_09042019.csv')\n",
    "#BEFORE SUBMITTING THE FILES, MAKE SURE TO DELETE THE INDEX COLUMN IN EXCEL!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step4a2: Ridge Linear Regression\n",
    "#Unlike the regular linear regression, there are MANY MANY POSSIBILITIES TO CHOOSE FROM.\n",
    "\n",
    "#alpha = 0.001\n",
    "# ridge_reg1 = Ridge(alpha = 0.001,normalize = True)\n",
    "# rr_model1 = ridge_reg1.fit(train_X1,train_y1)\n",
    "# rr_model1p = rr_model1.predict(test_X1)\n",
    "# rr_model1p = pd.Series(rr_model1p)\n",
    "# rr_model1p = pd.concat([rr_model1p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model1p = rr_model1p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model1p = rr_model1p[['Id','SalePrice']]\n",
    "\n",
    "#rr_model1p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model1p_09042019.csv')\n",
    "#rr_model1p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model1p_09042019.csv')\n",
    "\n",
    "#alpha = 0.01\n",
    "# ridge_reg2 = Ridge(alpha = 0.01,normalize = True)\n",
    "# rr_model2 = ridge_reg2.fit(train_X1,train_y1)\n",
    "# rr_model2p = rr_model2.predict(test_X1)\n",
    "# rr_model2p = pd.Series(rr_model2p)\n",
    "# rr_model2p = pd.concat([rr_model2p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model2p = rr_model2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model2p = rr_model2p[['Id','SalePrice']]\n",
    "\n",
    "#rr_model2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model2p_09052019.csv')\n",
    "#rr_model2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model2p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.1\n",
    "# ridge_reg3 = Ridge(alpha = 0.1,normalize = True)\n",
    "# rr_model3 = ridge_reg3.fit(train_X1,train_y1)\n",
    "# rr_model3p = rr_model3.predict(test_X1)\n",
    "# rr_model3p = pd.Series(rr_model3p)\n",
    "# rr_model3p = pd.concat([rr_model3p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model3p = rr_model3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model3p = rr_model3p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model3p_09052019.csv')\n",
    "# rr_model3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model3p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.25\n",
    "# ridge_reg4 = Ridge(alpha = 0.25,normalize = True)\n",
    "# rr_model4 = ridge_reg4.fit(train_X1,train_y1)\n",
    "# rr_model4p = rr_model4.predict(test_X1)\n",
    "# rr_model4p = pd.Series(rr_model4p)\n",
    "# rr_model4p = pd.concat([rr_model4p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model4p = rr_model4p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model4p = rr_model4p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model4p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model4p_09052019.csv')\n",
    "# rr_model4p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model4p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.5\n",
    "# ridge_reg5 = Ridge(alpha = 0.5,normalize = True)\n",
    "# rr_model5 = ridge_reg5.fit(train_X1,train_y1)\n",
    "# rr_model5p = rr_model5.predict(test_X1)\n",
    "# rr_model5p = pd.Series(rr_model5p)\n",
    "# rr_model5p = pd.concat([rr_model5p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model5p = rr_model5p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model5p = rr_model5p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model5p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model5p_09052019.csv')\n",
    "# rr_model5p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model5p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.75\n",
    "# ridge_reg6 = Ridge(alpha = 0.75,normalize = True)\n",
    "# rr_model6 = ridge_reg6.fit(train_X1,train_y1)\n",
    "# rr_model6p = rr_model6.predict(test_X1)\n",
    "# rr_model6p = pd.Series(rr_model6p)\n",
    "# rr_model6p = pd.concat([rr_model6p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model6p = rr_model6p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model6p = rr_model6p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model6p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model6p_09052019.csv')\n",
    "# rr_model6p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model6p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 1\n",
    "# ridge_reg7 = Ridge(alpha = 1,normalize = True)\n",
    "# rr_model7 = ridge_reg7.fit(train_X1,train_y1)\n",
    "# rr_model7p = rr_model7.predict(test_X1)\n",
    "# rr_model7p = pd.Series(rr_model7p)\n",
    "# rr_model7p = pd.concat([rr_model7p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model7p = rr_model7p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model7p = rr_model7p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model7p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model7p_09052019.csv')\n",
    "# rr_model7p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model7p_09052019.csv')\n",
    "\n",
    "#alpha = 10\n",
    "# ridge_reg8 = Ridge(alpha = 10,normalize = True)\n",
    "# rr_model8 = ridge_reg8.fit(train_X1,train_y1)\n",
    "# rr_model8p = rr_model8.predict(test_X1)\n",
    "# rr_model8p = pd.Series(rr_model8p)\n",
    "# rr_model8p = pd.concat([rr_model8p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model8p = rr_model8p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model8p = rr_model8p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model8p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model8p_09052019.csv')\n",
    "# rr_model8p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model8p_09052019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#alpha = 2\n",
    "# ridge_reg9 = Ridge(alpha = 2,normalize = True)\n",
    "# rr_model9 = ridge_reg9.fit(train_X1,train_y1)\n",
    "# rr_model9p = rr_model9.predict(test_X1)\n",
    "# rr_model9p = pd.Series(rr_model9p)\n",
    "# rr_model9p = pd.concat([rr_model9p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model9p = rr_model9p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model9p = rr_model9p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model9p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model9p_09052019.csv')\n",
    "# rr_model9p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model9p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 5\n",
    "# ridge_reg10 = Ridge(alpha = 5,normalize = True)\n",
    "# rr_model10 = ridge_reg10.fit(train_X1,train_y1)\n",
    "# rr_model10p = rr_model10.predict(test_X1)\n",
    "# rr_model10p = pd.Series(rr_model10p)\n",
    "# rr_model10p = pd.concat([rr_model10p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model10p = rr_model10p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model10p = rr_model10p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model10p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model10p_09052019.csv')\n",
    "# rr_model10p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model10p_09052019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step4b: Regression Trees \n",
    "\n",
    "#MSE\n",
    "# dt_reg = DecisionTreeRegressor()\n",
    "# dt_model = dt_reg.fit(train_X1,train_y1)\n",
    "# dt_modelp = dt_model.predict(test_X1)\n",
    "# dt_modelp = pd.Series(dt_modelp)\n",
    "# dt_modelp = pd.concat([dt_modelp,test_IDs.rename('Id')],axis=1)\n",
    "# dt_modelp = dt_modelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# dt_modelp = dt_modelp[['Id','SalePrice']]\n",
    "\n",
    "#dt_modelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_modelp_09052019.csv')\n",
    "#dt_modelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_modelp_09052019.csv')\n",
    "\n",
    "\n",
    "#Friedman MSE\n",
    "# dt_reg2 = DecisionTreeRegressor(criterion='friedman_mse')\n",
    "# dt_model2 = dt_reg2.fit(train_X1,train_y1)\n",
    "# dt_model2p = dt_model2.predict(test_X1)\n",
    "# dt_model2p = pd.Series(dt_model2p)\n",
    "# dt_model2p = pd.concat([dt_model2p,test_IDs.rename('Id')],axis=1)\n",
    "# dt_model2p = dt_model2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# dt_model2p = dt_model2p[['Id','SalePrice']]\n",
    "\n",
    "#dt_model2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_model2p_09052019.csv')\n",
    "#dt_model2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_model2p_09052019.csv')\n",
    "\n",
    "\n",
    "#MAE\n",
    "dt_reg3 = DecisionTreeRegressor(criterion='mae')\n",
    "dt_model3 = dt_reg3.fit(train_X1,train_y1)\n",
    "dt_model3p = dt_model3.predict(test_X1)\n",
    "dt_model3p = pd.Series(dt_model3p)\n",
    "dt_model3p = pd.concat([dt_model3p,test_IDs.rename('Id')],axis=1)\n",
    "dt_model3p = dt_model3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "dt_model3p = dt_model3p[['Id','SalePrice']]\n",
    "\n",
    "#dt_model3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_model3p_09052019.csv')\n",
    "#dt_model3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_model3p_09052019.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step4c: Regression Forests\n",
    "#n_estimators = 100, criterion = mse, bootstrap = True\n",
    "# forest100 = RandomForestRegressor(n_estimators = 100)\n",
    "# fmodel100 = forest100.fit(train_X1,train_y1)\n",
    "# fmodel100_p = fmodel100.predict(test_X1)\n",
    "# fmodel100_p = pd.Series(fmodel100_p)\n",
    "# fmodel100_p = pd.concat([fmodel100_p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_p = fmodel100_p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_p = fmodel100_p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_p_09062019.csv')\n",
    "# fmodel100_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 1000, criterion = mse, bootstrap = True\n",
    "# forest1000 = RandomForestRegressor(n_estimators = 1000)\n",
    "# fmodel1000 = forest1000.fit(train_X1,train_y1)\n",
    "# fmodel1000_p = fmodel1000.predict(test_X1)\n",
    "# fmodel1000_p = pd.Series(fmodel1000_p)\n",
    "# fmodel1000_p = pd.concat([fmodel1000_p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel1000_p = fmodel1000_p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel1000_p = fmodel1000_p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel1000_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel1000_p_09062019.csv')\n",
    "# fmodel1000_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel1000_p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 100, criterion = mae, bootstrap = True\n",
    "# forest1002 = RandomForestRegressor(n_estimators = 100,criterion = 'mae')\n",
    "# fmodel1002 = forest1002.fit(train_X1,train_y1)\n",
    "# fmodel100_2p = fmodel1002.predict(test_X1)\n",
    "# fmodel100_2p = pd.Series(fmodel100_2p)\n",
    "# fmodel100_2p = pd.concat([fmodel100_2p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_2p = fmodel100_2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_2p = fmodel100_2p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_2p_09062019.csv')\n",
    "# fmodel100_2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_2p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 1000, criterion = mae, bootstrap = True\n",
    "# forest10002 = RandomForestRegressor(n_estimators = 1000,criterion = 'mae')\n",
    "# fmodel10002 = forest10002.fit(train_X1,train_y1)\n",
    "# fmodel1000_2p = fmodel10002.predict(test_X1)\n",
    "# fmodel1000_2p = pd.Series(fmodel1000_2p)\n",
    "# fmodel1000_2p = pd.concat([fmodel1000_2p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel1000_2p = fmodel1000_2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel1000_2p = fmodel1000_2p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel1000_2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel1000_2p_09062019.csv')\n",
    "# fmodel1000_2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel1000_2p_09062019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#n_estimators = 100, criterion = mse, bootstrap = False\n",
    "# forest1003 = RandomForestRegressor(n_estimators = 100,criterion = 'mse', bootstrap = False)\n",
    "# fmodel1003 = forest1003.fit(train_X1,train_y1)\n",
    "# fmodel100_3p = fmodel1003.predict(test_X1)\n",
    "# fmodel100_3p = pd.Series(fmodel100_3p)\n",
    "# fmodel100_3p = pd.concat([fmodel100_3p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_3p = fmodel100_3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_3p = fmodel100_3p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_3p_09062019.csv')\n",
    "# fmodel100_3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_3p_09062019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#9/10/19 RandomForestRegressor using Cross Validation and Grid Search\n",
    "#using modelfitCV in order to tune some parameters for our random forests.\n",
    "#Using default values for the first run\n",
    "# rf_0 = RandomForestRegressor(random_state = 2, n_estimators=10)\n",
    "# modelfitCV(rf_0, train_X1, train_y1)\n",
    "\n",
    "#########Tuning of the n_estimators parameters 10 -> 100 in steps of 10\n",
    "# param_test1 = {'n_estimators':range(10,101,10)}\n",
    "# gsearch = grid search\n",
    "# gsearch1 = GridSearchCV(estimator = RandomForestRegressor(min_samples_split=14,min_samples_leaf=2, \n",
    "#                                                           max_depth=5,max_features='sqrt',random_state=2),\n",
    "#                        param_grid = param_test1, scoring='neg_mean_squared_log_error',n_jobs=4,iid=False,cv=5)\n",
    "# gsearch1.fit(train_X1,train_y1)\n",
    "#[gsearch1.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch1.best_params_,gsearch1.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of the n_estimators parameters 100 -> 200 in steps of 10\n",
    "# param_test2 = {'n_estimators':range(100,201,10)}\n",
    "# gsearch2 = GridSearchCV(estimator = RandomForestRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,max_features='sqrt',random_state=2),\n",
    "#                        param_grid = param_test2, scoring = 'neg_mean_squared_log_error',n_jobs=4,iid=False,cv=5)\n",
    "# gsearch2.fit(train_X1,train_y1)\n",
    "# [gsearch2.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch2.best_params_,gsearch2.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#########Tuning of max_depth (4->10 in steps of 2) and min_samples_split (10->50 in steps of 5)\n",
    "# param_test3 = {'max_depth':range(4,11,2), 'min_samples_split':range(10,51,5)}\n",
    "# gsearch3 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt',random_state=2),\n",
    "#                        param_grid = param_test3, scoring = 'neg_mean_squared_log_error', n_jobs=4,iid=False,cv=5)\n",
    "# gsearch3.fit(train_X1,train_y1)\n",
    "# [gsearch3.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch3.best_params_,gsearch3.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of max_depth (10->20 in steps of 2) and min_samples_split (2->12 in steps of 2)\n",
    "# param_test4 = {'max_depth':range(10,21,2), 'min_samples_split':range(2,13,2)}\n",
    "# gsearch4 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt',random_state=2),\n",
    "#                        param_grid = param_test4, scoring = 'neg_mean_squared_log_error', n_jobs=4,iid=False,cv=5)\n",
    "# gsearch4.fit(train_X1,train_y1)\n",
    "#results4 = pd.DataFrame(gsearch4.cv_results_)\n",
    "#results4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch4.best_params_, gsearch4.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of min_samples_leaf (1->10 in steps of 1)\n",
    "# param_test5 = {'min_samples_leaf':range(1,11,1)}\n",
    "# gsearch5 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt', max_depth=18, min_samples_split=2, random_state=2),\n",
    "#                        param_grid = param_test5, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch5.fit(train_X1,train_y1)\n",
    "# results5 = pd.DataFrame(gsearch5.cv_results_)\n",
    "#results5.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch5.best_params_, gsearch5.best_score_\n",
    "\n",
    "#Now we will check on how our mean score has improved since when we first ran modelfitCV\n",
    "#rcParams['figure.figsize'] = 12,4 #just in case the graph size isn't the same as before\n",
    "#modelfitCV(gsearch5.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "\n",
    "#########Tuning of max_features (12->40 in steps of 4)\n",
    "# param_test6 = {'max_features':range(12,41,4)}\n",
    "# gsearch6 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,min_samples_split=2,min_samples_leaf=1,random_state=2),\n",
    "#                        param_grid = param_test6, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch6.fit(train_X1,train_y1)\n",
    "# results6 = pd.DataFrame(gsearch6.cv_results_)\n",
    "#results6.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch6.best_params_, gsearch6.best_score_\n",
    "\n",
    "\n",
    "#Use our tuned parameters to create a model. Lets check modelfitCV and then fit our data to the test data.\n",
    "#rfr_tuned = RandomForestRegressor(n_estimators = 160, max_depth=18, min_samples_split=2, min_samples_leaf=1,max_features=36,random_state=2)\n",
    "#modelfitCV(rfr_tuned,train_X1,train_y1)\n",
    "\n",
    "\n",
    "###############################\n",
    "#TRAINING THE 1st TUNED MODEL\n",
    "##############################\n",
    "# rfr_tuned_model = rfr_tuned.fit(train_X1,train_y1)\n",
    "# rfr_tuned_modelp = rfr_tuned_model.predict(test_X1)\n",
    "# rfr_tuned_modelp = pd.Series(rfr_tuned_modelp)\n",
    "# rfr_tuned_modelp = pd.concat([rfr_tuned_modelp,test_IDs.rename('Id')],axis=1)\n",
    "# rfr_tuned_modelp = rfr_tuned_modelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rfr_tuned_modelp = rfr_tuned_modelp[['Id','SalePrice']]\n",
    "\n",
    "# rfr_tuned_modelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rfr_tuned_modelp_09102019.csv')\n",
    "# rfr_tuned_modelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rfr_tuned_modelp_09102019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#########Retuning of min_samples_split (2 -> 30 in steps of 2)\n",
    "# param_test7 = {'min_samples_split':range(2,31,2)}\n",
    "# gsearch7 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,min_samples_leaf=1,max_features=36,random_state=2),\n",
    "#                        param_grid = param_test7, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch7.fit(train_X1,train_y1)\n",
    "# results7 = pd.DataFrame(gsearch7.cv_results_)\n",
    "# results7.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch7.best_params_, gsearch7.best_score_ \n",
    "\n",
    "\n",
    "#########Retuning of min_samples_leaf (1 -> 15 in steps of 1)\n",
    "# param_test8 = {'min_samples_leaf':range(1,16,1)}\n",
    "# gsearch8 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,max_features=36,min_samples_split=2,random_state=2),\n",
    "#                        param_grid = param_test8, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch8.fit(train_X1,train_y1)\n",
    "# results8 = pd.DataFrame(gsearch8.cv_results_)\n",
    "# results8.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# gsearch8.best_params_, gsearch8.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Step4d: AdaBoost using CrossValidation and Grid Search\n",
    "#uncomment when you just open up the notebook\n",
    "#rfr_tuned = RandomForestRegressor(n_estimators = 160, max_depth=18, min_samples_split=2, min_samples_leaf=1,max_features=36,random_state=2)\n",
    "\n",
    "\n",
    "#Create the AdaBoosting model\n",
    "#AB1 = AdaBoostRegressor(rfr_tuned,random_state=2)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "########YOU DONT NEED TO RUN THIS SECTION AGAIN! USE THE NEW VALUES TO TUNE  \n",
    "###########################################################################\n",
    "############Tuning of the learning_rate (randomly generated) and n_estimators (randomly generated)\n",
    "#Creating the random values for the learning rate\n",
    "# r_exp = -4*np.random.rand(20)\n",
    "# alpha = 10**r_exp\n",
    "# param_testAB1 = {'learning_rate':alpha,'n_estimators': np.random.randint(10,100,20)} #the parameters that we are tuning\n",
    "# #rgsearch = random grid search\n",
    "# rgsearchAB1 = RandomizedSearchCV(estimator = AB1, param_distributions = param_testAB1,n_iter=20,scoring = 'neg_mean_squared_log_error',\n",
    "#                                   cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB1.fit(train_X1,train_y1) #This took approx 10 minutes to run #Ran 9/13/19 1:14AM \n",
    "\n",
    "# resultsAB1 = pd.DataFrame(rgsearchAB1.cv_results_) #convert the results to a dataframe to be easily read\n",
    "# resultsAB1.loc[:,('params','mean_test_score','std_test_score')] \n",
    "# rgsearchAB1.best_params_, rgsearchAB1.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitCV(rgsearchAB1.best_estimator_,train_X1,train_y1) #Create a graph to store in our power point\n",
    "\n",
    "# adaboost_tuned1 = rgsearchAB1.best_estimator_\n",
    "# adaboost_p1 = adaboost_tuned1.predict(test_X1)\n",
    "# adaboost_p1 = pd.Series(adaboost_p1)\n",
    "# adaboost_p1 = pd.concat([adaboost_p1,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p1 = adaboost_p1.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p1 = adaboost_p1[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p1.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p1_09122019.csv')\n",
    "# adaboost_p1.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p1_09122019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########YOU ARE HERE AS OF 9/13/19 1:35AM\n",
    "########Further tuning of the learning_rate (0.60 -> 0.75 in steps of 0.01), and the n_estimators (41 -> 59 in steps of 2)\n",
    "# param_testAB2 = {'learning_rate':np.arange(0.6,0.75,0.01),'n_estimators':range(41,60,2)}\n",
    "# rgsearchAB2 = GridSearchCV(estimator = AB1, param_grid = param_testAB2, scoring='neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB2.fit(train_X1,train_y1) #Took about 3 hours.\n",
    "\n",
    "# resultsAB2 = pd.DataFrame(rgsearchAB2.cv_results_)\n",
    "# resultsAB2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rgsearchAB2.best_params_, rgsearchAB2.best_score_\n",
    "\n",
    "#modelfitCV(rgsearchAB2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# adaboost_tuned2 = rgsearchAB2.best_estimator_\n",
    "# adaboost_p2 = adaboost_tuned2.predict(test_X1)\n",
    "# adaboost_p2 = pd.Series(adaboost_p2)\n",
    "# adaboost_p2 = pd.concat([adaboost_p2,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p2 = adaboost_p2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p2 = adaboost_p2[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p2_09132019.csv')\n",
    "# adaboost_p2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p2_09132019.csv')\n",
    "\n",
    "\n",
    "\n",
    "##########Further tuning of the learning_rate (0.75 -> 0.85 steps of 0.02), and the n_estimators (59 -> 71 in steps of 2)\n",
    "# param_testAB3 = {'learning_rate':np.arange(0.75,0.85,0.02),'n_estimators':range(59,72,2)}\n",
    "# rgsearchAB3 = GridSearchCV(estimator = AB1, param_grid = param_testAB3, scoring ='neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB3.fit(train_X1,train_y1)\n",
    "\n",
    "#resultsAB3 = pd.DataFrame(rgsearchAB3.cv_results_)\n",
    "# resultsAB3.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rgsearchAB3.best_params_, rgsearchAB3.best_score_\n",
    "\n",
    "#modelfitCV(rgsearchAB3.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# adaboost_tuned3 = rgsearchAB3.best_estimator_\n",
    "# adaboost_p3 = adaboost_tuned3.predict(test_X1)\n",
    "# adaboost_p3 = pd.Series(adaboost_p3)\n",
    "# adaboost_p3 = pd.concat([adaboost_p3,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p3 = adaboost_p3.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p3 = adaboost_p3[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p3.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p3_09132019.csv')\n",
    "# adaboost_p3.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p3_09132019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Tuning the loss function to see if any improvements will manifest.\n",
    "# AB2 = AdaBoostRegressor(rfr_tuned, learning_rate = 0.77,n_estimators = 67,random_state=2)\n",
    "# param_testAB4 = {'loss': ['linear','square','exponential']}\n",
    "# rgsearchAB4 = GridSearchCV(estimator=AB2, param_grid = param_testAB4, scoring = 'neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB4.fit(train_X1,train_y1)\n",
    "\n",
    "# resultsAB4 = pd.DataFrame(rgsearchAB4.cv_results_)\n",
    "# resultsAB4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#rgsearchAB4.best_params_, rgsearchAB4.best_score_ #linear is the best way to go!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DO NOT DELETE OR MOVE, KEEP THIS CODE HERE.\n",
    "# From a random website on Adaboost (but you had to pay for the rest)\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#The person that coded this created \n",
    "# param_dist = {\n",
    "#  'n_estimators': [50, 100],\n",
    "#  'learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
    "#  'loss' : ['linear', 'square', 'exponential']\n",
    "#  }\n",
    "\n",
    "# pre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(),\n",
    "#  param_distributions = param_dist,\n",
    "#  cv=3,\n",
    "#  n_iter = 10,\n",
    "#  n_jobs=-1)\n",
    "\n",
    "# pre_gs_inst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#Step 4e: Gradient Boosting with RandomizedCrossValidation\n",
    "#Lets see how generic gradient boosting works with no tuning\n",
    "# gb_0 = GradientBoostingRegressor(random_state=5)\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(gb_0, train_X1, train_y1)\n",
    "\n",
    "#Lets see how the base model performs on Kaggle.\n",
    "# GBmodel = gb_0.fit(train_X1,train_y1)\n",
    "# GBmodelp = GBmodel.predict(test_X1)\n",
    "# GBmodelp = pd.Series(GBmodelp)\n",
    "# GBmodelp = pd.concat([GBmodelp,test_IDs.rename('Id')],axis=1)\n",
    "# GBmodelp = GBmodelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBmodelp = GBmodelp[['Id','SalePrice']]\n",
    "\n",
    "# GBmodelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBmodelp_09162019.csv')\n",
    "# GBmodelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBmodelp_09162019.csv')\n",
    "\n",
    "#This regular model outperformed the tuned Adaboost + Regression Trees (Wow)\n",
    "\n",
    "\n",
    "\n",
    "#Random Tuning of learning_rate with n_estimators. \n",
    "#This is not recommended by the article because they affect each other but this is a little experiment that I \n",
    "#want to run to test how good this process will be.\n",
    "\n",
    "###########Tune learning_rate (randomly generated 20 values) and n_estimators (40 -> 140 in steps of 10)\n",
    "# r_expGB = -4*np.random.rand(20)\n",
    "# LR_GB = 10**r_expGB\n",
    "# NE = range(40,141,10)\n",
    "# GB_param_test1 = {'n_estimators':NE,'learning_rate':LR_GB}\n",
    "# GBrgsearch1 = RandomizedSearchCV(estimator = GradientBoostingRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,\n",
    "#                                                                       max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                                  param_distributions=GB_param_test1,n_iter=80, scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "#GBrgsearch1.fit(train_X1,train_y1)\n",
    "\n",
    "#GBresults1 = pd.DataFrame(GBrgsearch1.cv_results_)\n",
    "#GBresults1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#GBrgsearch1.best_params_,GBrgsearch1.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(GBrgsearch1.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB1 = GBrgsearch1.best_estimator_\n",
    "# GB_model1 = GB1.fit(train_X1,train_y1)\n",
    "# GBp1 = GB_model1.predict(test_X1)\n",
    "# GBp1 = pd.Series(GBp1)\n",
    "# GBp1 = pd.concat([GBp1,test_IDs.rename('Id')],axis=1)\n",
    "# GBp1 = GBp1.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp1 = GBp1[['Id','SalePrice']]\n",
    "\n",
    "# GBp1.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp1_09162019.csv')\n",
    "# GBp1.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp1_09162019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########Tune learning_rate (130 -> 200 steps of 10) and n_estimators (0.040 -> 0.060 in steps of 0.002)\n",
    "# GB_param_test2 = {'n_estimators':range(130,201,10),'learning_rate':np.arange(0.040,0.061,0.002)}\n",
    "# GBgsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,\n",
    "#                                                                 max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                           param_grid=GB_param_test2,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch2.fit(train_X1,train_y1)\n",
    "\n",
    "# GBresults2 = pd.DataFrame(GBgsearch2.cv_results_)\n",
    "# GBresults2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# GBgsearch2.best_params_,GBgsearch2.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(GBgsearch2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB2 = GBgsearch2.best_estimator_\n",
    "# GB_model2 = GB2.fit(train_X1,train_y1)\n",
    "# GBp2 = GB_model2.predict(test_X1)\n",
    "# GBp2 = pd.Series(GBp2)\n",
    "# GBp2 = pd.concat([GBp2,test_IDs.rename('Id')],axis=1)\n",
    "# GBp2 = GBp2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp2 = GBp2[['Id','SalePrice']]\n",
    "\n",
    "# GBp2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp2_09172019.csv')\n",
    "# GBp2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp2_09172019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########Tune n_estimators (180 -> 250 in steps of 10) and max_depth (4 -> 10 steps of 1)\n",
    "# GB_param_test3 = {'n_estimators':range(180,251,10),'max_depth':range(4,11,1)}\n",
    "# GBgsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test3,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch3.fit(train_X1,train_y1)\n",
    "\n",
    "# GBresults3 = pd.DataFrame(GBgsearch3.cv_results_)\n",
    "# GBresults3.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# GBgsearch3.best_params_,GBgsearch3.best_score_\n",
    "\n",
    "\n",
    "##########Tune n_estimators (230 -> 300 in steps of 10) and max_depth (3 -> 7 steps of 1)\n",
    "# GB_param_test4 = {'n_estimators':range(230,301,10),'max_depth':range(3,8,1)}\n",
    "# GBgsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test4,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch4.fit(train_X1,train_y1)\n",
    "# GBgsearch4.best_params_,GBgsearch4.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#########Tune n_estimators (190 -> 390 in steps of 10) \n",
    "# GB_param_test5 = {'n_estimators':range(190,391,10)}\n",
    "# GBgsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_depth=4,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test5,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch5.fit(train_X1,train_y1)\n",
    "# GBgsearch5.best_params_,GBgsearch5.best_score_\n",
    "\n",
    "\n",
    "########Tune min_samples_split (4 -> 20 steps of 1) and min_samples_leaf (2 -> 10 steps of 1)\n",
    "# GB_param_test6 = {'min_samples_split':range(4,21,1),'min_samples_leaf':range(2,11,1)}\n",
    "# GBgsearch6 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,n_estimators=380,max_depth=4,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test6,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch6.fit(train_X1,train_y1)\n",
    "# GBgsearch6.best_params_,GBgsearch6.best_score_\n",
    "\n",
    "# modelfitCV(GBgsearch6.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB6 = GBgsearch6.best_estimator_\n",
    "# GBmodel6 = GB6.fit(train_X1,train_y1)\n",
    "# GBp6 = GBmodel6.predict(test_X1)\n",
    "# GBp6 = pd.Series(GBp6)\n",
    "# GBp6 = pd.concat([GBp6,test_IDs.rename('Id')],axis=1)\n",
    "# GBp6 = GBp6.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp6 = GBp6[['Id','SalePrice']]\n",
    "\n",
    "# GBp6.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp6_09172019.csv')\n",
    "# GBp6.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp6_09172019.csv')\n",
    "\n",
    "\n",
    "#########Tune max_features (10 -> 32 steps of 2) and subsample (0.4->0.8 steps of 0.1)\n",
    "# GB_param_test7 = {'max_features':range(10,33,2),'subsample':np.arange(0.4,0.9,0.1)}\n",
    "# GBgsearch7 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,n_estimators=380,max_depth=4,min_samples_split=19,min_samples_leaf=2,random_state=5),\n",
    "#                          param_grid=GB_param_test7,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch7.fit(train_X1,train_y1)\n",
    "# GBgsearch7.best_params_,GBgsearch7.best_score_\n",
    "\n",
    "# modelfitCV(GBgsearch7.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB7 = GBgsearch7.best_estimator_\n",
    "# GBmodel7 = GB7.fit(train_X1,train_y1)\n",
    "# GBp7 = GBmodel7.predict(test_X1)\n",
    "# GBp7 = pd.Series(GBp7)\n",
    "# GBp7 = pd.concat([GBp7,test_IDs.rename('Id')],axis=1)\n",
    "# GBp7 = GBp7.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp7 = GBp7[['Id','SalePrice']]\n",
    "\n",
    "# GBp7.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp7_09172019.csv')\n",
    "# GBp7.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp7_09172019.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "#Step 4f: XGBoost (9/18/19)\n",
    "#Following the instructions from the paper.\n",
    "#First starting model\n",
    "\n",
    "# This code can remove the warning you get because its not that serious, but just in case. \n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "#############Untuned model\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=6,min_child_weight=1,gamma=0,subsample=0.6,\n",
    "#                    colsample_bytree=0.1,scale_pos_weight=1,seed=13,objective='reg:squarederror')\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb1,train_X1,train_y1)\n",
    "\n",
    "# XGBmodel = xgb1.fit(train_X1,train_y1)\n",
    "# XGBp = XGBmodel.predict(test_X1)\n",
    "# XGBp = pd.Series(XGBp)\n",
    "# XGBp = pd.concat([XGBp,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp = XGBp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp = XGBp[['Id','SalePrice']]\n",
    "\n",
    "# XGBp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp_09182019.csv')\n",
    "# XGBp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#############Tune max_depth and min_child_weight \n",
    "# XGB_param_test1 = {'max_depth':range(2,13,2),'min_child_weight':range(1,6,1)}\n",
    "# XGBgsearch1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),param_grid=XGB_param_test1,\n",
    "#                           scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch1.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults1 = pd.DataFrame(XGBgsearch1.cv_results_)\n",
    "# XGBresults1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch1.best_params_,XGBgsearch1.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#############Finetune max_depth and min_child_weight even more.\n",
    "# XGB_param_test2 = {'max_depth':range(3,7,1),'min_child_weight':range(2,6,1)}\n",
    "# XGBgsearch2 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                            param_grid=XGB_param_test2,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch2.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults2 = pd.DataFrame(XGBgsearch2.cv_results_)\n",
    "# XGBresults2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch2.best_params_,XGBgsearch2.best_score_\n",
    "\n",
    "# modelfitXGB(XGBgsearch2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# XGB2 = XGBgsearch2.best_estimator_\n",
    "# XGBmodel2 = XGB2.fit(train_X1,train_y1)\n",
    "# XGBp2 = XGBmodel2.predict(test_X1)\n",
    "# XGBp2 = pd.Series(XGBp2)\n",
    "# XGBp2 = pd.concat([XGBp2,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp2 = XGBp2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp2 = XGBp2[['Id','SalePrice']]\n",
    "\n",
    "# XGBp2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp2_09182019.csv')\n",
    "# XGBp2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp2_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "###########Tune gamma\n",
    "# XGB_param_test3 = {'gamma':np.arange(0.0,1.0,0.1)}\n",
    "# XGBgsearch3 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,max_depth=3,min_child_weight=2,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                  objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test3,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch3.fit(train_X1,train_y1)\n",
    "# XGBresults3 = pd.DataFrame(XGBgsearch3.cv_results_)\n",
    "# XGBresults3.loc[:,('params','mean_test_score','std_test_score')] #All these values are the same. In other words, Gamma doesn't really matter.\n",
    "#XGBgsearch3.best_params_,XGBgsearch3.best_score_\n",
    "\n",
    "\n",
    "##########Tune subsample and colsample_bytree\n",
    "# XGB_param_test4 = {'subsample':np.arange(0.3,1.0,0.1),'colsample_bytree':np.arange(0.1,1.0,0.1)}\n",
    "# XGBgsearch4 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                                                  objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test4,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch4.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults4 = pd.DataFrame(XGBgsearch4.cv_results_)\n",
    "# XGBresults4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch4.best_params_,XGBgsearch4.best_score_\n",
    "\n",
    "# xgb_recalibrate = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate,train_X1,train_y1) #the recalibration shows n_estimators = 100 is optimal.\n",
    "\n",
    "\n",
    "#Check on how this tuned model does on Kaggle. <- It got worse! It may be overfitting the training data\n",
    "# XGB4 = XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodel4 = XGB4.fit(train_X1,train_y1)\n",
    "# XGBp4 = XGBmodel4.predict(test_X1)\n",
    "# XGBp4 = pd.Series(XGBp4)\n",
    "# XGBp4 = pd.concat([XGBp4,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp4 = XGBp4.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp4 = XGBp4[['Id','SalePrice']]\n",
    "\n",
    "# XGBp4.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp4_09182019.csv')\n",
    "# XGBp4.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp4_09182019.csv')\n",
    "\n",
    "\n",
    "##########Tune 1st regularization parameter reg_alpha\n",
    "# XGB_param_test5 = {'reg_alpha':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "# XGBgsearch5 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test5,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch5.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults5 = pd.DataFrame(XGBgsearch5.cv_results_)\n",
    "# XGBresults5.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch5.best_params_,XGBgsearch5.best_score_\n",
    "\n",
    "\n",
    "##########Tune 2nd regularization parameter reg_lambda\n",
    "# XGB_param_test6 = {'reg_lambda':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "# XGBgsearch6 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test6,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch6.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults6 = pd.DataFrame(XGBgsearch6.cv_results_)\n",
    "# XGBresults6.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#XGBgsearch6.best_params_,XGBgsearch6.best_score_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Fine tune reg_lambda\n",
    "# XGB_param_test7 = {'reg_lambda':np.arange(0.3,2.0,0.1)}\n",
    "# XGBgsearch7 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test7,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch7.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults7 = pd.DataFrame(XGBgsearch7.cv_results_)\n",
    "# XGBresults7.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#XGBgsearch7.best_params_,XGBgsearch7.best_score_\n",
    "\n",
    "\n",
    "# xgb_recalibrate2 = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate2,train_X1,train_y1) #the recalibration shows n_estimators = 213 is optimal.\n",
    "\n",
    "# Check how it performs after the model is mostly tuned.\n",
    "# XGB7 = XGBRegressor(learning_rate=0.1,n_estimators=213,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodel7 = XGB7.fit(train_X1,train_y1)\n",
    "# XGBp7 = XGBmodel7.predict(test_X1)\n",
    "# XGBp7 = pd.Series(XGBp7)\n",
    "# XGBp7 = pd.concat([XGBp7,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp7 = XGBp7.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp7 = XGBp7[['Id','SalePrice']]\n",
    "\n",
    "# XGBp7.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp7_09182019.csv')\n",
    "# XGBp7.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp7_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "##########Reducing the learning_rate = 0.01 to see if the new n_estimators will improve the model.\n",
    "# xgb_recalibrate3 = XGBRegressor(learning_rate=0.01,n_estimators=5000,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                                subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate3,train_X1,train_y1) #the recalibration shows n_estimators = 2438\n",
    "\n",
    "# Final tuned model \n",
    "# XGBF = XGBRegressor(learning_rate=0.01,n_estimators=2438,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodelF = XGBF.fit(train_X1,train_y1)\n",
    "# XGBpF = XGBmodelF.predict(test_X1)\n",
    "# XGBpF = pd.Series(XGBpF)\n",
    "# XGBpF = pd.concat([XGBpF,test_IDs.rename('Id')],axis=1)\n",
    "# XGBpF = XGBpF.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBpF = XGBpF[['Id','SalePrice']]\n",
    "\n",
    "# XGBpF.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBpF_09182019.csv')\n",
    "# XGBpF.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBpF_09182019.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# #NOT NEEDED CODE (FOR NOW)\n",
    "# #Check the amount of missing values for each column. \n",
    "# #np.sum(train_X.isnull())\n",
    "#\n",
    "#\n",
    "#\n",
    "# #LOOKING AT THE DISTRIBUTION OF DIFFERENT VARIABLES (ENDED 5/14/19)\n",
    "# #Plot a histogram of some of the data.\n",
    "# #CATEGORICAL VARIABLES\n",
    "# curr_col = train_X.Neighborhood\n",
    "# curr_col.value_counts().plot(kind='bar')\n",
    "# curr_col.value_counts()\n",
    "#\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "#\n",
    "# curr_var = train_house.loc[train_house.Neighborhood == neighborhood[24],'SalePrice']\n",
    "# curr_var.hist(bins=25)\n",
    "# curr_var.describe()\n",
    "#\n",
    "# #LOOKING AT SPECIFIC COLUMNS TO SEE HOW WELL THEY AFFECT THE SALES PRICE \n",
    "#\n",
    "# var1 = train_house.loc[train_house.Alley == 'Pave', :]\n",
    "# var2 = train_house.loc[train_house.Alley == 'Grvl', :]\n",
    "# var3 = train_house.loc[train_house.SaleType == 'COD', :]\n",
    "# var4 = train_house.loc[train_house.SaleType == 'ConLD', :]\n",
    "# var5 = train_house.loc[train_house.SaleType == 'ConLw', :]\n",
    "# var6 = train_house.loc[train_house.SaleType == 'ConLI', :]\n",
    "# var7 = train_house.loc[train_house.SaleType == 'CWD', :]\n",
    "# var8 = train_house.loc[train_house.SaleType == 'Oth', :]\n",
    "# var9 = train_house.loc[train_house.SaleType == 'Con', :]\n",
    "#\n",
    "#\n",
    "# var1.SalePrice.hist(bins=25)\n",
    "# var1.SalePrice.describe()\n",
    "#\n",
    "# var2.SalePrice.hist(bins=25)\n",
    "# var2.SalePrice.describe()\n",
    "#\n",
    "# var3.SalePrice.hist(bins=25)\n",
    "# var3.SalePrice.describe()\n",
    "#\n",
    "# var4.SalePrice.hist(bins=25)\n",
    "# var4.SalePrice.describe()\n",
    "#\n",
    "# var5.SalePrice.hist(bins=25)\n",
    "# var5.SalePrice.describe()\n",
    "#\n",
    "# var6.SalePrice.hist(bins=25)\n",
    "# var6.SalePrice.describe()\n",
    "#\n",
    "# var7.SalePrice.hist(bins=25)\n",
    "# var7.SalePrice.describe()\n",
    "#\n",
    "# var8.SalePrice.hist(bins=25)\n",
    "# var8.SalePrice.describe()\n",
    "#\n",
    "# var9.SalePrice.hist(bins=25)\n",
    "# var9.SalePrice.describe()\n",
    "#\n",
    "# #Plot a histogram of some of the data \n",
    "# #NUMERICAL VARIABLES \n",
    "# num_col = train_X.loc[:,'MiscVal']\n",
    "# num_col.hist(bins = 50)\n",
    "#\n",
    "# num_col.describe()\n",
    "# num_col.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# #Looking at the house with the largest lot area to see if it makes sense.\n",
    "# #train_house.loc[train_house.LotArea.idxmax(),:]\n",
    "\n",
    "# #Summary\n",
    "# # The idea I am getting of this house is that it is:\n",
    "# # - A house not surrounded by any other houses.\n",
    "# # - It has a total of ~4000 sq ft of house, and ~210,000 sq ft of land surronding it\n",
    "# # - Bit of an old school (1965), brick faced, hip roof style house. Kind of like a haunted looking house with brick face\n",
    "# # - All of this factored in with the quality of the house makes sense why its not the most expensive house even if it \n",
    "# # the largest. \n",
    "# # In conclusion, this house can stay in this dataset.\n",
    "\n",
    "# train_house.SalePrice.hist(bins = 25)\n",
    "# train_house.SalePrice.describe()\n",
    "\n",
    "# LQ = train_house.loc[train_house.LowQualFinSF > 0 ,]\n",
    "# LQ.SalePrice.hist(bins=25)\n",
    "\n",
    "# InsidePorches = train_house.loc[train_house.EnclosedPorch > 0, ]\n",
    "# InsidePorches.EnclosedPorch.hist(bins=25)\n",
    "# InsidePorches.EnclosedPorch.describe()\n",
    "\n",
    "# InsidePorches.SalePrice.hist(bins=25)\n",
    "\n",
    "# SznP = train_house.loc[train_house.loc[:,'3SsnPorch']>0,]\n",
    "# SznP.SalePrice.hist(bins=25)\n",
    "#\n",
    "# screenporch = train_house.loc[train_house.ScreenPorch >0,]\n",
    "# screenporch.SalePrice.hist(bins=25)\n",
    "\n",
    "# pool_niggas = train_house.loc[train_house.PoolArea >0,]\n",
    "# pool_niggas.SalePrice.hist(bins=25)\n",
    "\n",
    "# misc_val = train_house.loc[train_house.MiscVal > 0,]\n",
    "# misc_val.SalePrice.hist(bins=25)\n",
    "\n",
    "\n",
    "#For BsmtExposure variable\n",
    "# inds2 = train_house.BsmtExposure.loc[pd.isnull(train_house.BsmtExposure)]\n",
    "# inds2 = inds2.index\n",
    "\n",
    "\n",
    "# for x in inds2:\n",
    "#      if(sum(inds == x) == 0):\n",
    "#         index = x\n",
    "        \n",
    "# index\n",
    "\n",
    "# #For BsmtExposure variable\n",
    "# inds2 = train_house.BsmtFinType2.loc[pd.isnull(train_house.BsmtFinType2)]\n",
    "# inds2 = inds2.index\n",
    "\n",
    "\n",
    "# for x in inds2:\n",
    "#      if(sum(inds == x) == 0):\n",
    "#         index = x\n",
    "        \n",
    "# index\n",
    "\n",
    "\n",
    "# #Checking out the condition and quality of houses that have terrible functionality scores to see what the missing \n",
    "# #functional values could possibly be.\n",
    "\n",
    "# combined_df.loc[combined_df.Functional == 'Maj2',:]\n",
    "\n",
    "#SPECIAL CELL *NEED TO FIND A WAY TO INCORPORATE THIS CELL WITH THE ONE BELOW IT.\n",
    "#\n",
    "#Gotta make some corrections for the following columns: BsmtExposure, and BsmtFinType2\n",
    "#train_X.loc[948,'BsmtExposure'] = 'No'\n",
    "#\n",
    "#Comment back in and Comment out once you run ONCE!\n",
    "#train_X = train_X.drop(332) #we are dropping this record because we do not know what to fill in for BsmtFinType2 for index = 332\n",
    "#must do the same thing for our Y dataset\n",
    "#train_y = train_y.drop(332)\n",
    "# combined_df = combined_df.drop(combined_df.index[[2150,2119,2187,1554,2215,2472,2575,2488,1914,1944,2249,2903]])\n",
    "\n",
    "\n",
    "# #COMBINING THE TRAINING AND TESTING DATASETS TO CREATE A SUPER DATA SET.\n",
    "# combined_df = pd.concat([train_X,test_X])\n",
    "# combined_df = combined_df.reset_index(drop = True) #the drop variables removes the old indices. Otherwise it gets created as a new column\n",
    "# #combined_df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########We need to handle all the missing values in the following columns:\n",
    "# #THESE WERE HANDLED FROM THE TRAIN_X DATASET.\n",
    "# #LotFrontage = 484\n",
    "# #Alley = 2709\n",
    "# #MasVnrType = 24 \n",
    "# #MasVnrArea = 23\n",
    "# #BsmtQual = 76\n",
    "# #BsmtCond = 77\n",
    "# #BsmtExposure = 76\n",
    "# #BsmtFinType1 = 74\n",
    "# #BsmtFinType2 = 74\n",
    "# #FireplaceQu = 1412\n",
    "# #GarageType = 156\n",
    "# #GarageYrBlt = 157\n",
    "# #GarageFinish = 157\n",
    "# #GarageQual = 157\n",
    "# #GarageCond = 157\n",
    "# #PoolQC = 2896\n",
    "# #Fence = 2337\n",
    "# #MiscFeature = 2802\n",
    "\n",
    "# #THESE ARE NEW FEATURES WITH MISSING VALUES ADDED FROM TEST_X DATASET (EXCEPT ELECTRICAL).\n",
    "# #THESE WILL ALL BE REMOVED (12 ROWS IN TOTAL)\n",
    "# #MSZoning = 4, Utilities = 2, Exterior1st = 1, Exterior2nd = 1, BsmtFinSF1 = 1, BsmtFinSF2 = 1, BsmtUnfSF = 1, TotalBsmtSF = 1, \n",
    "# #BsmtFullBath = 2, BsmtHalfBath = 2, KitchenQual = 1, Functional = 2, GarageCars = 1, GarageArea = 1, SaleType = 1, Electrical = 1\n",
    "\n",
    "# #Removing some indices that I discovered from preliminary research along with some bad data from the columns listed above\n",
    "# #332 -> BsmtFinType2 is NaN while the other basement variables are okay, so I didn't know what to replace this with.\n",
    "# #948 -> BsmtExposure was NaN while other basement variables are okay.\n",
    "# #1379 -> Removed the Electrical NaN in the dataset, it is stupid to keep this.\n",
    "# #2151 - 2904 -> Related to all of the missing values listed above.\n",
    "# combined_df = combined_df.drop(combined_df.index[[332,948,1379,2151,2120,2188,1555,2216,2473,2576,2489,1915,1945,2250,2904]])\n",
    "# combined_df = combined_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "# #Uncomment this once we've handled the missing values that need to be removed. Do this for the\n",
    "# #combined dataset, not just the training dataset.\n",
    "# #First we will do the transformations on practice_train_X to make sure it does what we really want to do.\n",
    "# # train_X1 = train_X1.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "# #                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "# #                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None',\n",
    "# #                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 'None',\n",
    "# #                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "# #                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "\n",
    "\n",
    "# #LOOKING AT SUSPICIOUS MISSING VALUES\n",
    "# combined_df.loc[combined_df.BsmtCond.isnull()==True,]\n",
    "\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "\n",
    "\n",
    "#Left over code from checking different attributes of combined_df\n",
    "#combined_df.loc[combined_df.SaleType.isnull() == True,:]\n",
    "#Used this snippet to coordinate the row #'s with their listed index\n",
    "#combined_df.index[2903]\n",
    "#combined_df.loc[combined_df.Id == 2905,:]\n",
    "# test_X1 = test_X1.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "#                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "#                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None','Electrical' : 'NA',\n",
    "#                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 'None',\n",
    "#                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "\n",
    "\n",
    "#The bad indices were due to GarageYrBlt and I replaced the missing values with None instead\n",
    "#of a number (like 0).\n",
    "#row1 = train_X1.loc[0,:]\n",
    "#len(row1[row1=='None'])\n",
    "# bad_indices = []\n",
    "# for i in range(train_X1.shape[0]):\n",
    "#     curr_row = train_X1.loc[i,:]\n",
    "#     if(len(curr_row[curr_row=='None']) != 0):\n",
    "#         bad_indices.append(i)\n",
    "# train_X1.loc[bad_indices,:]\n",
    "\n",
    "#pd.unique(train_X1.LandSlope)\n",
    "\n",
    "# #Used to get the current directory\n",
    "# import os \n",
    "# os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
