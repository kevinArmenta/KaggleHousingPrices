{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "############# BASIC PACKAGES TO IMPORT ############\n",
    "import os\n",
    "import pandas as pd #To allow us to work with dataframes\n",
    "import numpy as np #To allow us to make mathematical transformations\n",
    "import matplotlib.mlab as mlab #To create plots\n",
    "import matplotlib.pylab as plt #To create plots\n",
    "%matplotlib inline \n",
    "from matplotlib.pylab import rcParams\n",
    "from matplotlib.colors import ListedColormap\n",
    "rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "from mpl_toolkits import mplot3d\n",
    "import category_encoders as ce #To encode our nominal and categorical variables\n",
    "from sklearn import preprocessing, metrics #This module can be helpful when processing data\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, cross_val_score, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import uniform, chi2_contingency, chisquare\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import pylab as py\n",
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import scipy as sp #To play with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.7/site-packages (1.1.0-SNAPSHOT)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.17.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.7/site-packages (from xgboost) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "############# Models to import #############\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "import sklearn.neighbors\n",
    "from sklearn import neighbors\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "#from xgboost import XGBClassifier\n",
    "import sys \n",
    "!{sys.executable} -m pip install xgboost\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "#updating xgboost and scipy to get rid of an error (9/18/19)\n",
    "#RUN IN TERMINAL\n",
    "# pip install --upgrade pip\n",
    "# pip install --upgrade xgboost \n",
    "# pip install --upgrade scipy\n",
    "# pip install --upgrade sklearn\n",
    "# pip install --upgrade plotly\n",
    "# pip install --upgrade pydotplus \n",
    "# pip install --upgrade graphviz\n",
    "# Use 'brew' instead of pip for updates to get the right packages on your computer.\n",
    "\n",
    "\n",
    "#I don't believe that we need this if we export the entire sklearn library. I will comment these out \n",
    "#until it's time to start training models.\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################### PROCESS THE DATA ##########################\n",
    "\n",
    "######### PREPPING THE DATASET #########\n",
    "#Load in the test and train data\n",
    "train_house = pd.read_csv('train.csv')\n",
    "test_house = pd.read_csv('test.csv')\n",
    "\n",
    "######## IMPORT THE DATSET ########\n",
    "#Take a look at a summary of the training data.\n",
    "#train_house.describe()\n",
    "#We will seperate the training set into features (train_X) and the predictor variable (train_y)\n",
    "train_X = train_house.loc[:,train_house.columns != 'SalePrice']\n",
    "train_y = train_house.SalePrice\n",
    "\n",
    "#There are no predictions to compare with, you submit them on Kaggle.\n",
    "test_X = test_house\n",
    "test_IDs = test_X.loc[:,'Id']\n",
    "\n",
    "#This is used so we can see the full output display of the iPython Notebook.\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#Look at the column names\n",
    "#train_X.columns\n",
    "#Verified that all the column names are the names on the sheet of paper \n",
    "#test_X.shape  (1459,80)\n",
    "#train_X.shape (1460, 80)\n",
    "\n",
    "#Look at what the values look like for each column\n",
    "#train_X.MiscVal\n",
    "#train_X.OverallCond.dtypes\n",
    "\n",
    "#LETS LOOK AT HISTS OF VARS IF WE NEED TO.\n",
    "#NUMERICAL VARIABLES \n",
    "#curr_col = train_house.LotFrontage\n",
    "#curr_col.hist(bins=25)\n",
    "#curr_col.value_counts()\n",
    "\n",
    "#CAT VARIABLES\n",
    "# curr_col = train_X.BsmtExposure\n",
    "# curr_col.value_counts().plot(kind='bar')\n",
    "# curr_col.value_counts()\n",
    "\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "\n",
    "# curr_var = train_house.loc[train_house.Neighborhood == neighborhood[24],'SalePrice']\n",
    "# curr_var.hist(bins=25)\n",
    "# curr_var.describe()\n",
    "\n",
    "#Checking out the null values for each column where there are null values.\n",
    "#inds = train_house.BsmtQual.loc[pd.isnull(train_house.BsmtQual)]\n",
    "#inds = inds.index\n",
    "#train_house.loc[inds,]\n",
    "\n",
    "\n",
    "######## HANDLING MISSING VALUES <ONLY RUN ONCE>\n",
    "\n",
    "#THESE WERE HANDLED FROM THE TRAIN_X DATASET.\n",
    "#LotFrontage = 484\n",
    "#Alley = 2709\n",
    "#MasVnrType = 24 \n",
    "#MasVnrArea = 23\n",
    "#BsmtQual = 76\n",
    "#BsmtCond = 77\n",
    "#BsmtExposure = 76\n",
    "#BsmtFinType1 = 74\n",
    "#BsmtFinType2 = 74\n",
    "#FireplaceQu = 1412\n",
    "#GarageType = 156\n",
    "#GarageYrBlt = 157\n",
    "#GarageFinish = 157\n",
    "#GarageQual = 157\n",
    "#GarageCond = 157\n",
    "#PoolQC = 2896\n",
    "#Fence = 2337\n",
    "#MiscFeature = 2802\n",
    "\n",
    "#THESE ARE NEW FEATURES WITH MISSING VALUES ADDED FROM TEST_X DATASET (EXCEPT ELECTRICAL).\n",
    "#THESE WILL ALL BE REMOVED (12 ROWS IN TOTAL)\n",
    "#MSZoning = 4, Utilities = 2, Exterior1st = 1, Exterior2nd = 1, BsmtFinSF1 = 1, BsmtFinSF2 = 1, BsmtUnfSF = 1, TotalBsmtSF = 1, \n",
    "#BsmtFullBath = 2, BsmtHalfBath = 2, KitchenQual = 1, Functional = 2, GarageCars = 1, GarageArea = 1, SaleType = 1, Electrical = 1\n",
    "\n",
    "#Removing some indices that I discovered from preliminary research along with some bad data from the columns listed above\n",
    "#332 -> BsmtFinType2 is NaN while the other basement variables are okay, so I didn't know what to replace this with.\n",
    "#948 -> BsmtExposure was NaN while other basement variables are okay.\n",
    "#1379 -> Removed the Electrical NaN in the dataset, it is stupid to keep this.\n",
    "#We can still clean up the training data and throw some rows out but we cannot do this for the\n",
    "#testing data because Kaggle requires all 1459 rows to be intact. Therefore, we need to fix the \n",
    "#the NaN's in the testing set by setting them as NA instead.\n",
    "train_X = train_X.drop(train_X.index[[332,948,1379]])\n",
    "train_X = train_X.reset_index(drop = True)\n",
    "train_y = train_y.drop(train_y.index[[332,948,1379]])\n",
    "train_y = train_y.reset_index(drop = True)\n",
    "\n",
    "train_IDs = train_X.Id #Save these so we can extract the training data from the combined_df later\n",
    "test_IDs = test_X.Id #Save these so we can extract the testing datra from the combined_df later\n",
    "\n",
    "\n",
    "#COMBINING THE TRAINING AND TESTING DATASETS TO CREATE A SUPER DATA SET.\n",
    "combined_df = pd.concat([train_X,test_X])\n",
    "combined_df = combined_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "combined_df = combined_df.fillna({'MSZoning':'NA','LotFrontage' : 0,'Utilities':'NA','Alley' : 'No Alley','MasVnrType': 'NA',\n",
    "                                  'MasVnrArea':0,'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', 'Exterior1st':'NA',\n",
    "                                  'Exterior2nd':'NA','BsmtFinType1' : 'None','BsmtFinSF1':0,'BsmtFinType2':'None',\n",
    "                                  'BsmtFinSF2':0,'BsmtUnfSF':0,'TotalBsmtSF':0,'BsmtFullBath':0,'BsmtHalfBath':0,\n",
    "                                  'KitchenQual':'NA','Functional':'NA','FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "                                  'GarageFinish' : 'None','GarageCars':0,'GarageArea':0,'GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "                                  'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None','SaleType':'NA'})\n",
    "\n",
    "\n",
    "##########Remove the ID column because it essentially just numbers the rows. ########## \n",
    "#Also, lets instantiate new variables so that anything that we do to them won't be reflected in the original \n",
    "#dataset, and won't affect the other pipelines I plan to create in the future. \n",
    "#train_X1 = train_X.loc[:,train_X.columns != 'Id']\n",
    "#train_y1 = train_y\n",
    "#test_X1 = test_X .loc[:,test_X.columns != 'Id']\n",
    "combined_df1 = combined_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Convert all ORD variables into numbers, that way we can use them in regression. #########\n",
    "#I will try to use category_encoder for basic ordinal encoding.\n",
    "\n",
    "#8/21/19\n",
    "#THIS IS THE CORRECT FORMATING FOR THE CATEGORICAL_ENCODER MODULE!!! \n",
    "#THE DOCUMENTATION THAT I WAS USING WAS OUTDATED!!!\n",
    "\n",
    "#We create a STANDARD mapping for all ORDINAL VARIABLES. \n",
    "ordinal_cols_mapping1 = [{\"col\":\"Utilities\",\n",
    "                            \"mapping\": {'AllPub':4,\n",
    "                                       'NoSewr':3,\n",
    "                                       'NoSeWa':2,\n",
    "                                       'ELO':1}},\n",
    "                         {\"col\":\"LandSlope\",\n",
    "                            \"mapping\":{'Sev':3,\n",
    "                                       'Mod':2,\n",
    "                                       'Gtl':1}},\n",
    "                         {\"col\":\"ExterQual\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"ExterCond\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"BsmtQual\",\n",
    "                            \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtCond\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtExposure\",\n",
    "                             \"mapping\":{'Gd':4,\n",
    "                                        'Av':3,\n",
    "                                        'Mn':2,\n",
    "                                        'No':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtFinType1\",\n",
    "                             \"mapping\":{'GLQ':6,\n",
    "                                        'ALQ':5,\n",
    "                                        'BLQ':4,\n",
    "                                        'Rec':3,\n",
    "                                        'LwQ':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"BsmtFinType2\",\n",
    "                             \"mapping\":{'GLQ':6,\n",
    "                                        'ALQ':5,\n",
    "                                        'BLQ':4,\n",
    "                                        'Rec':3,\n",
    "                                        'LwQ':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"HeatingQC\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"KitchenQual\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1}},\n",
    "                         {\"col\":\"FireplaceQu\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageFinish\",\n",
    "                             \"mapping\":{'Fin':3,\n",
    "                                        'RFn':2,\n",
    "                                        'Unf':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageQual\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"GarageCond\",\n",
    "                             \"mapping\":{'Ex':5,\n",
    "                                        'Gd':4,\n",
    "                                        'TA':3,\n",
    "                                        'Fa':2,\n",
    "                                        'Po':1,\n",
    "                                        'None':0}},\n",
    "                         {\"col\":\"PoolQC\",\n",
    "                             \"mapping\":{'Ex':4,\n",
    "                                        'Gd':3,\n",
    "                                        'TA':2,\n",
    "                                        'Fa':1,\n",
    "                                        'None':0}}\n",
    "                       ]\n",
    "\n",
    "#We input these columns and there corresponding dictionaries into ce.OrdinalEncoder in order to swap these values out\n",
    "ce_ord = ce.OrdinalEncoder(mapping = ordinal_cols_mapping1,return_df = True)\n",
    "#Now we have to fit the encoder to our training data.\n",
    "#train_X1 = ce_ord.fit_transform(train_X1,train_y1) #This doesn't create a combined dataframe like I originally thought, this is just the X dataframe.\n",
    "#test_X1 = ce_ord.fit_transform(test_X1)\n",
    "combined_df1 = ce_ord.fit_transform(combined_df1)\n",
    "\n",
    "\n",
    "##########Convert all CAT variables into numerical values. ##########\n",
    "#We need to examine each CAT column and determine which would be the best way to convert that column.\n",
    "#For this pipeline, all categorical variables are going to be converted into one hot vectors.\n",
    "\n",
    "ce_one_hot = ce.OneHotEncoder(cols = ['MSSubClass','MSZoning','Street','Alley','LotShape','LandContour','LotConfig',\n",
    "                                     'Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle',\n",
    "                                     'RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation','Heating',\n",
    "                                     'CentralAir','Electrical','Functional','GarageType','PavedDrive','Fence',\n",
    "                                     'MiscFeature','SaleType','SaleCondition'])\n",
    "\n",
    "# train_X1 = ce_one_hot.fit_transform(train_X1,train_y1)\n",
    "# test_X1 = ce_one_hot.fit_transform(test_X1)\n",
    "combined_df1 = ce_one_hot.fit_transform(combined_df1)\n",
    "\n",
    "\n",
    "#First, we break up combined_df1 into train_X1 and test_X1\n",
    "train_X1 = combined_df1.loc[combined_df1.Id <= train_IDs[len(train_IDs)-1],:]\n",
    "train_X1 = train_X1.loc[:,train_X1.columns != 'Id']\n",
    "train_X1 = train_X1.reset_index(drop = True)\n",
    "\n",
    "test_X1 = combined_df1.loc[combined_df1.Id >= test_IDs[0],:]\n",
    "test_X1 = test_X1.loc[:,test_X1.columns != 'Id']\n",
    "test_X1 = test_X1.reset_index(drop = True)\n",
    "\n",
    "train_y1 = train_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################# FUNCTIONS PT.1 #################\n",
    "#Defining important functions for evaluating Boosting models.\n",
    "\n",
    "def modelfitCV(alg, train_X, train_y, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_X,train_y)\n",
    "    \n",
    "    #Predict on the training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Perform cross-validation\n",
    "    if performCV:\n",
    "        cv_score = cross_val_score(alg, train_X, train_y, cv = cv_folds, scoring='neg_mean_squared_log_error')\n",
    "        \n",
    "    #Print the model report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    if performCV:\n",
    "        #print('CV Score: %s'% cv_score)\n",
    "        print(\"CV Scores \\nMean : %.7g | Std : %.7g | Min : %.7g | Max : %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
    "    \n",
    "    #Print Feature Importance\n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.feature_importances_,train_X.columns).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar', title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        #print(feat_imp) #I may add this\n",
    "\n",
    "\n",
    "        \n",
    "def modelfitXGB(alg, train_X, train_y, useTrainCV=True, printFeatureImportance=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_params = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_X.values,label=train_y.values)\n",
    "        cvresult = xgb.cv(xgb_params,xgtrain,num_boost_round=alg.get_params()['n_estimators'],nfold=cv_folds,metrics='rmse',\n",
    "                          early_stopping_rounds=early_stopping_rounds,verbose_eval=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(\"n_estimators: %.4g\" % alg.get_params()['n_estimators'])\n",
    "        \n",
    "    #Fit Algorithm on the data\n",
    "    alg.fit(train_X,train_y,eval_metric='rmse')\n",
    "    \n",
    "    #Predict training set\n",
    "    train_predictions = alg.predict(train_X)\n",
    "    \n",
    "    #Print Model Report\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Mean Squared Log Error : %.4g\" % metrics.mean_squared_log_error(train_y, train_predictions))\n",
    "    print(\"Explained Variance Score : %.4g\" % metrics.explained_variance_score(train_y, train_predictions)) #1.0 is the best value\n",
    "    \n",
    "    if printFeatureImportance:\n",
    "        feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)[0:30]\n",
    "        feat_imp.plot(kind='bar',title = 'Feature Importances')\n",
    "        plt.ylabel('Feature Important Score')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "######### SAVEFITMODELS #########\n",
    "#Save our predictions to the proper directory.\n",
    "def SaveFitModels(pred, IDs, fileName, saveDirectory1 = '/Users/armenta/Kaggle/Housing Prices/Predictions/', \n",
    "                  saveDirectory2 ='/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/'):\n",
    "    \n",
    "    #Converting the predictions into a form that can be combined with their ID's\n",
    "    pred = pd.Series(pred)\n",
    "    pred = pd.concat([pred,IDs.rename('Id')],axis=1)\n",
    "    pred = pred.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "    pred = pred[['Id','SalePrice']]\n",
    "    #Create the path to save the outputs\n",
    "    path1 = saveDirectory1 + fileName\n",
    "    path2 = saveDirectory2 + fileName\n",
    "    #Save the outputs\n",
    "    pred.to_csv(path_or_buf = path1)\n",
    "    pred.to_csv(path_or_buf = path2)\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "def TDComp(model_results,column,number=10):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that takes the model_results from a parameter gridsearch or randomizedsearch, and\n",
    "    #grabs the top # of column values (specified by column and number) and transforms it into a viable \n",
    "    #format that can be used for the XGBTrainDevComparisons function. The output is supposed to be used for \n",
    "    #the xgb_parameter_values variable.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***model_results = the model results from running a parameter grid or randomized search on a ML model.\n",
    "    #***column = The column that you want to look at, or the parameter that was tuned. Usually, the string is\n",
    "    # 'param_' + paramter name \n",
    "    #***number = The number of values that you want to collect from the model_results. Cannot be bigger than \n",
    "    # model_results.shape[0], or else you would be wanting to grab more values than there are in the dataframe.\n",
    "\n",
    "    #Example code:\n",
    "    #top_n_est = pd.Series(model_results_dart1_2.loc[model_results_dart1_2.rank_test_score<=10,'param_n_estimators']).reset_index(drop=True)\n",
    "    \n",
    "    #Grab the specific values that you want. This series is designed to be used as xgb_parameter_values for the \n",
    "    #XGBTrainDevComparisons function below.\n",
    "    top_values = pd.Series(model_results.loc[model_results.rank_test_score<=number,column]).reset_index(drop=True)\n",
    "    return top_values\n",
    "\n",
    "\n",
    "\n",
    "def Standardizer(train_X = train_X1, test_X = test_X1, StandardScaler = preprocessing.StandardScaler(), Standardizer = True, Normalizer = False, SandN = False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This function will do a Standard transform_fit on train_X and test_X. This is done so we do not get test data leak \n",
    "    #when we do the transform_fit on train_X and can get a purer examination of our models and compare them.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only). (I may include a predict option later, I am not sure)\n",
    "    #***Standardizer = boolean that determines if we scale the features or not. Cannot be used \n",
    "    # if Normalizer is True.\n",
    "    #***scaler = the sklearn scaler that we will use to scale the data along the columns. \n",
    "    # The 3 options are MinMaxScaler, RobustScaler, and StandardScaler. \n",
    "    #***Normalizer = boolean value that determines if we normalize the values along the rows. \n",
    "    # scalers will scale across the features so that the distribution of values along the features \n",
    "    # changes, but this variable affects the actual rows (or vectors if you will) instead. Not \n",
    "    # recommended unless you understand the changes that will occur after normalization.\n",
    "    # Cannot be used with Standardizer = True\n",
    "    #***SandN = Boolean that determines if we Normalize (first) and Standardize (second) the data. \n",
    "    \n",
    "    #Save the column names so that we can convert the arrays to dataframes\n",
    "    columns = train_X.columns\n",
    "    \n",
    "    if Standardizer:\n",
    "        #Now we standardize our data.\n",
    "        #We initially fit the scaler to the train data (find the mean and std to be used on the other sets)\n",
    "        #then we take the fit scaler and transform the dev and test set.\n",
    "        standardized_train_X = StandardScaler.fit_transform(train_X) #Transform the train data\n",
    "        standardized_train_X = pd.DataFrame(standardized_train_X, columns=columns) #Convert to a dataframe\n",
    "        standardized_test_X = StandardScaler.transform(test_X) #Transform the test data\n",
    "        standardized_test_X = pd.DataFrame(standardized_test_X, columns=columns) #Convert to a dataframe\n",
    "        #Return the standardized datasets\n",
    "        return standardized_train_X, standardized_test_X\n",
    "    \n",
    "    elif Normalizer:\n",
    "        #We can normalize the data\n",
    "        normalizer = preprocessing.Normalizer() #Instantiate the normalizer\n",
    "        normalized_train_X = normalizer.fit_transform(train_X) #Transform the train data\n",
    "        normalized_train_X = pd.DataFrame(normalized_train_X,columns=columns) #Convert to a dataframe\n",
    "        normalized_test_X = normalizer.transform(test_X) #Transform the test data\n",
    "        normalized_test_X = pd.DataFrame(normalized_test_X,columns=columns) #Convert to a dataframe\n",
    "        #Return the normalized datasets\n",
    "        return normalized_train_X, normalized_test_X\n",
    "    \n",
    "    elif SandN:\n",
    "        normalizer = preprocessing.Normalizer() #Instantiate the normalizer\n",
    "        s_train_X = StandardScaler.fit_transform(train_X) #Standardize the train data\n",
    "        s_test_X = StandardScaler.transform(test_X) #Standardize the test data \n",
    "        sn_train_X = normalizer.fit_transform(s_train_X) #Normalize the train data \n",
    "        sn_train_X = pd.DataFrame(sn_train_X,columns=columns) #Convert to a dataframe\n",
    "        sn_test_X = normalizer.transform(s_test_X) #Normalize the test data\n",
    "        sn_test_X = pd.DataFrame(sn_test_X,columns=columns) #Convert to a dataframe\n",
    "        #Return the standardized / normalized datasets\n",
    "        return sn_train_X, sn_test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# STANDARDIZE THE DATA ##############\n",
    "#12/20/19\n",
    "#Standardized Data\n",
    "strain_X1, stest_X1 = Standardizer()\n",
    "\n",
    "#Normalized Data\n",
    "ntrain_X1, ntest_X1 = Standardizer(Standardizer=False, Normalizer=True)\n",
    "\n",
    "#Standardized and Normalized Data\n",
    "sntrain_X1, sntest_X1 = Standardizer(Standardizer=False, SandN=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "################# FUNCTIONS PT.2 #################\n",
    "################### XGBRModelTune Function ###################\n",
    "def XGBRModelTune(xgb_alg, xgb_param, xgb_param_vals, train_X=strain_X1, train_y=train_y1, test_X=stest_X1, \n",
    "                  cv_num=3, scoring='neg_mean_squared_log_error',Randomized = False, n_iter = 10, \n",
    "                  plot2d = True, modelfit = False):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that is used to tune parameters for the XGBoost parameters. There are a total of approximately\n",
    "    #11 parameters to change in XGBoost, but there will only be 9 that can be tuned in this function. \n",
    "    #The only 2 that are not being tuned: objective and booster. You can change these in the definition of the function,\n",
    "    #but they will not be tuned in this function because the number of values are so low, that I think its best \n",
    "    #to manually test it.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only). (I may include a predict option later, I am not sure)\n",
    "    #***xgb_alg = the XGBRegressor algorithm with starting parameters (can decide to leave some parameters blank)\n",
    "    #***xgb_param = This is the parameter name. This will be a string of the parameter we are tuning.\n",
    "    #***xgb_param_vals = This will be the range that we will search for when we grid search for the best variable values.\n",
    "    # The range should be as long as you can possibly make it so we can test a plethora of values.\n",
    "    # If Randomized = True, make sure that the array is larger than the value given for n_jobs, \n",
    "    # as this will return an error for RandomizedSearchCV. If you are unsure, then just leave Randomized = False. \n",
    "    # The different variables are as follows:\n",
    "          #***learning_rate = the learning rate of the XGBRegressor algorithm.\n",
    "          #***n_estimators = the number of trees to use in this ensemble model. \n",
    "          #***max_depth = maximum depth allowed for an individual tree.\n",
    "          #***min_child_weight = minimum number of weights allowed for a child node; basically a variable that describes the amount of \n",
    "          # observations that are allowed in each child node. The higher the value, the more values that are required in each node.\n",
    "          #***gamma = A value that defines the minimum positive reduction in the loss function that must occur for a node to split.\n",
    "          #***subsample = A value that denotes the % of samples to be used in each node of the tree.\n",
    "          #***colsample_bytree = A value that determines the % of columns to be used for each tree.\n",
    "          #***objective = The loss function to be minimized.\n",
    "          #***booster = The type of model that we run at each iteration. Can choose gbtree (tree-based models), gblinear (linear models),\n",
    "          # or dart which is similar to gbtree but it implements deep neural networks drop-out technique.\n",
    "          #***reg_lambda = L2 regularization term on weights. Used to handle the main regularization part of XGBoost.\n",
    "          #***reg_alpha = L1 regularization term on weights.  \n",
    "    #***cv_num = The number of cross-validation folds that will be used in the parameter search process.\n",
    "    #***Randomized = A boolean value that decides if the first search you do for parameter searches is randomized or not.\n",
    "    #***n_iter = A number that is only used if Randomized is true. It essentially determines the number of minimum iterations \n",
    "    # RandomizedSearchCV will do before it stops testing random values of the variable in the distribution.\n",
    "    # I recommend len(xgb_param_vals) - 10.\n",
    "    #***plot2d = A boolean that will decide whether we show a 2d plot of error vs variable values. This will essentially help\n",
    "    # us determine a more effective and smaller range to look at after we do the search.\n",
    "    #***modelfit = A boolean that will determine if we run the modelfitXGB function to observe important features \n",
    "    # in the XGBR model\n",
    "    \n",
    "    #This prevents us from getting warnings that are unnecessary and don't add to anything.\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    #For RandomizedCVSearch\n",
    "    if Randomized:\n",
    "        #Create the dictionary object that is used in RandomizedSearchCV\n",
    "        param_distributions = {xgb_param:xgb_param_vals}\n",
    "        #Create the RandomizedSearchCV object\n",
    "        random_search_model = RandomizedSearchCV(estimator = xgb_alg,param_distributions = param_distributions,\n",
    "                                           n_iter = n_iter,scoring = scoring,n_jobs=-1,iid=False,cv=cv_num)\n",
    "        #Fit the data to our random search object\n",
    "        random_search_model.fit(train_X,train_y)\n",
    "        #These variables will be returned along with the model.\n",
    "        rs_results = pd.DataFrame(random_search_model.cv_results_) #The results of the random search\n",
    "        best_param_val = random_search_model.best_params_ #The best parameter\n",
    "        best_score_val = random_search_model.best_score_ #The best score associated with the best parameter\n",
    "        \n",
    "        #Store the returned values in a single list \n",
    "        return_values = [random_search_model,rs_results,best_param_val,best_score_val]\n",
    "        print(best_param_val, best_score_val)\n",
    "        #Create a 2d plot of mean_test_score (y) vs parameter values (x)\n",
    "        if plot2d:\n",
    "            rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "            param_name = 'param_'+ xgb_param\n",
    "            fig = px.scatter(rs_results,x=param_name,y='mean_test_score',color='mean_test_score')\n",
    "            fig.show()\n",
    "        #Create a bar plot showing the weights of the most important features so far. \n",
    "        if modelfit:\n",
    "            p_dict = {xgb_param:best_param_val[xgb_param]}\n",
    "            xgb_alg.set_params(**p_dict)\n",
    "            modelfitXGB(xgb_alg,train_X,train_y,cv_folds=cv_num)  \n",
    "        \n",
    "        return return_values \n",
    "    \n",
    "    \n",
    "    \n",
    "    #For a GridSearchCV\n",
    "    else:\n",
    "        #Create the dictionary object that is used in GridSearchCV\n",
    "        param_grid = {xgb_param:xgb_param_vals}\n",
    "        #Create the GridSearch object that will be fitted on the training_data.\n",
    "        grid_search_model = GridSearchCV(estimator = xgb_alg,param_grid = param_grid,scoring = scoring,\n",
    "                                        n_jobs = -1,iid = False, cv = cv_num)\n",
    "        #Fit the training data to the grid search object\n",
    "        grid_search_model.fit(train_X,train_y)\n",
    "        \n",
    "        #Save these following three variables to be returned later \n",
    "        gs_results = pd.DataFrame(grid_search_model.cv_results_) #The results of the grid search\n",
    "        best_param_val = grid_search_model.best_params_ #The best parameter value\n",
    "        best_score_val = grid_search_model.best_score_ #The best score associated with the best parameter value\n",
    "        \n",
    "        #Save the return values in a single list\n",
    "        return_values = [grid_search_model,gs_results,best_param_val,best_score_val]\n",
    "        print(best_param_val, best_score_val)\n",
    "        \n",
    "        #Create a 2d plot of mean_test_score (y) vs parameter values (x)\n",
    "        if plot2d:\n",
    "            rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "            param_name = 'param_'+ xgb_param\n",
    "            fig = px.scatter(gs_results,x=param_name,y='mean_test_score',color='mean_test_score')\n",
    "            fig.show()\n",
    "        #Create a bar plot showing the weights of the most important features so far. \n",
    "        if modelfit:\n",
    "            p_dict = {xgb_param:best_param_val[xgb_param]}\n",
    "            xgb_alg.set_params(**p_dict)\n",
    "            modelfitXGB(xgb_alg,train_X,train_y,cv_folds=cv_num)\n",
    "        return return_values \n",
    "    \n",
    "    \n",
    "    \n",
    "def TrainTestErrors(model,savefileName=None,save=True,train_X=strain_X1,train_y=train_y1,test_X=stest_X1,\n",
    "                    t_IDs=test_IDs,metric=metrics.mean_squared_log_error):\n",
    "    #################################################### FUNCTION DESCRIPTION ##############################################\n",
    "    #################################################### INTRODUCTION ################################################\n",
    "    #This is a function that will compute the train set errors and explained variances of a specific model.\n",
    "    #This will also compute the test predictions, and save them if save=True.\n",
    "    #################################################### VARIABLE DEFINITIONS ##############################################\n",
    "    #***model = The Machine Learning model.\n",
    "    #***savefileName = The string of the filename.\n",
    "    #***save = Boolean that determines whether we save the test predictions.\n",
    "    #***train_X = the training data (features only).  \n",
    "    #***train_y = the training data (target only).\n",
    "    #***test_X = the testing data (features only).\n",
    "    #***t_IDs = IDs for the testing data.\n",
    "    #***metric = the metric for which we are examining the error.\n",
    "    \n",
    "    #Fit the model to the training data \n",
    "    model_fit = model.fit(train_X,train_y)\n",
    "    \n",
    "    #Create predictions on the training set. Compute the error and explained variance.\n",
    "    train_pred = model_fit.predict(train_X)\n",
    "    train_error = metric(train_y,train_pred)\n",
    "    train_explained_var = metrics.explained_variance_score(train_y,train_pred)\n",
    "    \n",
    "    #If save = True, create predictions on the test set, and save the predictions using SaveFitModels() \n",
    "    if save:\n",
    "        test_pred = model_fit.predict(test_X)\n",
    "        SaveFitModels(test_pred,t_IDs,savefileName)\n",
    "    \n",
    "    return train_error, train_explained_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ Linear Regression #############\n",
    "# regressor = LinearRegression(normalize = True)\n",
    "# linmodel = regressor.fit(train_X1,train_y1)\n",
    "# linmodel_p = linmodel.predict(test_X1) #These are the predictions for simple linear regression'\n",
    "# linmodel_p = pd.Series(linmodel_p) #Need to convert it to a series before we can concatenate it.\n",
    "# linmodel_p = pd.concat([linmodel_p,test_IDs.rename('Id')],axis=1) #Add the IDs\n",
    "# linmodel_p = linmodel_p.rename(columns = {0:'SalePrice','Id':'Id'}) #Rename the Columns\n",
    "# linmodel_p = linmodel_p[['Id','SalePrice']] #Switch the order of the columns \n",
    "#Comment out after you run once.\n",
    "#linmodel_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/linmodel_p_09042019.csv')\n",
    "#linmodel_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/linmodel_p_09042019.csv')\n",
    "#BEFORE SUBMITTING THE FILES, MAKE SURE TO DELETE THE INDEX COLUMN IN EXCEL!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ Ridge Linear Regression ############\n",
    "#Unlike the regular linear regression, there are MANY MANY POSSIBILITIES TO CHOOSE FROM.\n",
    "\n",
    "#alpha = 0.001\n",
    "# ridge_reg1 = Ridge(alpha = 0.001,normalize = True)\n",
    "# rr_model1 = ridge_reg1.fit(train_X1,train_y1)\n",
    "# rr_model1p = rr_model1.predict(test_X1)\n",
    "# rr_model1p = pd.Series(rr_model1p)\n",
    "# rr_model1p = pd.concat([rr_model1p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model1p = rr_model1p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model1p = rr_model1p[['Id','SalePrice']]\n",
    "\n",
    "#rr_model1p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model1p_09042019.csv')\n",
    "#rr_model1p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model1p_09042019.csv')\n",
    "\n",
    "#alpha = 0.01\n",
    "# ridge_reg2 = Ridge(alpha = 0.01,normalize = True)\n",
    "# rr_model2 = ridge_reg2.fit(train_X1,train_y1)\n",
    "# rr_model2p = rr_model2.predict(test_X1)\n",
    "# rr_model2p = pd.Series(rr_model2p)\n",
    "# rr_model2p = pd.concat([rr_model2p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model2p = rr_model2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model2p = rr_model2p[['Id','SalePrice']]\n",
    "\n",
    "#rr_model2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model2p_09052019.csv')\n",
    "#rr_model2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model2p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.1\n",
    "# ridge_reg3 = Ridge(alpha = 0.1,normalize = True)\n",
    "# rr_model3 = ridge_reg3.fit(train_X1,train_y1)\n",
    "# rr_model3p = rr_model3.predict(test_X1)\n",
    "# rr_model3p = pd.Series(rr_model3p)\n",
    "# rr_model3p = pd.concat([rr_model3p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model3p = rr_model3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model3p = rr_model3p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model3p_09052019.csv')\n",
    "# rr_model3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model3p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.25\n",
    "# ridge_reg4 = Ridge(alpha = 0.25,normalize = True)\n",
    "# rr_model4 = ridge_reg4.fit(train_X1,train_y1)\n",
    "# rr_model4p = rr_model4.predict(test_X1)\n",
    "# rr_model4p = pd.Series(rr_model4p)\n",
    "# rr_model4p = pd.concat([rr_model4p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model4p = rr_model4p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model4p = rr_model4p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model4p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model4p_09052019.csv')\n",
    "# rr_model4p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model4p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.5\n",
    "# ridge_reg5 = Ridge(alpha = 0.5,normalize = True)\n",
    "# rr_model5 = ridge_reg5.fit(train_X1,train_y1)\n",
    "# rr_model5p = rr_model5.predict(test_X1)\n",
    "# rr_model5p = pd.Series(rr_model5p)\n",
    "# rr_model5p = pd.concat([rr_model5p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model5p = rr_model5p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model5p = rr_model5p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model5p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model5p_09052019.csv')\n",
    "# rr_model5p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model5p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 0.75\n",
    "# ridge_reg6 = Ridge(alpha = 0.75,normalize = True)\n",
    "# rr_model6 = ridge_reg6.fit(train_X1,train_y1)\n",
    "# rr_model6p = rr_model6.predict(test_X1)\n",
    "# rr_model6p = pd.Series(rr_model6p)\n",
    "# rr_model6p = pd.concat([rr_model6p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model6p = rr_model6p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model6p = rr_model6p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model6p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model6p_09052019.csv')\n",
    "# rr_model6p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model6p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 1\n",
    "# ridge_reg7 = Ridge(alpha = 1,normalize = True)\n",
    "# rr_model7 = ridge_reg7.fit(train_X1,train_y1)\n",
    "# rr_model7p = rr_model7.predict(test_X1)\n",
    "# rr_model7p = pd.Series(rr_model7p)\n",
    "# rr_model7p = pd.concat([rr_model7p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model7p = rr_model7p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model7p = rr_model7p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model7p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model7p_09052019.csv')\n",
    "# rr_model7p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model7p_09052019.csv')\n",
    "\n",
    "#alpha = 10\n",
    "# ridge_reg8 = Ridge(alpha = 10,normalize = True)\n",
    "# rr_model8 = ridge_reg8.fit(train_X1,train_y1)\n",
    "# rr_model8p = rr_model8.predict(test_X1)\n",
    "# rr_model8p = pd.Series(rr_model8p)\n",
    "# rr_model8p = pd.concat([rr_model8p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model8p = rr_model8p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model8p = rr_model8p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model8p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model8p_09052019.csv')\n",
    "# rr_model8p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model8p_09052019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#alpha = 2\n",
    "# ridge_reg9 = Ridge(alpha = 2,normalize = True)\n",
    "# rr_model9 = ridge_reg9.fit(train_X1,train_y1)\n",
    "# rr_model9p = rr_model9.predict(test_X1)\n",
    "# rr_model9p = pd.Series(rr_model9p)\n",
    "# rr_model9p = pd.concat([rr_model9p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model9p = rr_model9p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model9p = rr_model9p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model9p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model9p_09052019.csv')\n",
    "# rr_model9p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model9p_09052019.csv')\n",
    "\n",
    "\n",
    "#alpha = 5\n",
    "# ridge_reg10 = Ridge(alpha = 5,normalize = True)\n",
    "# rr_model10 = ridge_reg10.fit(train_X1,train_y1)\n",
    "# rr_model10p = rr_model10.predict(test_X1)\n",
    "# rr_model10p = pd.Series(rr_model10p)\n",
    "# rr_model10p = pd.concat([rr_model10p,test_IDs.rename('Id')],axis=1)\n",
    "# rr_model10p = rr_model10p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rr_model10p = rr_model10p[['Id','SalePrice']]\n",
    "\n",
    "# rr_model10p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rr_model10p_09052019.csv')\n",
    "# rr_model10p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rr_model10p_09052019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ Regression Trees #############\n",
    "\n",
    "#MSE\n",
    "# dt_reg = DecisionTreeRegressor()\n",
    "# dt_model = dt_reg.fit(train_X1,train_y1)\n",
    "# dt_modelp = dt_model.predict(test_X1)\n",
    "# dt_modelp = pd.Series(dt_modelp)\n",
    "# dt_modelp = pd.concat([dt_modelp,test_IDs.rename('Id')],axis=1)\n",
    "# dt_modelp = dt_modelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# dt_modelp = dt_modelp[['Id','SalePrice']]\n",
    "\n",
    "#dt_modelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_modelp_09052019.csv')\n",
    "#dt_modelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_modelp_09052019.csv')\n",
    "\n",
    "\n",
    "#Friedman MSE\n",
    "# dt_reg2 = DecisionTreeRegressor(criterion='friedman_mse')\n",
    "# dt_model2 = dt_reg2.fit(train_X1,train_y1)\n",
    "# dt_model2p = dt_model2.predict(test_X1)\n",
    "# dt_model2p = pd.Series(dt_model2p)\n",
    "# dt_model2p = pd.concat([dt_model2p,test_IDs.rename('Id')],axis=1)\n",
    "# dt_model2p = dt_model2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# dt_model2p = dt_model2p[['Id','SalePrice']]\n",
    "\n",
    "#dt_model2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_model2p_09052019.csv')\n",
    "#dt_model2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_model2p_09052019.csv')\n",
    "\n",
    "\n",
    "#MAE\n",
    "dt_reg3 = DecisionTreeRegressor(criterion='mae')\n",
    "dt_model3 = dt_reg3.fit(train_X1,train_y1)\n",
    "dt_model3p = dt_model3.predict(test_X1)\n",
    "dt_model3p = pd.Series(dt_model3p)\n",
    "dt_model3p = pd.concat([dt_model3p,test_IDs.rename('Id')],axis=1)\n",
    "dt_model3p = dt_model3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "dt_model3p = dt_model3p[['Id','SalePrice']]\n",
    "\n",
    "#dt_model3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/dt_model3p_09052019.csv')\n",
    "#dt_model3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/dt_model3p_09052019.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ Regression Forests ###########\n",
    "#n_estimators = 100, criterion = mse, bootstrap = True\n",
    "# forest100 = RandomForestRegressor(n_estimators = 100)\n",
    "# fmodel100 = forest100.fit(train_X1,train_y1)\n",
    "# fmodel100_p = fmodel100.predict(test_X1)\n",
    "# fmodel100_p = pd.Series(fmodel100_p)\n",
    "# fmodel100_p = pd.concat([fmodel100_p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_p = fmodel100_p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_p = fmodel100_p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_p_09062019.csv')\n",
    "# fmodel100_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 1000, criterion = mse, bootstrap = True\n",
    "# forest1000 = RandomForestRegressor(n_estimators = 1000)\n",
    "# fmodel1000 = forest1000.fit(train_X1,train_y1)\n",
    "# fmodel1000_p = fmodel1000.predict(test_X1)\n",
    "# fmodel1000_p = pd.Series(fmodel1000_p)\n",
    "# fmodel1000_p = pd.concat([fmodel1000_p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel1000_p = fmodel1000_p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel1000_p = fmodel1000_p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel1000_p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel1000_p_09062019.csv')\n",
    "# fmodel1000_p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel1000_p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 100, criterion = mae, bootstrap = True\n",
    "# forest1002 = RandomForestRegressor(n_estimators = 100,criterion = 'mae')\n",
    "# fmodel1002 = forest1002.fit(train_X1,train_y1)\n",
    "# fmodel100_2p = fmodel1002.predict(test_X1)\n",
    "# fmodel100_2p = pd.Series(fmodel100_2p)\n",
    "# fmodel100_2p = pd.concat([fmodel100_2p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_2p = fmodel100_2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_2p = fmodel100_2p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_2p_09062019.csv')\n",
    "# fmodel100_2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_2p_09062019.csv')\n",
    "\n",
    "\n",
    "#n_estimators = 1000, criterion = mae, bootstrap = True\n",
    "# forest10002 = RandomForestRegressor(n_estimators = 1000,criterion = 'mae')\n",
    "# fmodel10002 = forest10002.fit(train_X1,train_y1)\n",
    "# fmodel1000_2p = fmodel10002.predict(test_X1)\n",
    "# fmodel1000_2p = pd.Series(fmodel1000_2p)\n",
    "# fmodel1000_2p = pd.concat([fmodel1000_2p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel1000_2p = fmodel1000_2p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel1000_2p = fmodel1000_2p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel1000_2p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel1000_2p_09062019.csv')\n",
    "# fmodel1000_2p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel1000_2p_09062019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#n_estimators = 100, criterion = mse, bootstrap = False\n",
    "# forest1003 = RandomForestRegressor(n_estimators = 100,criterion = 'mse', bootstrap = False)\n",
    "# fmodel1003 = forest1003.fit(train_X1,train_y1)\n",
    "# fmodel100_3p = fmodel1003.predict(test_X1)\n",
    "# fmodel100_3p = pd.Series(fmodel100_3p)\n",
    "# fmodel100_3p = pd.concat([fmodel100_3p,test_IDs.rename('Id')],axis=1)\n",
    "# fmodel100_3p = fmodel100_3p.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# fmodel100_3p = fmodel100_3p[['Id','SalePrice']]\n",
    "\n",
    "# fmodel100_3p.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/fmodel100_3p_09062019.csv')\n",
    "# fmodel100_3p.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/fmodel100_3p_09062019.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ 9/10/19 RandomForestRegressor using Cross Validation and Grid Search ##########\n",
    "#using modelfitCV in order to tune some parameters for our random forests.\n",
    "#Using default values for the first run\n",
    "# rf_0 = RandomForestRegressor(random_state = 2, n_estimators=10)\n",
    "# modelfitCV(rf_0, train_X1, train_y1)\n",
    "\n",
    "#########Tuning of the n_estimators parameters 10 -> 100 in steps of 10\n",
    "# param_test1 = {'n_estimators':range(10,101,10)}\n",
    "# gsearch = grid search\n",
    "# gsearch1 = GridSearchCV(estimator = RandomForestRegressor(min_samples_split=14,min_samples_leaf=2, \n",
    "#                                                           max_depth=5,max_features='sqrt',random_state=2),\n",
    "#                        param_grid = param_test1, scoring='neg_mean_squared_log_error',n_jobs=4,iid=False,cv=5)\n",
    "# gsearch1.fit(train_X1,train_y1)\n",
    "#[gsearch1.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch1.best_params_,gsearch1.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of the n_estimators parameters 100 -> 200 in steps of 10\n",
    "# param_test2 = {'n_estimators':range(100,201,10)}\n",
    "# gsearch2 = GridSearchCV(estimator = RandomForestRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,max_features='sqrt',random_state=2),\n",
    "#                        param_grid = param_test2, scoring = 'neg_mean_squared_log_error',n_jobs=4,iid=False,cv=5)\n",
    "# gsearch2.fit(train_X1,train_y1)\n",
    "# [gsearch2.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch2.best_params_,gsearch2.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#########Tuning of max_depth (4->10 in steps of 2) and min_samples_split (10->50 in steps of 5)\n",
    "# param_test3 = {'max_depth':range(4,11,2), 'min_samples_split':range(10,51,5)}\n",
    "# gsearch3 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt',random_state=2),\n",
    "#                        param_grid = param_test3, scoring = 'neg_mean_squared_log_error', n_jobs=4,iid=False,cv=5)\n",
    "# gsearch3.fit(train_X1,train_y1)\n",
    "# [gsearch3.cv_results_[x] for x in ('params','mean_test_score','std_test_score')],gsearch3.best_params_,gsearch3.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of max_depth (10->20 in steps of 2) and min_samples_split (2->12 in steps of 2)\n",
    "# param_test4 = {'max_depth':range(10,21,2), 'min_samples_split':range(2,13,2)}\n",
    "# gsearch4 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt',random_state=2),\n",
    "#                        param_grid = param_test4, scoring = 'neg_mean_squared_log_error', n_jobs=4,iid=False,cv=5)\n",
    "# gsearch4.fit(train_X1,train_y1)\n",
    "#results4 = pd.DataFrame(gsearch4.cv_results_)\n",
    "#results4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch4.best_params_, gsearch4.best_score_\n",
    "\n",
    "\n",
    "#########Tuning of min_samples_leaf (1->10 in steps of 1)\n",
    "# param_test5 = {'min_samples_leaf':range(1,11,1)}\n",
    "# gsearch5 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160, max_features = 'sqrt', max_depth=18, min_samples_split=2, random_state=2),\n",
    "#                        param_grid = param_test5, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch5.fit(train_X1,train_y1)\n",
    "# results5 = pd.DataFrame(gsearch5.cv_results_)\n",
    "#results5.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch5.best_params_, gsearch5.best_score_\n",
    "\n",
    "#Now we will check on how our mean score has improved since when we first ran modelfitCV\n",
    "#rcParams['figure.figsize'] = 12,4 #just in case the graph size isn't the same as before\n",
    "#modelfitCV(gsearch5.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "\n",
    "#########Tuning of max_features (12->40 in steps of 4)\n",
    "# param_test6 = {'max_features':range(12,41,4)}\n",
    "# gsearch6 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,min_samples_split=2,min_samples_leaf=1,random_state=2),\n",
    "#                        param_grid = param_test6, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch6.fit(train_X1,train_y1)\n",
    "# results6 = pd.DataFrame(gsearch6.cv_results_)\n",
    "#results6.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch6.best_params_, gsearch6.best_score_\n",
    "\n",
    "\n",
    "#Use our tuned parameters to create a model. Lets check modelfitCV and then fit our data to the test data.\n",
    "#rfr_tuned = RandomForestRegressor(n_estimators = 160, max_depth=18, min_samples_split=2, min_samples_leaf=1,max_features=36,random_state=2)\n",
    "#modelfitCV(rfr_tuned,train_X1,train_y1)\n",
    "\n",
    "\n",
    "###############################\n",
    "#TRAINING THE 1st TUNED MODEL\n",
    "##############################\n",
    "# rfr_tuned_model = rfr_tuned.fit(train_X1,train_y1)\n",
    "# rfr_tuned_modelp = rfr_tuned_model.predict(test_X1)\n",
    "# rfr_tuned_modelp = pd.Series(rfr_tuned_modelp)\n",
    "# rfr_tuned_modelp = pd.concat([rfr_tuned_modelp,test_IDs.rename('Id')],axis=1)\n",
    "# rfr_tuned_modelp = rfr_tuned_modelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# rfr_tuned_modelp = rfr_tuned_modelp[['Id','SalePrice']]\n",
    "\n",
    "# rfr_tuned_modelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/rfr_tuned_modelp_09102019.csv')\n",
    "# rfr_tuned_modelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/rfr_tuned_modelp_09102019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#########Retuning of min_samples_split (2 -> 30 in steps of 2)\n",
    "# param_test7 = {'min_samples_split':range(2,31,2)}\n",
    "# gsearch7 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,min_samples_leaf=1,max_features=36,random_state=2),\n",
    "#                        param_grid = param_test7, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch7.fit(train_X1,train_y1)\n",
    "# results7 = pd.DataFrame(gsearch7.cv_results_)\n",
    "# results7.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#gsearch7.best_params_, gsearch7.best_score_ \n",
    "\n",
    "\n",
    "#########Retuning of min_samples_leaf (1 -> 15 in steps of 1)\n",
    "# param_test8 = {'min_samples_leaf':range(1,16,1)}\n",
    "# gsearch8 = GridSearchCV(estimator = RandomForestRegressor(n_estimators=160,max_depth=18,max_features=36,min_samples_split=2,random_state=2),\n",
    "#                        param_grid = param_test8, scoring = 'neg_mean_squared_log_error', n_jobs=4, iid=False, cv=5)\n",
    "# gsearch8.fit(train_X1,train_y1)\n",
    "# results8 = pd.DataFrame(gsearch8.cv_results_)\n",
    "# results8.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# gsearch8.best_params_, gsearch8.best_score_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ AdaBoost using CrossValidation and Grid Search ###########\n",
    "#uncomment when you just open up the notebook\n",
    "#rfr_tuned = RandomForestRegressor(n_estimators = 160, max_depth=18, min_samples_split=2, min_samples_leaf=1,max_features=36,random_state=2)\n",
    "\n",
    "\n",
    "#Create the AdaBoosting model\n",
    "#AB1 = AdaBoostRegressor(rfr_tuned,random_state=2)\n",
    "\n",
    "\n",
    "############################################################################\n",
    "########YOU DONT NEED TO RUN THIS SECTION AGAIN! USE THE NEW VALUES TO TUNE  \n",
    "###########################################################################\n",
    "############Tuning of the learning_rate (randomly generated) and n_estimators (randomly generated)\n",
    "#Creating the random values for the learning rate\n",
    "# r_exp = -4*np.random.rand(20)\n",
    "# alpha = 10**r_exp\n",
    "# param_testAB1 = {'learning_rate':alpha,'n_estimators': np.random.randint(10,100,20)} #the parameters that we are tuning\n",
    "# #rgsearch = random grid search\n",
    "# rgsearchAB1 = RandomizedSearchCV(estimator = AB1, param_distributions = param_testAB1,n_iter=20,scoring = 'neg_mean_squared_log_error',\n",
    "#                                   cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB1.fit(train_X1,train_y1) #This took approx 10 minutes to run #Ran 9/13/19 1:14AM \n",
    "\n",
    "# resultsAB1 = pd.DataFrame(rgsearchAB1.cv_results_) #convert the results to a dataframe to be easily read\n",
    "# resultsAB1.loc[:,('params','mean_test_score','std_test_score')] \n",
    "# rgsearchAB1.best_params_, rgsearchAB1.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitCV(rgsearchAB1.best_estimator_,train_X1,train_y1) #Create a graph to store in our power point\n",
    "\n",
    "# adaboost_tuned1 = rgsearchAB1.best_estimator_\n",
    "# adaboost_p1 = adaboost_tuned1.predict(test_X1)\n",
    "# adaboost_p1 = pd.Series(adaboost_p1)\n",
    "# adaboost_p1 = pd.concat([adaboost_p1,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p1 = adaboost_p1.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p1 = adaboost_p1[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p1.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p1_09122019.csv')\n",
    "# adaboost_p1.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p1_09122019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########YOU ARE HERE AS OF 9/13/19 1:35AM\n",
    "########Further tuning of the learning_rate (0.60 -> 0.75 in steps of 0.01), and the n_estimators (41 -> 59 in steps of 2)\n",
    "# param_testAB2 = {'learning_rate':np.arange(0.6,0.75,0.01),'n_estimators':range(41,60,2)}\n",
    "# rgsearchAB2 = GridSearchCV(estimator = AB1, param_grid = param_testAB2, scoring='neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB2.fit(train_X1,train_y1) #Took about 3 hours.\n",
    "\n",
    "# resultsAB2 = pd.DataFrame(rgsearchAB2.cv_results_)\n",
    "# resultsAB2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rgsearchAB2.best_params_, rgsearchAB2.best_score_\n",
    "\n",
    "#modelfitCV(rgsearchAB2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# adaboost_tuned2 = rgsearchAB2.best_estimator_\n",
    "# adaboost_p2 = adaboost_tuned2.predict(test_X1)\n",
    "# adaboost_p2 = pd.Series(adaboost_p2)\n",
    "# adaboost_p2 = pd.concat([adaboost_p2,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p2 = adaboost_p2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p2 = adaboost_p2[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p2_09132019.csv')\n",
    "# adaboost_p2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p2_09132019.csv')\n",
    "\n",
    "\n",
    "\n",
    "##########Further tuning of the learning_rate (0.75 -> 0.85 steps of 0.02), and the n_estimators (59 -> 71 in steps of 2)\n",
    "# param_testAB3 = {'learning_rate':np.arange(0.75,0.85,0.02),'n_estimators':range(59,72,2)}\n",
    "# rgsearchAB3 = GridSearchCV(estimator = AB1, param_grid = param_testAB3, scoring ='neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB3.fit(train_X1,train_y1)\n",
    "\n",
    "#resultsAB3 = pd.DataFrame(rgsearchAB3.cv_results_)\n",
    "# resultsAB3.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# rgsearchAB3.best_params_, rgsearchAB3.best_score_\n",
    "\n",
    "#modelfitCV(rgsearchAB3.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# adaboost_tuned3 = rgsearchAB3.best_estimator_\n",
    "# adaboost_p3 = adaboost_tuned3.predict(test_X1)\n",
    "# adaboost_p3 = pd.Series(adaboost_p3)\n",
    "# adaboost_p3 = pd.concat([adaboost_p3,test_IDs.rename('Id')],axis=1)\n",
    "# adaboost_p3 = adaboost_p3.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# adaboost_p3 = adaboost_p3[['Id','SalePrice']]\n",
    "\n",
    "# adaboost_p3.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/adaboost_p3_09132019.csv')\n",
    "# adaboost_p3.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/adaboost_p3_09132019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Tuning the loss function to see if any improvements will manifest.\n",
    "# AB2 = AdaBoostRegressor(rfr_tuned, learning_rate = 0.77,n_estimators = 67,random_state=2)\n",
    "# param_testAB4 = {'loss': ['linear','square','exponential']}\n",
    "# rgsearchAB4 = GridSearchCV(estimator=AB2, param_grid = param_testAB4, scoring = 'neg_mean_squared_log_error',\n",
    "#                           cv=5, iid=False, n_jobs=-1)\n",
    "# rgsearchAB4.fit(train_X1,train_y1)\n",
    "\n",
    "# resultsAB4 = pd.DataFrame(rgsearchAB4.cv_results_)\n",
    "# resultsAB4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#rgsearchAB4.best_params_, rgsearchAB4.best_score_ #linear is the best way to go!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DO NOT DELETE OR MOVE, KEEP THIS CODE HERE.\n",
    "# From a random website on Adaboost (but you had to pay for the rest)\n",
    "# from sklearn.ensemble import AdaBoostRegressor\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#The person that coded this created \n",
    "# param_dist = {\n",
    "#  'n_estimators': [50, 100],\n",
    "#  'learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
    "#  'loss' : ['linear', 'square', 'exponential']\n",
    "#  }\n",
    "\n",
    "# pre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(),\n",
    "#  param_distributions = param_dist,\n",
    "#  cv=3,\n",
    "#  n_iter = 10,\n",
    "#  n_jobs=-1)\n",
    "\n",
    "# pre_gs_inst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ Gradient Boosting with RandomizedCrossValidation ############\n",
    "#Lets see how generic gradient boosting works with no tuning\n",
    "# gb_0 = GradientBoostingRegressor(random_state=5)\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(gb_0, train_X1, train_y1)\n",
    "\n",
    "#Lets see how the base model performs on Kaggle.\n",
    "# GBmodel = gb_0.fit(train_X1,train_y1)\n",
    "# GBmodelp = GBmodel.predict(test_X1)\n",
    "# GBmodelp = pd.Series(GBmodelp)\n",
    "# GBmodelp = pd.concat([GBmodelp,test_IDs.rename('Id')],axis=1)\n",
    "# GBmodelp = GBmodelp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBmodelp = GBmodelp[['Id','SalePrice']]\n",
    "\n",
    "# GBmodelp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBmodelp_09162019.csv')\n",
    "# GBmodelp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBmodelp_09162019.csv')\n",
    "\n",
    "#This regular model outperformed the tuned Adaboost + Regression Trees (Wow)\n",
    "\n",
    "\n",
    "\n",
    "#Random Tuning of learning_rate with n_estimators. \n",
    "#This is not recommended by the article because they affect each other but this is a little experiment that I \n",
    "#want to run to test how good this process will be.\n",
    "\n",
    "###########Tune learning_rate (randomly generated 20 values) and n_estimators (40 -> 140 in steps of 10)\n",
    "# r_expGB = -4*np.random.rand(20)\n",
    "# LR_GB = 10**r_expGB\n",
    "# NE = range(40,141,10)\n",
    "# GB_param_test1 = {'n_estimators':NE,'learning_rate':LR_GB}\n",
    "# GBrgsearch1 = RandomizedSearchCV(estimator = GradientBoostingRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,\n",
    "#                                                                       max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                                  param_distributions=GB_param_test1,n_iter=80, scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "#GBrgsearch1.fit(train_X1,train_y1)\n",
    "\n",
    "#GBresults1 = pd.DataFrame(GBrgsearch1.cv_results_)\n",
    "#GBresults1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#GBrgsearch1.best_params_,GBrgsearch1.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(GBrgsearch1.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB1 = GBrgsearch1.best_estimator_\n",
    "# GB_model1 = GB1.fit(train_X1,train_y1)\n",
    "# GBp1 = GB_model1.predict(test_X1)\n",
    "# GBp1 = pd.Series(GBp1)\n",
    "# GBp1 = pd.concat([GBp1,test_IDs.rename('Id')],axis=1)\n",
    "# GBp1 = GBp1.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp1 = GBp1[['Id','SalePrice']]\n",
    "\n",
    "# GBp1.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp1_09162019.csv')\n",
    "# GBp1.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp1_09162019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########Tune learning_rate (130 -> 200 steps of 10) and n_estimators (0.040 -> 0.060 in steps of 0.002)\n",
    "# GB_param_test2 = {'n_estimators':range(130,201,10),'learning_rate':np.arange(0.040,0.061,0.002)}\n",
    "# GBgsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(min_samples_split=14,min_samples_leaf=2,max_depth=5,\n",
    "#                                                                 max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                           param_grid=GB_param_test2,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch2.fit(train_X1,train_y1)\n",
    "\n",
    "# GBresults2 = pd.DataFrame(GBgsearch2.cv_results_)\n",
    "# GBresults2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# GBgsearch2.best_params_,GBgsearch2.best_score_\n",
    "# rcParams['figure.figsize'] = 12, 4\n",
    "# modelfitCV(GBgsearch2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB2 = GBgsearch2.best_estimator_\n",
    "# GB_model2 = GB2.fit(train_X1,train_y1)\n",
    "# GBp2 = GB_model2.predict(test_X1)\n",
    "# GBp2 = pd.Series(GBp2)\n",
    "# GBp2 = pd.concat([GBp2,test_IDs.rename('Id')],axis=1)\n",
    "# GBp2 = GBp2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp2 = GBp2[['Id','SalePrice']]\n",
    "\n",
    "# GBp2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp2_09172019.csv')\n",
    "# GBp2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp2_09172019.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########Tune n_estimators (180 -> 250 in steps of 10) and max_depth (4 -> 10 steps of 1)\n",
    "# GB_param_test3 = {'n_estimators':range(180,251,10),'max_depth':range(4,11,1)}\n",
    "# GBgsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test3,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch3.fit(train_X1,train_y1)\n",
    "\n",
    "# GBresults3 = pd.DataFrame(GBgsearch3.cv_results_)\n",
    "# GBresults3.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# GBgsearch3.best_params_,GBgsearch3.best_score_\n",
    "\n",
    "\n",
    "##########Tune n_estimators (230 -> 300 in steps of 10) and max_depth (3 -> 7 steps of 1)\n",
    "# GB_param_test4 = {'n_estimators':range(230,301,10),'max_depth':range(3,8,1)}\n",
    "# GBgsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test4,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch4.fit(train_X1,train_y1)\n",
    "# GBgsearch4.best_params_,GBgsearch4.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#########Tune n_estimators (190 -> 390 in steps of 10) \n",
    "# GB_param_test5 = {'n_estimators':range(190,391,10)}\n",
    "# GBgsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,max_depth=4,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test5,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch5.fit(train_X1,train_y1)\n",
    "# GBgsearch5.best_params_,GBgsearch5.best_score_\n",
    "\n",
    "\n",
    "########Tune min_samples_split (4 -> 20 steps of 1) and min_samples_leaf (2 -> 10 steps of 1)\n",
    "# GB_param_test6 = {'min_samples_split':range(4,21,1),'min_samples_leaf':range(2,11,1)}\n",
    "# GBgsearch6 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,n_estimators=380,max_depth=4,max_features='sqrt',subsample=0.8,random_state=5),\n",
    "#                          param_grid=GB_param_test6,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch6.fit(train_X1,train_y1)\n",
    "# GBgsearch6.best_params_,GBgsearch6.best_score_\n",
    "\n",
    "# modelfitCV(GBgsearch6.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB6 = GBgsearch6.best_estimator_\n",
    "# GBmodel6 = GB6.fit(train_X1,train_y1)\n",
    "# GBp6 = GBmodel6.predict(test_X1)\n",
    "# GBp6 = pd.Series(GBp6)\n",
    "# GBp6 = pd.concat([GBp6,test_IDs.rename('Id')],axis=1)\n",
    "# GBp6 = GBp6.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp6 = GBp6[['Id','SalePrice']]\n",
    "\n",
    "# GBp6.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp6_09172019.csv')\n",
    "# GBp6.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp6_09172019.csv')\n",
    "\n",
    "\n",
    "#########Tune max_features (10 -> 32 steps of 2) and subsample (0.4->0.8 steps of 0.1)\n",
    "# GB_param_test7 = {'max_features':range(10,33,2),'subsample':np.arange(0.4,0.9,0.1)}\n",
    "# GBgsearch7 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.0580,n_estimators=380,max_depth=4,min_samples_split=19,min_samples_leaf=2,random_state=5),\n",
    "#                          param_grid=GB_param_test7,scoring='neg_mean_squared_log_error',cv=5,iid=False,n_jobs=-1)\n",
    "# GBgsearch7.fit(train_X1,train_y1)\n",
    "# GBgsearch7.best_params_,GBgsearch7.best_score_\n",
    "\n",
    "# modelfitCV(GBgsearch7.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# GB7 = GBgsearch7.best_estimator_\n",
    "# GBmodel7 = GB7.fit(train_X1,train_y1)\n",
    "# GBp7 = GBmodel7.predict(test_X1)\n",
    "# GBp7 = pd.Series(GBp7)\n",
    "# GBp7 = pd.concat([GBp7,test_IDs.rename('Id')],axis=1)\n",
    "# GBp7 = GBp7.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# GBp7 = GBp7[['Id','SalePrice']]\n",
    "\n",
    "# GBp7.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/GBp7_09172019.csv')\n",
    "# GBp7.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/GBp7_09172019.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "############ XGBoost (9/18/19) ############\n",
    "#Following the instructions from the paper.\n",
    "#First starting model\n",
    "\n",
    "# This code can remove the warning you get because its not that serious, but just in case. \n",
    "# import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "#############Untuned model\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=6,min_child_weight=1,gamma=0,subsample=0.6,\n",
    "#                    colsample_bytree=0.1,scale_pos_weight=1,seed=13,objective='reg:squarederror')\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb1,train_X1,train_y1)\n",
    "\n",
    "# XGBmodel = xgb1.fit(train_X1,train_y1)\n",
    "# XGBp = XGBmodel.predict(test_X1)\n",
    "# XGBp = pd.Series(XGBp)\n",
    "# XGBp = pd.concat([XGBp,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp = XGBp.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp = XGBp[['Id','SalePrice']]\n",
    "\n",
    "# XGBp.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp_09182019.csv')\n",
    "# XGBp.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "#############Tune max_depth and min_child_weight \n",
    "# XGB_param_test1 = {'max_depth':range(2,13,2),'min_child_weight':range(1,6,1)}\n",
    "# XGBgsearch1 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),param_grid=XGB_param_test1,\n",
    "#                           scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch1.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults1 = pd.DataFrame(XGBgsearch1.cv_results_)\n",
    "# XGBresults1.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch1.best_params_,XGBgsearch1.best_score_\n",
    "\n",
    "\n",
    "\n",
    "#############Finetune max_depth and min_child_weight even more.\n",
    "# XGB_param_test2 = {'max_depth':range(3,7,1),'min_child_weight':range(2,6,1)}\n",
    "# XGBgsearch2 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,gamma=0,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                   objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                            param_grid=XGB_param_test2,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch2.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults2 = pd.DataFrame(XGBgsearch2.cv_results_)\n",
    "# XGBresults2.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch2.best_params_,XGBgsearch2.best_score_\n",
    "\n",
    "# modelfitXGB(XGBgsearch2.best_estimator_,train_X1,train_y1)\n",
    "\n",
    "# XGB2 = XGBgsearch2.best_estimator_\n",
    "# XGBmodel2 = XGB2.fit(train_X1,train_y1)\n",
    "# XGBp2 = XGBmodel2.predict(test_X1)\n",
    "# XGBp2 = pd.Series(XGBp2)\n",
    "# XGBp2 = pd.concat([XGBp2,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp2 = XGBp2.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp2 = XGBp2[['Id','SalePrice']]\n",
    "\n",
    "# XGBp2.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp2_09182019.csv')\n",
    "# XGBp2.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp2_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "###########Tune gamma\n",
    "# XGB_param_test3 = {'gamma':np.arange(0.0,1.0,0.1)}\n",
    "# XGBgsearch3 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,max_depth=3,min_child_weight=2,subsample=0.6,colsample_bytree=0.1,\n",
    "#                                                  objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test3,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch3.fit(train_X1,train_y1)\n",
    "# XGBresults3 = pd.DataFrame(XGBgsearch3.cv_results_)\n",
    "# XGBresults3.loc[:,('params','mean_test_score','std_test_score')] #All these values are the same. In other words, Gamma doesn't really matter.\n",
    "#XGBgsearch3.best_params_,XGBgsearch3.best_score_\n",
    "\n",
    "\n",
    "##########Tune subsample and colsample_bytree\n",
    "# XGB_param_test4 = {'subsample':np.arange(0.3,1.0,0.1),'colsample_bytree':np.arange(0.1,1.0,0.1)}\n",
    "# XGBgsearch4 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=300,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                                                  objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test4,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch4.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults4 = pd.DataFrame(XGBgsearch4.cv_results_)\n",
    "# XGBresults4.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch4.best_params_,XGBgsearch4.best_score_\n",
    "\n",
    "# xgb_recalibrate = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate,train_X1,train_y1) #the recalibration shows n_estimators = 100 is optimal.\n",
    "\n",
    "\n",
    "#Check on how this tuned model does on Kaggle. <- It got worse! It may be overfitting the training data\n",
    "# XGB4 = XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodel4 = XGB4.fit(train_X1,train_y1)\n",
    "# XGBp4 = XGBmodel4.predict(test_X1)\n",
    "# XGBp4 = pd.Series(XGBp4)\n",
    "# XGBp4 = pd.concat([XGBp4,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp4 = XGBp4.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp4 = XGBp4[['Id','SalePrice']]\n",
    "\n",
    "# XGBp4.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp4_09182019.csv')\n",
    "# XGBp4.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp4_09182019.csv')\n",
    "\n",
    "\n",
    "##########Tune 1st regularization parameter reg_alpha\n",
    "# XGB_param_test5 = {'reg_alpha':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "# XGBgsearch5 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test5,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch5.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults5 = pd.DataFrame(XGBgsearch5.cv_results_)\n",
    "# XGBresults5.loc[:,('params','mean_test_score','std_test_score')]\n",
    "# XGBgsearch5.best_params_,XGBgsearch5.best_score_\n",
    "\n",
    "\n",
    "##########Tune 2nd regularization parameter reg_lambda\n",
    "# XGB_param_test6 = {'reg_lambda':[0.0001,0.001,0.01,0.1,1,10]}\n",
    "# XGBgsearch6 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test6,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch6.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults6 = pd.DataFrame(XGBgsearch6.cv_results_)\n",
    "# XGBresults6.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#XGBgsearch6.best_params_,XGBgsearch6.best_score_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##########Fine tune reg_lambda\n",
    "# XGB_param_test7 = {'reg_lambda':np.arange(0.3,2.0,0.1)}\n",
    "# XGBgsearch7 = GridSearchCV(estimator=XGBRegressor(learning_rate=0.1,n_estimators=100,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,\n",
    "#                                                  subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13),\n",
    "#                           param_grid=XGB_param_test7,scoring='neg_mean_squared_log_error',n_jobs=-1,iid=False,cv=5)\n",
    "# XGBgsearch7.fit(train_X1,train_y1)\n",
    "\n",
    "# XGBresults7 = pd.DataFrame(XGBgsearch7.cv_results_)\n",
    "# XGBresults7.loc[:,('params','mean_test_score','std_test_score')]\n",
    "#XGBgsearch7.best_params_,XGBgsearch7.best_score_\n",
    "\n",
    "\n",
    "# xgb_recalibrate2 = XGBRegressor(learning_rate=0.1,n_estimators=1000,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate2,train_X1,train_y1) #the recalibration shows n_estimators = 213 is optimal.\n",
    "\n",
    "# Check how it performs after the model is mostly tuned.\n",
    "# XGB7 = XGBRegressor(learning_rate=0.1,n_estimators=213,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodel7 = XGB7.fit(train_X1,train_y1)\n",
    "# XGBp7 = XGBmodel7.predict(test_X1)\n",
    "# XGBp7 = pd.Series(XGBp7)\n",
    "# XGBp7 = pd.concat([XGBp7,test_IDs.rename('Id')],axis=1)\n",
    "# XGBp7 = XGBp7.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBp7 = XGBp7[['Id','SalePrice']]\n",
    "\n",
    "# XGBp7.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBp7_09182019.csv')\n",
    "# XGBp7.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBp7_09182019.csv')\n",
    "\n",
    "\n",
    "\n",
    "##########Reducing the learning_rate = 0.01 to see if the new n_estimators will improve the model.\n",
    "# xgb_recalibrate3 = XGBRegressor(learning_rate=0.01,n_estimators=5000,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                                subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "# modelfitXGB(xgb_recalibrate3,train_X1,train_y1) #the recalibration shows n_estimators = 2438\n",
    "\n",
    "# Final tuned model \n",
    "# XGBF = XGBRegressor(learning_rate=0.01,n_estimators=2438,max_depth=3,min_child_weight=2,gamma=0,reg_alpha=0.001,reg_lambda=1,\n",
    "#                               subsample=0.9,colsample_bytree=0.8,objective='reg:squarederror',scale_pos_weight=1,seed=13)\n",
    "# XGBmodelF = XGBF.fit(train_X1,train_y1)\n",
    "# XGBpF = XGBmodelF.predict(test_X1)\n",
    "# XGBpF = pd.Series(XGBpF)\n",
    "# XGBpF = pd.concat([XGBpF,test_IDs.rename('Id')],axis=1)\n",
    "# XGBpF = XGBpF.rename(columns = {0:'SalePrice','Id':'Id'})\n",
    "# XGBpF = XGBpF[['Id','SalePrice']]\n",
    "\n",
    "# XGBpF.to_csv(r'/Users/armenta/Kaggle/Housing Prices/Predictions/XGBpF_09182019.csv')\n",
    "# XGBpF.to_csv(r'/Users/armenta/Desktop/Data Science/Kaggle/Getting Started Projects/Housing Data/PREDICTIONS/XGBpF_09182019.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0,
     6,
     12,
     18,
     24,
     30,
     36,
     42,
     48,
     54,
     62,
     74,
     81,
     92,
     98,
     117,
     124,
     139,
     145,
     156,
     162,
     194,
     200
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004472215000110959"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############ STANDARDIZED: XGBOOST (12/22/19) ###########\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "rcParams['figure.figsize'] = 12, 4 #width x height in inches\n",
    "\n",
    "########### PART I ############\n",
    "\n",
    "############ Round I - n_est #############\n",
    "# xgb1 = XGBRegressor(learning_rate=0.1, max_depth=5, min_child_weight=4, subsample=0.6, colsample_bytree=0.0555, \n",
    "#                          objective='reg:squarederror',seed=7)\n",
    "# n_est_range = range(100,355,5)\n",
    "# model1, model_results1, best_n_est, best_score1 = XGBRModelTune(xgb1,'n_estimators',n_est_range)\n",
    "\n",
    "############ Round II - max_depth ###########\n",
    "# xgb2 = XGBRegressor(learning_rate=0.1, n_estimators=330, min_child_weight=4, subsample=0.6, colsample_bytree=0.0555, \n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# max_d_range = range(1,11,1)\n",
    "# model2, model_results2, best_n_est, best_score2 = XGBRModelTune(xgb2, 'max_depth', max_d_range)\n",
    "\n",
    "############ Round III - min_child_weight ############\n",
    "# xgb3 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, subsample=0.6, colsample_bytree=0.0555, \n",
    "#                      objective='reg:squarederror',seed=7)\n",
    "# mcw_range = range(1,11,1)\n",
    "# model3, model_results3, best_mcw, best_score3 = XGBRModelTune(xgb3, 'min_child_weight', mcw_range)\n",
    "\n",
    "############ Round IV - gamma ##########\n",
    "# xgb4 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, subsample=0.6, colsample_bytree=0.0555, \n",
    "#                       objective='reg:squarederror',seed=7)\n",
    "# gamma_range = range(0,21,1)\n",
    "# model4, model_results4, best_gamma, best_score4 = XGBRModelTune(xgb4, 'gamma', gamma_range)\n",
    "\n",
    "############ Round V - subsample ############\n",
    "# xgb5 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, colsample_bytree=0.0555, \n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# ss_range = np.arange(0.1,1.01,0.01)\n",
    "# model5, model_results5, best_ss, best_score5 = XGBRModelTune(xgb5, 'subsample', ss_range)\n",
    "\n",
    "############ Round VI - colsample_bytree ############\n",
    "# xgb6 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, subsample=1, \n",
    "#                     objective='reg:squarederror',seed=7)\n",
    "# colsamp_range = np.arange(0.01,1.0,0.01)\n",
    "# model6, model_results6, best_colsamp, best_score6 = XGBRModelTune(xgb6, 'colsample_bytree', colsamp_range)\n",
    "\n",
    "############ Round VII - lambda ###########\n",
    "# xgb7 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, subsample=1,colsample_bytree=0.46, \n",
    "#                      objective='reg:squarederror',seed=7)\n",
    "# lamb_range = range(0,16,1)\n",
    "# model7, model_results7, best_lambda, best_score7 = XGBRModelTune(xgb7, 'reg_lambda', lamb_range)\n",
    "\n",
    "############ Round VIII - alpha ############\n",
    "# xgb8 = XGBRegressor(learning_rate=0.1, n_estimators=330, max_depth=4, subsample=1,colsample_bytree=0.46, \n",
    "#                      objective='reg:squarederror',seed=7)\n",
    "# alpha_range = range(0,16,1)\n",
    "# model8, model_results8, best_alpha, best_score8 = XGBRModelTune(xgb8, 'reg_alpha', alpha_range)\n",
    "\n",
    "############ Round IX - learning_rate #############\n",
    "# xgb9 = XGBRegressor(n_estimators=1000, max_depth=4, subsample=1, colsample_bytree=0.46, objective='reg:squarederror',\n",
    "#                     seed=7,reg_alpha=1)\n",
    "# exp_LR = -3*np.random.rand(100)\n",
    "# learning_rate_range = 10**exp_LR\n",
    "# model9, model_results9, best_LR, best_score9 = XGBRModelTune(xgb9, 'learning_rate', learning_rate_range, \n",
    "#                                                              Randomized=True, n_iter=60, modelfit=False)\n",
    "\n",
    "############ Save Tuned Model ############\n",
    "# lr9 = 0.024852786825405707\n",
    "# xgb_tuned = XGBRegressor(learning_rate=lr9, n_estimators=1000, max_depth=4, subsample=0.66, colsample_bytree=0.46,\n",
    "#                         objective='reg:squarederror',seed=7, reg_alpha=1)\n",
    "# modelfitXGB(xgb_tuned,strain_X1,train_y1)\n",
    "#train_err1, train_var1 = TrainTestErrors(xgb_tuned,'xgbtuned_DP1_strain_X1_01192020.csv')\n",
    "#train_err1\n",
    "\n",
    "\n",
    "\n",
    "########### PART II ##########\n",
    "\n",
    "########### Round I - min_child_weight #########\n",
    "# lr9 = 0.024852786825405707\n",
    "# xgb1 = XGBRegressor(learning_rate=lr9, n_estimators=1000, max_depth=4, subsample=0.66, colsample_bytree=0.46, \n",
    "#                    objective='reg:squarederror',seed=7,reg_alpha=1, reg_lambda=1)\n",
    "# mcw_range = range(1,11,1)\n",
    "# model1, model_results1, best_mcw, best_score1 = XGBRModelTune(xgb1, 'min_child_weight', mcw_range)\n",
    "\n",
    "############ TEST VALUES ##########\n",
    "# xgbI_MCW3 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=3,max_depth=4,subsample=0.66,\n",
    "#                          colsample_bytree=0.46,reg_alpha=1, reg_lambda=1, objective='reg:squarederror',seed=7)\n",
    "# train_err1, train_exp_var1 = TrainTestErrors(xgbI_MCW3,'xgbI_MCW3_02102020.csv')\n",
    "# train_err1\n",
    "# xgbI_MCW5 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=5,max_depth=4,subsample=0.66,\n",
    "#                           colsample_bytree=0.46,reg_alpha=1, reg_lambda=1, objective='reg:squarederror',seed=7)\n",
    "# train_err2, train_exp_var2 = TrainTestErrors(xgbI_MCW5, 'xgbI_MCW5_02102020.csv')\n",
    "# train_err2\n",
    "# model_results1\n",
    "\n",
    "########### Round II - reg_lambda #########\n",
    "# xgb2 = XGBRegressor(learning_rate=lr9, n_estimators=1000, max_depth=4, subsample=0.66, colsample_bytree=0.46, \n",
    "#                     min_child_weight=1, objective='reg:squarederror',seed=7,reg_alpha=1)\n",
    "# lamb_range = range(0,21,1)\n",
    "# model2, model_results2, best_lambda, best_score2 = XGBRModelTune(xgb2, 'reg_lambda', lamb_range)\n",
    "\n",
    "########### TEST VALUES ############\n",
    "# xgbI_RL3 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                        colsample_bytree=0.46,reg_alpha=1, reg_lambda=3, objective='reg:squarederror',seed=7)\n",
    "# train_err3, train_exp_var3 = TrainTestErrors(xgbI_RL3,'xgbI_RL3_02102020.csv')\n",
    "# train_err3\n",
    "# xgbI_RL7 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                        colsample_bytree=0.46,reg_alpha=1, reg_lambda=7, objective='reg:squarederror',seed=7)\n",
    "# train_err4, train_exp_var4 = TrainTestErrors(xgbI_RL7,'xgbI_RL7_02102020.csv')\n",
    "# train_err4\n",
    "# xgbI_RL11 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                         colsample_bytree=0.46,reg_alpha=1, reg_lambda=11, objective='reg:squarederror',seed=7)\n",
    "# train_err5, train_exp_var5 = TrainTestErrors(xgbI_RL11,'xgbI_RL11_02102020.csv')\n",
    "# train_err5\n",
    "# xgbI_RL15 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                         colsample_bytree=0.46,reg_alpha=1, reg_lambda=15, objective='reg:squarederror',seed=7)\n",
    "# train_err6, train_exp_var6 = TrainTestErrors(xgbI_RL15,'xgbI_RL15_02102020.csv')\n",
    "# train_err6\n",
    "# model_results2\n",
    "\n",
    "########## Round III - reg_alpha ############\n",
    "# lr9=0.024852786825405707\n",
    "# xgb3 = XGBRegressor(learning_rate=lr9, n_estimators=1000, max_depth=4, subsample=0.66, colsample_bytree=0.46, \n",
    "#                     min_child_weight=1, objective='reg:squarederror',seed=7,reg_lambda=7)\n",
    "# alpha_range = range(0,21,1)\n",
    "# model3, model_results3, best_alpha, best_score3 = XGBRModelTune(xgb3, 'reg_alpha', alpha_range)\n",
    "\n",
    "########## TEST VALUES ###########\n",
    "# xgbI_RA17 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                           colsample_bytree=0.46,reg_alpha=17, reg_lambda=7, objective='reg:squarederror',seed=7)\n",
    "# train_err7, train_exp_var7 = TrainTestErrors(xgbI_RA17, 'xgbI_RA17_02172019.csv')\n",
    "# train_err7\n",
    "# xgbI_RA11 =XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                           colsample_bytree=0.46,reg_alpha=11, reg_lambda=7, objective='reg:squarederror',seed=7)\n",
    "# train_err8, train_exp_var8 = TrainTestErrors(xgbI_RA11, 'xgbI_RA11_02172019.csv')\n",
    "# train_err8\n",
    "# xgbI_RA5 = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "#                         colsample_bytree=0.46,reg_alpha=5, reg_lambda=7, objective='reg:squarederror',seed=7)\n",
    "# train_err9, train_exp_var9 = TrainTestErrors(xgbI_RA5, 'xgbI_RA5_02172019.csv')\n",
    "# train_err9\n",
    "#model_results3\n",
    "\n",
    "########## Round IV - max_depth ###########\n",
    "# xgb4 = XGBRegressor(learning_rate=lr9, n_estimators=1000, subsample=0.66, colsample_bytree=0.46, \n",
    "#                      min_child_weight=1, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# md_range = range(1,11,1)\n",
    "# model4, model_results4, best_md, best_score4 = XGBRModelTune(xgb4, 'max_depth', md_range)\n",
    "\n",
    "########## TEST VALUES ##########\n",
    "# xgbI_MD3 = XGBRegressor(learning_rate=lr9, n_estimators=1000, subsample=0.66, colsample_bytree=0.46,max_depth=3, \n",
    "#                         min_child_weight=1, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err10, train_exp_var10 = TrainTestErrors(xgbI_MD3, 'xgbI_MD3_02172019.csv')\n",
    "# train_err10\n",
    "# xgbI_MD5 = XGBRegressor(learning_rate=lr9, n_estimators=1000, subsample=0.66, colsample_bytree=0.46,max_depth=5, \n",
    "#                         min_child_weight=1, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err11, train_exp_var11 = TrainTestErrors(xgbI_MD5, 'xgbI_MD5_02172019.csv')\n",
    "# train_err11\n",
    "# model_results4\n",
    "\n",
    "########## Round V - subsample ##########\n",
    "# xgb5 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, \n",
    "#                     max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# sub_range = np.arange(0.35,0.51,0.01)\n",
    "# model5, model_results5, best_sub, best_score5 = XGBRModelTune(xgb5, 'subsample', sub_range)\n",
    "\n",
    "########## TEST VALUES ###########\n",
    "# lr9=0.024852786825405707\n",
    "# xgbI_S82 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.82,\n",
    "#                      max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err12, train_exp_var12 = TrainTestErrors(xgbI_S82, 'xgbI_S82_02172019.csv')\n",
    "# train_err12\n",
    "# xgbI_S70 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.7,\n",
    "#                         max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err13, train_exp_var13 = TrainTestErrors(xgbI_S70, 'xgbI_S70_02172019.csv')\n",
    "# train_err13\n",
    "# xgbI_S60 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.6,\n",
    "#                         max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err14, train_exp_var14 = TrainTestErrors(xgbI_S60, 'xgbI_S60_02172019.csv')\n",
    "# train_err14\n",
    "# xgbI_S50 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.5,\n",
    "#                         max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err15, train_exp_var15 = TrainTestErrors(xgbI_S50, 'xgbI_S50_02172019.csv')\n",
    "# train_err15\n",
    "# xgbI_S40 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.4,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err16, train_exp_var16 = TrainTestErrors(xgbI_S40, 'xgbI_S40_02242020.csv')\n",
    "# train_err16\n",
    "# xgbI_S45 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.45,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err17, train_exp_var17 = TrainTestErrors(xgbI_S45, 'xgbI_S45_02242020.csv')\n",
    "# train_err17\n",
    "# xgbI_S49 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.46, min_child_weight=1, subsample=0.49,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err18, train_exp_var18 = TrainTestErrors(xgbI_S49, 'xgbI_S49_02242020.csv')\n",
    "# train_err18\n",
    "#model_results5\n",
    "\n",
    "########## Round VI - colsample_bytree ###########\n",
    "# xgb6 = XGBRegressor(learning_rate=lr9, n_estimators=1000, subsample=0.66, min_child_weight=1, \n",
    "#                     max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# cs_bt_range = np.arange(0.05,1.05,0.05)\n",
    "# model6, model_results6, best_colsample, best_score6 = XGBRModelTune(xgb6, 'colsample_bytree', cs_bt_range)\n",
    "\n",
    "########## TEST VALUES ##########\n",
    "# xgbI_CS25 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.25, min_child_weight=1, subsample=0.66,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err19, train_exp_var19 = TrainTestErrors(xgbI_CS25, 'xgbI_CS25_02242020.csv')\n",
    "# train_err19\n",
    "# xgbI_CS30 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.30, min_child_weight=1, subsample=0.66,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err20, train_exp_var20 = TrainTestErrors(xgbI_CS30, 'xgbI_CS30_02242020.csv')\n",
    "# train_err20\n",
    "# xgbI_CS20 = XGBRegressor(learning_rate=lr9, n_estimators=1000, colsample_bytree=0.20, min_child_weight=1, subsample=0.66,\n",
    "#                          max_depth=4, objective='reg:squarederror',seed=7,reg_lambda=7,reg_alpha=5)\n",
    "# train_err21, train_exp_var21 = TrainTestErrors(xgbI_CS20, 'xgbI_CS20_02242020.csv')\n",
    "# train_err21\n",
    "#model_results6\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning:\n",
      "\n",
      "The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############# FINAL (3/9/20) ##############\n",
    "\n",
    "#We'll save the feature importances of the model to an Excel file so we can compare it with data from \n",
    "#our other pipeline in Tableau.\n",
    "#According to the Excel file, our best model was: \n",
    "#xgbI_RL7 (learning_rate ~ 0.0248527, n_est = 1000, max_depth=4, min_child_weight = 1, subsample=0.66, colsample_bytree=0.46, objective='reg:squarederror', seed=7,  reg_alpha=1, reg_lambda=3) *Standardized data\n",
    "\n",
    "#Create the model.\n",
    "lr9= 0.024852786825405707\n",
    "best_model = XGBRegressor(learning_rate=lr9,n_estimators=1000,min_child_weight=1,max_depth=4,subsample=0.66,\n",
    "                        colsample_bytree=0.46, reg_alpha=1, reg_lambda=7, objective='reg:squarederror',seed=7)\n",
    "#Fit the model to the data.\n",
    "best_model.fit(strain_X1,train_y1)\n",
    "\n",
    "#Grab the important features.\n",
    "feat_imp = pd.Series(best_model.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "\n",
    "#Save the feature importances.\n",
    "feat_imp.to_csv('/Users/armenta/Kaggle/Housing Prices/FI_pipeline1.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "########## NOT NEEDED CODE (FOR NOW) ############\n",
    "# #Check the amount of missing values for each column. \n",
    "# #np.sum(train_X.isnull())\n",
    "#\n",
    "#\n",
    "#\n",
    "# #LOOKING AT THE DISTRIBUTION OF DIFFERENT VARIABLES (ENDED 5/14/19)\n",
    "# #Plot a histogram of some of the data.\n",
    "# #CATEGORICAL VARIABLES\n",
    "# curr_col = train_X.Neighborhood\n",
    "# curr_col.value_counts().plot(kind='bar')\n",
    "# curr_col.value_counts()\n",
    "#\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "#\n",
    "# curr_var = train_house.loc[train_house.Neighborhood == neighborhood[24],'SalePrice']\n",
    "# curr_var.hist(bins=25)\n",
    "# curr_var.describe()\n",
    "#\n",
    "# #LOOKING AT SPECIFIC COLUMNS TO SEE HOW WELL THEY AFFECT THE SALES PRICE \n",
    "#\n",
    "# var1 = train_house.loc[train_house.Alley == 'Pave', :]\n",
    "# var2 = train_house.loc[train_house.Alley == 'Grvl', :]\n",
    "# var3 = train_house.loc[train_house.SaleType == 'COD', :]\n",
    "# var4 = train_house.loc[train_house.SaleType == 'ConLD', :]\n",
    "# var5 = train_house.loc[train_house.SaleType == 'ConLw', :]\n",
    "# var6 = train_house.loc[train_house.SaleType == 'ConLI', :]\n",
    "# var7 = train_house.loc[train_house.SaleType == 'CWD', :]\n",
    "# var8 = train_house.loc[train_house.SaleType == 'Oth', :]\n",
    "# var9 = train_house.loc[train_house.SaleType == 'Con', :]\n",
    "#\n",
    "#\n",
    "# var1.SalePrice.hist(bins=25)\n",
    "# var1.SalePrice.describe()\n",
    "#\n",
    "# var2.SalePrice.hist(bins=25)\n",
    "# var2.SalePrice.describe()\n",
    "#\n",
    "# var3.SalePrice.hist(bins=25)\n",
    "# var3.SalePrice.describe()\n",
    "#\n",
    "# var4.SalePrice.hist(bins=25)\n",
    "# var4.SalePrice.describe()\n",
    "#\n",
    "# var5.SalePrice.hist(bins=25)\n",
    "# var5.SalePrice.describe()\n",
    "#\n",
    "# var6.SalePrice.hist(bins=25)\n",
    "# var6.SalePrice.describe()\n",
    "#\n",
    "# var7.SalePrice.hist(bins=25)\n",
    "# var7.SalePrice.describe()\n",
    "#\n",
    "# var8.SalePrice.hist(bins=25)\n",
    "# var8.SalePrice.describe()\n",
    "#\n",
    "# var9.SalePrice.hist(bins=25)\n",
    "# var9.SalePrice.describe()\n",
    "#\n",
    "# #Plot a histogram of some of the data \n",
    "# #NUMERICAL VARIABLES \n",
    "# num_col = train_X.loc[:,'MiscVal']\n",
    "# num_col.hist(bins = 50)\n",
    "#\n",
    "# num_col.describe()\n",
    "# num_col.value_counts()\n",
    "\n",
    "\n",
    "\n",
    "# #Looking at the house with the largest lot area to see if it makes sense.\n",
    "# #train_house.loc[train_house.LotArea.idxmax(),:]\n",
    "\n",
    "# #Summary\n",
    "# # The idea I am getting of this house is that it is:\n",
    "# # - A house not surrounded by any other houses.\n",
    "# # - It has a total of ~4000 sq ft of house, and ~210,000 sq ft of land surronding it\n",
    "# # - Bit of an old school (1965), brick faced, hip roof style house. Kind of like a haunted looking house with brick face\n",
    "# # - All of this factored in with the quality of the house makes sense why its not the most expensive house even if it \n",
    "# # the largest. \n",
    "# # In conclusion, this house can stay in this dataset.\n",
    "\n",
    "# train_house.SalePrice.hist(bins = 25)\n",
    "# train_house.SalePrice.describe()\n",
    "\n",
    "# LQ = train_house.loc[train_house.LowQualFinSF > 0 ,]\n",
    "# LQ.SalePrice.hist(bins=25)\n",
    "\n",
    "# InsidePorches = train_house.loc[train_house.EnclosedPorch > 0, ]\n",
    "# InsidePorches.EnclosedPorch.hist(bins=25)\n",
    "# InsidePorches.EnclosedPorch.describe()\n",
    "\n",
    "# InsidePorches.SalePrice.hist(bins=25)\n",
    "\n",
    "# SznP = train_house.loc[train_house.loc[:,'3SsnPorch']>0,]\n",
    "# SznP.SalePrice.hist(bins=25)\n",
    "#\n",
    "# screenporch = train_house.loc[train_house.ScreenPorch >0,]\n",
    "# screenporch.SalePrice.hist(bins=25)\n",
    "\n",
    "# pool_niggas = train_house.loc[train_house.PoolArea >0,]\n",
    "# pool_niggas.SalePrice.hist(bins=25)\n",
    "\n",
    "# misc_val = train_house.loc[train_house.MiscVal > 0,]\n",
    "# misc_val.SalePrice.hist(bins=25)\n",
    "\n",
    "\n",
    "#For BsmtExposure variable\n",
    "# inds2 = train_house.BsmtExposure.loc[pd.isnull(train_house.BsmtExposure)]\n",
    "# inds2 = inds2.index\n",
    "\n",
    "\n",
    "# for x in inds2:\n",
    "#      if(sum(inds == x) == 0):\n",
    "#         index = x\n",
    "        \n",
    "# index\n",
    "\n",
    "# #For BsmtExposure variable\n",
    "# inds2 = train_house.BsmtFinType2.loc[pd.isnull(train_house.BsmtFinType2)]\n",
    "# inds2 = inds2.index\n",
    "\n",
    "\n",
    "# for x in inds2:\n",
    "#      if(sum(inds == x) == 0):\n",
    "#         index = x\n",
    "        \n",
    "# index\n",
    "\n",
    "\n",
    "# #Checking out the condition and quality of houses that have terrible functionality scores to see what the missing \n",
    "# #functional values could possibly be.\n",
    "\n",
    "# combined_df.loc[combined_df.Functional == 'Maj2',:]\n",
    "\n",
    "#SPECIAL CELL *NEED TO FIND A WAY TO INCORPORATE THIS CELL WITH THE ONE BELOW IT.\n",
    "#\n",
    "#Gotta make some corrections for the following columns: BsmtExposure, and BsmtFinType2\n",
    "#train_X.loc[948,'BsmtExposure'] = 'No'\n",
    "#\n",
    "#Comment back in and Comment out once you run ONCE!\n",
    "#train_X = train_X.drop(332) #we are dropping this record because we do not know what to fill in for BsmtFinType2 for index = 332\n",
    "#must do the same thing for our Y dataset\n",
    "#train_y = train_y.drop(332)\n",
    "# combined_df = combined_df.drop(combined_df.index[[2150,2119,2187,1554,2215,2472,2575,2488,1914,1944,2249,2903]])\n",
    "\n",
    "\n",
    "# #COMBINING THE TRAINING AND TESTING DATASETS TO CREATE A SUPER DATA SET.\n",
    "# combined_df = pd.concat([train_X,test_X])\n",
    "# combined_df = combined_df.reset_index(drop = True) #the drop variables removes the old indices. Otherwise it gets created as a new column\n",
    "# #combined_df.isnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########We need to handle all the missing values in the following columns:\n",
    "# #THESE WERE HANDLED FROM THE TRAIN_X DATASET.\n",
    "# #LotFrontage = 484\n",
    "# #Alley = 2709\n",
    "# #MasVnrType = 24 \n",
    "# #MasVnrArea = 23\n",
    "# #BsmtQual = 76\n",
    "# #BsmtCond = 77\n",
    "# #BsmtExposure = 76\n",
    "# #BsmtFinType1 = 74\n",
    "# #BsmtFinType2 = 74\n",
    "# #FireplaceQu = 1412\n",
    "# #GarageType = 156\n",
    "# #GarageYrBlt = 157\n",
    "# #GarageFinish = 157\n",
    "# #GarageQual = 157\n",
    "# #GarageCond = 157\n",
    "# #PoolQC = 2896\n",
    "# #Fence = 2337\n",
    "# #MiscFeature = 2802\n",
    "\n",
    "# #THESE ARE NEW FEATURES WITH MISSING VALUES ADDED FROM TEST_X DATASET (EXCEPT ELECTRICAL).\n",
    "# #THESE WILL ALL BE REMOVED (12 ROWS IN TOTAL)\n",
    "# #MSZoning = 4, Utilities = 2, Exterior1st = 1, Exterior2nd = 1, BsmtFinSF1 = 1, BsmtFinSF2 = 1, BsmtUnfSF = 1, TotalBsmtSF = 1, \n",
    "# #BsmtFullBath = 2, BsmtHalfBath = 2, KitchenQual = 1, Functional = 2, GarageCars = 1, GarageArea = 1, SaleType = 1, Electrical = 1\n",
    "\n",
    "# #Removing some indices that I discovered from preliminary research along with some bad data from the columns listed above\n",
    "# #332 -> BsmtFinType2 is NaN while the other basement variables are okay, so I didn't know what to replace this with.\n",
    "# #948 -> BsmtExposure was NaN while other basement variables are okay.\n",
    "# #1379 -> Removed the Electrical NaN in the dataset, it is stupid to keep this.\n",
    "# #2151 - 2904 -> Related to all of the missing values listed above.\n",
    "# combined_df = combined_df.drop(combined_df.index[[332,948,1379,2151,2120,2188,1555,2216,2473,2576,2489,1915,1945,2250,2904]])\n",
    "# combined_df = combined_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "# #Uncomment this once we've handled the missing values that need to be removed. Do this for the\n",
    "# #combined dataset, not just the training dataset.\n",
    "# #First we will do the transformations on practice_train_X to make sure it does what we really want to do.\n",
    "# # train_X1 = train_X1.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "# #                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "# #                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None',\n",
    "# #                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 'None',\n",
    "# #                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "# #                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "\n",
    "\n",
    "# #LOOKING AT SUSPICIOUS MISSING VALUES\n",
    "# combined_df.loc[combined_df.BsmtCond.isnull()==True,]\n",
    "\n",
    "# #Allows us to look at the unique values and match up with the sheet\n",
    "# curr_col.unique()\n",
    "# curr_col.describe()\n",
    "\n",
    "\n",
    "#Left over code from checking different attributes of combined_df\n",
    "#combined_df.loc[combined_df.SaleType.isnull() == True,:]\n",
    "#Used this snippet to coordinate the row #'s with their listed index\n",
    "#combined_df.index[2903]\n",
    "#combined_df.loc[combined_df.Id == 2905,:]\n",
    "# test_X1 = test_X1.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "#                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "#                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None','Electrical' : 'NA',\n",
    "#                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 'None',\n",
    "#                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "\n",
    "\n",
    "#The bad indices were due to GarageYrBlt and I replaced the missing values with None instead\n",
    "#of a number (like 0).\n",
    "#row1 = train_X1.loc[0,:]\n",
    "#len(row1[row1=='None'])\n",
    "# bad_indices = []\n",
    "# for i in range(train_X1.shape[0]):\n",
    "#     curr_row = train_X1.loc[i,:]\n",
    "#     if(len(curr_row[curr_row=='None']) != 0):\n",
    "#         bad_indices.append(i)\n",
    "# train_X1.loc[bad_indices,:]\n",
    "\n",
    "#pd.unique(train_X1.LandSlope)\n",
    "\n",
    "# #Used to get the current directory\n",
    "# import os \n",
    "# os.getcwd()\n",
    "\n",
    "\n",
    "#Uncomment this once we've handled the missing values that need to be removed. Do this for the\n",
    "#combined dataset, not just the training dataset.\n",
    "#First we will do the transformations on practice_train_X to make sure it does what we really want to do.\n",
    "# train_X = train_X.fillna({'LotFrontage' : 0,'Alley' : 'No Alley','MasVnrType': 'NA','MasVnrArea':0,\n",
    "#                                             'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', \n",
    "#                                             'BsmtFinType1' : 'None','BsmtFinType2' : 'None',\n",
    "#                                             'FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "#                                             'GarageFinish' : 'None','GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None'})\n",
    "\n",
    "# test_X = test_X.fillna({'MSZoning':'NA','LotFrontage' : 0,'Utilities':'NA','Alley' : 'No Alley','MasVnrType': 'NA',\n",
    "#                         'MasVnrArea':0,'BsmtQual' : 'None','BsmtCond' : 'None','BsmtExposure' : 'None', 'Exterior1st':'NA',\n",
    "#                                             'Exterior2nd':'NA','BsmtFinType1' : 'None','BsmtFinSF1':0,'BsmtFinType2':'None',\n",
    "#                                             'BsmtFinSF2':0,'BsmtUnfSF':0,'TotalBsmtSF':0,'BsmtFullBath':0,'BsmtHalfBath':0,\n",
    "#                         'KitchenQual':'NA','Functional':'NA','FireplaceQu' : 'None','GarageType' : 'None','GarageYrBlt' : 0,\n",
    "#                                             'GarageFinish' : 'None','GarageCars':0,'GarageArea':0,'GarageQual' : 'None', 'GarageCond' : 'None',\n",
    "#                                             'PoolQC' : 'None', 'Fence' : 'None', 'MiscFeature' : 'None','SaleType':'NA'})\n",
    "\n",
    "\n",
    "\n",
    "#We need to make sure that they have the exact number of columns and that they are lined up the same.\n",
    "#Shows the columns that are in train_X1 but are not in test_X1\n",
    "#missing_col_from_test = train_X1.columns.difference(test_X1.columns)\n",
    "#missing_col_from_train = test_X1.columns.difference(train_X1.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
